{"meta":{"title":"天天","subtitle":"那马路上天天都在塞","description":"blog","author":"Luhumu","url":"http://example.com","root":"/"},"pages":[{"title":"所有分类","date":"2023-07-30T15:04:27.672Z","updated":"2023-07-30T15:04:27.672Z","comments":true,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"react落地项目","slug":"react基础落地项目","date":"2023-07-31T09:33:27.494Z","updated":"2023-07-31T09:33:48.323Z","comments":true,"path":"2023/07/31/react基础落地项目/","link":"","permalink":"http://example.com/2023/07/31/react%E5%9F%BA%E7%A1%80%E8%90%BD%E5%9C%B0%E9%A1%B9%E7%9B%AE/","excerpt":"","text":"项目前置准备 项目介绍本节目标: 了解项目的定位和功能项目功能演示登录、退出首页内容（文章）管理：文章列表、发布文章、修改文章技术React 官方脚手架 create-react-appreact hooks状态管理：mobxUI 组件库：antd v4ajax请求库：axios路由：react-router-dom 以及 history富文本编辑器：react-quillCSS 预编译器：sass 初始化项目使用 vitenpm create vite@latestaxiosnpm i axiosantdnpm install antd --saverouternpm install react-router-domsassnpm i sassmobxnpm install mobx --savenpm install mobx-react --save 路由配置 1234567891011121314import &#123;createBrowserRouter&#125; from &#x27;react-router-dom&#x27;import Login from &quot;../pages/Login/index.jsx&quot;;import Layout from &quot;../pages/Layout/index.jsx&quot;;const router = createBrowserRouter([ &#123; path: &#x27;/&#x27;, element: &lt;Layout/&gt; &#125;, &#123; path: &#x27;/login&#x27;, element: &lt;Login/&gt; &#125;,])export default router 1234567891011import React from &#x27;react&#x27;import ReactDOM from &#x27;react-dom/client&#x27;import &#123;RouterProvider&#125; from &quot;react-router-dom&quot;;import router from &quot;./router/index.jsx&quot;;import &#x27;./styles/index.scss&#x27;ReactDOM.createRoot(document.getElementById(&#x27;root&#x27;)).render( &lt;React.StrictMode&gt; &lt;RouterProvider router=&#123;router&#125; /&gt; &lt;/React.StrictMode&gt;,) 二次封装axios 12345678910111213141516171819202122232425import axios from &#x27;axios&#x27;const http = axios.create(&#123; baseURL: &#x27;http://geek.itheima.net/v1_0&#x27;, timeout: 20000&#125;)// 添加请求拦截器http.interceptors.request.use((config)=&gt; &#123; return config&#125;, (error)=&gt; &#123; return Promise.reject(error)&#125;)// 添加响应拦截器http.interceptors.response.use((response)=&gt; &#123; // 2xx 范围内的状态码都会触发该函数。 // 对响应数据做点什么 return response.data &#125;, (error)=&gt; &#123; // 超出 2xx 范围的状态码都会触发该函数。 // 对响应错误做点什么 return Promise.reject(error)&#125;)export &#123; http &#125; 登录模块1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import &#123;Button, Card, Checkbox, Form, Input&#125; from &#x27;antd&#x27;import logo from &#x27;../../assets/react.svg&#x27;import &#x27;./index.scss&#x27;import &#123;http&#125; from &quot;../../request/request.js&quot;;const Login = () =&gt; &#123; const onFinish = (values) =&gt; &#123; // todo 发请求 &#125;; return ( &lt;div className=&quot;login&quot;&gt; &lt;Card className=&quot;login-container&quot;&gt; &lt;img className=&quot;login-logo&quot; src=&#123;logo&#125; alt=&quot;&quot; /&gt; &#123;/* 登录表单 */&#125; &#123;/* 最外层的form */&#125; &lt;Form initialValues=&#123;&#123; mobile: &#x27;13911111111&#x27;, code: &#x27;246810&#x27;, remember: true &#125;&#125; onFinish=&#123;onFinish&#125; &gt; &#123;/* 表单如何获取表单的数据? 一定就是name字段 这个name绑定其中的input框 最终用户输入的数据会变为一个对象 values */&#125; &lt;Form.Item label=&quot;手机号&quot; name=&quot;mobile&quot; rules=&#123;[ &#123; pattern: /^1[3-9]\\d&#123;9&#125;$/, message: &#x27;手机号码格式不对&#x27;, validateTrigger: &#x27;onBlur&#x27; &#125;, &#123; required: true, message: &#x27;请输入手机号&#x27; &#125; ]&#125; &gt; &lt;Input size=&quot;large&quot; placeholder=&quot;请输入手机号&quot; /&gt; &lt;/Form.Item&gt; &lt;Form.Item label=&quot;验证码&quot; name=&quot;code&quot; rules=&#123;[ &#123; len: 6, message: &#x27;验证码6个字符&#x27;, validateTrigger: &#x27;onBlur&#x27; &#125;, &#123; required: true, message: &#x27;请输入验证码&#x27; &#125; ]&#125; &gt; &lt;Input size=&quot;large&quot; placeholder=&quot;请输入验证码&quot; /&gt; &lt;/Form.Item&gt; &lt;Form.Item name=&quot;remember&quot; valuePropName=&quot;checked&quot; &gt; &lt;Checkbox className=&quot;login-checkbox-label&quot;&gt; 我已阅读并同意「用户协议」和「隐私条款」 &lt;/Checkbox&gt; &lt;/Form.Item&gt; &lt;Form.Item&gt; &lt;Button type=&quot;primary&quot; htmlType=&quot;submit&quot; size=&quot;large&quot; block&gt; 登录 &lt;/Button&gt; &lt;/Form.Item&gt; &lt;/Form&gt; &lt;/Card&gt; &lt;/div&gt; )&#125;export default Login MObX12345678910111213141516171819202122import &#123;makeAutoObservable&#125; from &quot;mobx&quot;;import &#123;http&#125; from &quot;../request/request.js&quot;;class LoginStore &#123; token = &#x27;&#x27; constructor() &#123; // 响应式 makeAutoObservable(this) &#125; // 调用接口 获取token 把这个token存入仓库 // 登录 setToken = async (&#123;mobile, code&#125;) =&gt; &#123; const res = await http.post(&#x27;http://geek.itheima.net/v1_0/authorizations&#x27;, &#123; mobile, code &#125;) this.token = res.data.token &#125;&#125;export default LoginStore 123456789101112import React from &quot;react&quot;import LoginStore from &#x27;./login.Store&#x27;class RootStore &#123; // 组合模块 constructor() &#123; this.loginStore = new LoginStore() &#125;&#125;// 导入useStore方法供组件使用数据const StoresContext = React.createContext(new RootStore())export const useStore = () =&gt; React.useContext(StoresContext) 请求发送 进入主页 12345678const &#123; loginStore &#125; = useStore()const navigate = new useNavigate();const onFinish = (values) =&gt; &#123; // 发请求 loginStore.getToken(&#123;mobile: values.mobile, code: values.code&#125;) navigate(&#x27;/&#x27;, &#123;replace: true&#125;)&#125;; try写 123456789101112131415const &#123; loginStore &#125; = useStore()const navigate = new useNavigate();const onFinish = async (values) =&gt; &#123; // 发请求 // 这里try 最好写一下 容错还是很重要的 try &#123; await loginStore.getToken(&#123;mobile: values.mobile, code: values.code&#125;) navigate(&#x27;/&#x27;, &#123;replace: true&#125;) message.success(&#x27;虽然~~~~~~~~~~ 有这慢慢的感伤~&#x27;) &#125; catch (e) &#123; message.error(&quot;系统错误&quot;) &#125;&#125;; token 持久化存在本地的 localstore 1234567const TOKEN_KEY = &#x27;geek_pc&#x27;const getToken = () =&gt; localStorage.getItem(TOKEN_KEY)const setToken = token =&gt; localStorage.setItem(TOKEN_KEY, token)const clearToken = () =&gt; localStorage.removeItem(TOKEN_KEY)export &#123; getToken, setToken, clearToken &#125; 123456789101112131415161718192021222324import &#123;makeAutoObservable&#125; from &quot;mobx&quot;;import &#123;http&#125; from &quot;../request/request.js&quot;;import &#123;getToken, setToken&#125; from &quot;../utils/token.js&quot;;class LoginStore &#123; token = getToken || &#x27;&#x27; constructor() &#123; // 响应式 makeAutoObservable(this) &#125; // 调用接口 获取token 把这个token存入仓库 // 登录 getToken = async (&#123;mobile, code&#125;) =&gt; &#123; const res = await http.post(&#x27;http://geek.itheima.net/v1_0/authorizations&#x27;, &#123; mobile, code &#125;) this.token = res.data.token setToken(res.data.token) &#125;&#125;export default LoginStore 现在的问题是 所有的请求都需要借助这个 token 因此我们需要每次发请求前都去验证 有没有存在这个token 也就需要调方法 有没有一种拦截器 直接一劳永逸这就需要axios二次封装的 请求拦截器中 每次请求前都携带 123456// if not login add token const token = getToken() if (token) &#123; config.headers.Authorization = `Bearer $&#123;token&#125;` &#125; return config 此时的问题是 用户只要知道我们的网站 就可以直接进入网站内部 这太不安全了 vue中有路由的前置守卫 路由鉴权 *** 这里真的有点意思 写一个鉴权的组件 运用children技术 高级组件 来实现鉴权 修改路由 使用这个组件包裹具体组件 鉴权组件 123456789101112131415161718// 鉴权组件// 判断token是否存在 存在直接放行 否则 重定向到登陆页面// 高阶组件: 把一个组件当作另一个组件传入 通过判断返回新的组件import &#123;getToken&#125; from &quot;../utils/token.js&quot;;import &#123;Navigate&#125; from &quot;react-router-dom&quot;;const AuthComponent = (props) =&gt; &#123; const isToken = getToken(); if (isToken) &#123; &#123;/* 这里爆红了 无所谓 编译器问题 */&#125; return &lt;&gt;&#123;props.children&#125;&lt;/&gt; &#125; else &#123; // 路由组件 如果没有token 直接使用这个组件 定位到 登录页 return &lt;Navigate to=&#123;&#x27;/login&#x27;&#125; replace &gt;&lt;/Navigate&gt; &#125;&#125;export default AuthComponent 应用在组件上 123456789101112131415import &#123;createBrowserRouter&#125; from &#x27;react-router-dom&#x27;import Login from &quot;../pages/Login/index.jsx&quot;;import Layout from &quot;../pages/Layout/index.jsx&quot;;import AuthComponent from &quot;../components/AuthComponent.jsx&quot;;const router = createBrowserRouter([ &#123; path: &#x27;/&#x27;, element: &lt;AuthComponent&gt;&lt;Layout/&gt;&lt;/AuthComponent&gt; &#125;, &#123; path: &#x27;/login&#x27;, element: &lt;Login/&gt; &#125;,])export default router 主页 Layout模块1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465import &#123;Layout, Menu, message, Popconfirm&#125; from &#x27;antd&#x27;import &#123; HomeOutlined, DiffOutlined, EditOutlined, LogoutOutlined&#125; from &#x27;@ant-design/icons&#x27;import &#x27;./index.scss&#x27;import &#123;clearToken&#125; from &quot;../../utils/token.js&quot;;import &#123;useNavigate&#125; from &quot;react-router-dom&quot;;const &#123;Header, Sider&#125; = Layoutconst GeekLayout = () =&gt; &#123; const navigate = useNavigate() const logout = () =&gt; &#123; clearToken() navigate(&#x27;/login&#x27;) message.success(&quot;那再见虽无奈~~~ &quot;) &#125; return ( // Layout 整体布局 &lt;Layout&gt; &#123;/* 头部 onConfirm 当点击ok以后的方法 写个函数来设置 */&#125; &lt;Header className=&quot;header&quot;&gt; &lt;div className=&quot;logo&quot;/&gt; &lt;div className=&quot;user-info&quot;&gt; &lt;span className=&quot;user-name&quot;&gt;user.name&lt;/span&gt; &lt;span className=&quot;user-logout&quot;&gt; &lt;Popconfirm title=&quot;是否确认退出？&quot; okText=&quot;退出&quot; cancelText=&quot;取消&quot; onConfirm=&#123;() =&gt; logout()&#125; &gt; &lt;LogoutOutlined/&gt; 退出 &lt;/Popconfirm&gt; &lt;/span&gt; &lt;/div&gt; &lt;/Header&gt; &lt;Layout&gt; &lt;Sider width=&#123;200&#125; className=&quot;site-layout-background&quot;&gt; &lt;Menu mode=&quot;inline&quot; theme=&quot;dark&quot; defaultSelectedKeys=&#123;[&#x27;1&#x27;]&#125; style=&#123;&#123;height: &#x27;100%&#x27;, borderRight: 0&#125;&#125; &gt; &lt;Menu.Item icon=&#123;&lt;HomeOutlined/&gt;&#125; key=&quot;1&quot;&gt; 数据概览 &lt;/Menu.Item&gt; &lt;Menu.Item icon=&#123;&lt;DiffOutlined/&gt;&#125; key=&quot;2&quot;&gt; 内容管理 &lt;/Menu.Item&gt; &lt;Menu.Item icon=&#123;&lt;EditOutlined/&gt;&#125; key=&quot;3&quot;&gt; 发布文章 &lt;/Menu.Item&gt; &lt;/Menu&gt; &lt;/Sider&gt; &lt;Layout className=&quot;layout-content&quot; style=&#123;&#123;padding: 20&#125;&#125;&gt;内容&lt;/Layout&gt; &lt;/Layout&gt; &lt;/Layout&gt; )&#125;export default GeekLayout 12345678910111213141516171819202122232425262728293031323334353637.ant-layout &#123; height: 100%;&#125;.header &#123; padding: 0;&#125;.logo &#123; width: 200px; height: 60px; background: url(&#x27;../../assets/react.svg&#x27;) no-repeat center / 160px auto;&#125;.layout-content &#123; overflow-y: auto;&#125;.user-info &#123; position: absolute; right: 0; top: 0; padding-right: 20px; color: #fff; .user-name &#123; margin-right: 20px; &#125; .user-logout &#123; display: inline-block; cursor: pointer; &#125;&#125;.ant-layout-header &#123; padding: 0 !important;&#125; 填写二级路由注意二级路由要有出口 1234567891011121314151617181920212223242526272829303132import &#123;createBrowserRouter&#125; from &#x27;react-router-dom&#x27;import Login from &quot;../pages/Login/index.jsx&quot;;import Layout from &quot;../pages/Layout/index.jsx&quot;;import AuthComponent from &quot;../components/AuthComponent.jsx&quot;;import Home from &quot;../pages/Home/index.jsx&quot;;import Article from &quot;../pages/Article/index.jsx&quot;;import Publish from &quot;../pages/publish/index.jsx&quot;;const router = createBrowserRouter([ &#123; path: &#x27;/&#x27;, element: &lt;AuthComponent&gt;&lt;Layout/&gt;&lt;/AuthComponent&gt;, children:[ &#123; index: true, // 作为初始展示 element: &lt;AuthComponent&gt;&lt;Home/&gt;&lt;/AuthComponent&gt;, &#125;, &#123; path: &#x27;article&#x27;, element: &lt;AuthComponent&gt;&lt;Article/&gt;&lt;/AuthComponent&gt;, &#125;, &#123; path: &#x27;publish&#x27;, element: &lt;AuthComponent&gt;&lt;Publish/&gt;&lt;/AuthComponent&gt;, &#125;, ] &#125;, &#123; path: &#x27;/login&#x27;, element: &lt;Login/&gt; &#125;,])export default router 1&lt;Layout className=&quot;layout-content&quot; style=&#123;&#123;padding: 20&#125;&#125;&gt;&lt;Outlet/&gt;&lt;/Layout&gt; 这里使用的是的方式的 但是在新版本中 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import React from &#x27;react&#x27;;import &#123; AppstoreOutlined, MailOutlined, SettingOutlined &#125; from &#x27;@ant-design/icons&#x27;;import &#123; Menu &#125; from &#x27;antd&#x27;;function getItem(label, key, icon, children, type) &#123; return &#123; key, icon, children, label, type, &#125;;&#125;const items = [ getItem(&#x27;Navigation One&#x27;, &#x27;sub1&#x27;, &lt;MailOutlined /&gt;, [ getItem(&#x27;Item 1&#x27;, &#x27;g1&#x27;, null, [getItem(&#x27;Option 1&#x27;, &#x27;1&#x27;), getItem(&#x27;Option 2&#x27;, &#x27;2&#x27;)], &#x27;group&#x27;), getItem(&#x27;Item 2&#x27;, &#x27;g2&#x27;, null, [getItem(&#x27;Option 3&#x27;, &#x27;3&#x27;), getItem(&#x27;Option 4&#x27;, &#x27;4&#x27;)], &#x27;group&#x27;), ]), getItem(&#x27;Navigation Two&#x27;, &#x27;sub2&#x27;, &lt;AppstoreOutlined /&gt;, [ getItem(&#x27;Option 5&#x27;, &#x27;5&#x27;), getItem(&#x27;Option 6&#x27;, &#x27;6&#x27;), getItem(&#x27;Submenu&#x27;, &#x27;sub3&#x27;, null, [getItem(&#x27;Option 7&#x27;, &#x27;7&#x27;), getItem(&#x27;Option 8&#x27;, &#x27;8&#x27;)]), ]), &#123; type: &#x27;divider&#x27;, &#125;, getItem(&#x27;Navigation Three&#x27;, &#x27;sub4&#x27;, &lt;SettingOutlined /&gt;, [ getItem(&#x27;Option 9&#x27;, &#x27;9&#x27;), getItem(&#x27;Option 10&#x27;, &#x27;10&#x27;), getItem(&#x27;Option 11&#x27;, &#x27;11&#x27;), getItem(&#x27;Option 12&#x27;, &#x27;12&#x27;), ]), getItem(&#x27;Group&#x27;, &#x27;grp&#x27;, null, [getItem(&#x27;Option 13&#x27;, &#x27;13&#x27;), getItem(&#x27;Option 14&#x27;, &#x27;14&#x27;)], &#x27;group&#x27;),];const App = () =&gt; &#123; const onClick = (e) =&gt; &#123; console.log(&#x27;click &#x27;, e); &#125;; return ( &lt;Menu onClick=&#123;onClick&#125; style=&#123;&#123; width: 256, &#125;&#125; defaultSelectedKeys=&#123;[&#x27;1&#x27;]&#125; defaultOpenKeys=&#123;[&#x27;sub1&#x27;]&#125; mode=&quot;inline&quot; items=&#123;items&#125; /&gt; );&#125;;export default App; 可以看到Menu变成了 一个组件的形式我们通过设置key为路由路径 再通过onclike绑定的方法 获取key 通过路由跳转 Menu布局 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071import &#123;Layout, Menu, message, Popconfirm&#125; from &#x27;antd&#x27;import &#123; HomeOutlined, DiffOutlined, EditOutlined, LogoutOutlined&#125; from &#x27;@ant-design/icons&#x27;import &#x27;./index.scss&#x27;import &#123;clearToken&#125; from &quot;../../utils/token.js&quot;;import &#123;Link, Outlet, useLocation, useNavigate&#125; from &quot;react-router-dom&quot;;const &#123;Header, Sider&#125; = Layoutconst GeekLayout = () =&gt; &#123; const navigate = useNavigate() // 获取当前路径 // const location = useLocation() const &#123;pathname&#125; = useLocation() const logout = () =&gt; &#123; clearToken() navigate(&#x27;/login&#x27;) message.success(&quot;那再见虽无奈~~~ &quot;) &#125; return ( // Layout 整体布局 &lt;Layout&gt; &#123;/* 头部 onConfirm 当点击ok以后的方法 写个函数来设置 */&#125; &lt;Header className=&quot;header&quot;&gt; &lt;div className=&quot;logo&quot;/&gt; &lt;div className=&quot;user-info&quot;&gt; &lt;span className=&quot;user-name&quot;&gt;user.name&lt;/span&gt; &lt;span className=&quot;user-logout&quot;&gt; &lt;Popconfirm title=&quot;是否确认退出？&quot; okText=&quot;退出&quot; cancelText=&quot;取消&quot; onConfirm=&#123;() =&gt; logout()&#125; &gt; &lt;LogoutOutlined/&gt; 退出 &lt;/Popconfirm&gt; &lt;/span&gt; &lt;/div&gt; &lt;/Header&gt; &lt;Layout&gt; &lt;Sider width=&#123;200&#125; className=&quot;site-layout-background&quot;&gt; &#123;/* Menu的高亮 取决于defaultSelectedKeys和以下key的绑定 */&#125; &lt;Menu mode=&quot;inline&quot; theme=&quot;dark&quot; defaultSelectedKeys=&#123;[pathname]&#125; style=&#123;&#123;height: &#x27;100%&#x27;, borderRight: 0&#125;&#125; &gt; &lt;Menu.Item icon=&#123;&lt;HomeOutlined/&gt;&#125; key=&quot;/&quot;&gt; &lt;Link to=&#123;&quot;/&quot;&#125;&gt;数据概览&lt;/Link&gt; &lt;/Menu.Item&gt; &lt;Menu.Item icon=&#123;&lt;DiffOutlined/&gt;&#125; key=&quot;/article&quot;&gt; &lt;Link to=&#123;&quot;/article&quot;&#125;&gt;内容管理&lt;/Link&gt; &lt;/Menu.Item&gt; &lt;Menu.Item icon=&#123;&lt;EditOutlined/&gt;&#125; key=&quot;/publish&quot;&gt; &lt;Link to=&#123;&quot;/publish&quot;&#125;&gt;发布文章&lt;/Link&gt; &lt;/Menu.Item&gt; &lt;/Menu&gt; &lt;/Sider&gt; &lt;Layout className=&quot;layout-content&quot; style=&#123;&#123;padding: 20&#125;&#125;&gt;&lt;Outlet/&gt;&lt;/Layout&gt; &lt;/Layout&gt; &lt;/Layout&gt; )&#125;export default GeekLayout 获取用户名称发请求 渲染这个请求要在渲染以后发一次 1234567891011121314151617// 用户模块import &#123; makeAutoObservable &#125; from &quot;mobx&quot;import &#123;http&#125; from &quot;../request/request.js&quot;;class UserStore &#123; userInfo = &#123;&#125; constructor() &#123; makeAutoObservable(this) &#125; getUserInfo = async () =&gt; &#123; const res = await http.get(&#x27;/user/profile&#x27;) this.userInfo = res.data console.log(this.userInfo.name) &#125;&#125;export default UserStore 1234567891011121314import React from &quot;react&quot;import LoginStore from &#x27;./login.Store&#x27;import UserStore from &quot;./user.Store.js&quot;;class RootStore &#123; // 组合模块 constructor() &#123; this.loginStore = new LoginStore() this.userStore = new UserStore() &#125;&#125;// 导入useStore方法供组件使用数据const StoresContext = React.createContext(new RootStore())export const useStore = () =&gt; React.useContext(StoresContext) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778import &#123;Layout, Menu, message, Popconfirm&#125; from &#x27;antd&#x27;import &#123; HomeOutlined, DiffOutlined, EditOutlined, LogoutOutlined&#125; from &#x27;@ant-design/icons&#x27;import &#x27;./index.scss&#x27;import &#123;clearToken&#125; from &quot;../../utils/token.js&quot;;import &#123;Link, Outlet, useLocation, useNavigate&#125; from &quot;react-router-dom&quot;;import &#123;useStore&#125; from &quot;../../store/index.js&quot;;import &#123;useEffect&#125; from &quot;react&quot;;import &#123;observer&#125; from &quot;mobx-react&quot;;const &#123;Header, Sider&#125; = Layoutconst GeekLayout = () =&gt; &#123; const navigate = useNavigate() // 获取当前路径 // const location = useLocation() const &#123;pathname&#125; = useLocation() const &#123;userStore&#125; = useStore() useEffect(() =&gt; &#123; userStore.getUserInfo() &#125;, [userStore]) const logout = () =&gt; &#123; clearToken() navigate(&#x27;/login&#x27;) message.success(&quot;那再见虽无奈~~~ &quot;) &#125; return ( // Layout 整体布局 &lt;Layout&gt; &#123;/* 头部 onConfirm 当点击ok以后的方法 写个函数来设置 */&#125; &lt;Header className=&quot;header&quot;&gt; &lt;div className=&quot;logo&quot;/&gt; &lt;div className=&quot;user-info&quot;&gt; &lt;span className=&quot;user-name&quot;&gt;&#123; userStore.userInfo.name &#125;&lt;/span&gt; &lt;span className=&quot;user-logout&quot;&gt; &lt;Popconfirm title=&quot;是否确认退出？&quot; okText=&quot;退出&quot; cancelText=&quot;取消&quot; onConfirm=&#123;() =&gt; logout()&#125; &gt; &lt;LogoutOutlined/&gt; 退出 &lt;/Popconfirm&gt; &lt;/span&gt; &lt;/div&gt; &lt;/Header&gt; &lt;Layout&gt; &lt;Sider width=&#123;200&#125; className=&quot;site-layout-background&quot;&gt; &#123;/* Menu的高亮 取决于defaultSelectedKeys和以下key的绑定 */&#125; &lt;Menu mode=&quot;inline&quot; theme=&quot;dark&quot; defaultSelectedKeys=&#123;[pathname]&#125; style=&#123;&#123;height: &#x27;100%&#x27;, borderRight: 0&#125;&#125; &gt; &lt;Menu.Item icon=&#123;&lt;HomeOutlined/&gt;&#125; key=&quot;/&quot;&gt; &lt;Link to=&#123;&quot;/&quot;&#125;&gt;数据概览&lt;/Link&gt; &lt;/Menu.Item&gt; &lt;Menu.Item icon=&#123;&lt;DiffOutlined/&gt;&#125; key=&quot;/article&quot;&gt; &lt;Link to=&#123;&quot;/article&quot;&#125;&gt;内容管理&lt;/Link&gt; &lt;/Menu.Item&gt; &lt;Menu.Item icon=&#123;&lt;EditOutlined/&gt;&#125; key=&quot;/publish&quot;&gt; &lt;Link to=&#123;&quot;/publish&quot;&#125;&gt;发布文章&lt;/Link&gt; &lt;/Menu.Item&gt; &lt;/Menu&gt; &lt;/Sider&gt; &lt;Layout className=&quot;layout-content&quot; style=&#123;&#123;padding: 20&#125;&#125;&gt;&lt;Outlet/&gt;&lt;/Layout&gt; &lt;/Layout&gt; &lt;/Layout&gt; )&#125;// 需要用observerexport default observer(GeekLayout) 这里学到了 所有的hook只能在函数组件内使用 也就是说 useNavigate()没办法在store 仓库中使用 处理登录失效token是会失效的 失效以后用户无法进行操作 因此这里需要续签 这里其实是后端应该做的 echarts 首页展示如何使用echarts? 导入依赖 导入官方用例 跑起来一个demo 进行修改 npm install echarts --save 12345678910111213141516171819202122232425262728293031323334353637383940import * as echarts from &#x27;echarts&#x27;;import &#123;useEffect, useRef&#125; from &quot;react&quot;;const Home = () =&gt; &#123; const domRef = useRef(); const chartInfo = () =&gt; &#123; // 基于准备好的dom，初始化echarts实例 var myChart = echarts.init(domRef.current);// 绘制图表 myChart.setOption(&#123; title: &#123; text: &#x27;ECharts 入门示例&#x27; &#125;, tooltip: &#123;&#125;, xAxis: &#123; data: [&#x27;衬衫&#x27;, &#x27;羊毛衫&#x27;, &#x27;雪纺衫&#x27;, &#x27;裤子&#x27;, &#x27;高跟鞋&#x27;, &#x27;袜子&#x27;] &#125;, yAxis: &#123;&#125;, series: [ &#123; name: &#x27;销量&#x27;, type: &#x27;bar&#x27;, data: [5, 20, 36, 10, 10, 20] &#125; ] &#125;); &#125; useEffect(() =&gt; &#123; chartInfo() &#125;,[]) return &lt;&gt; &lt;div&gt; &lt;div ref=&#123;domRef&#125; style=&#123;&#123;height:&#x27;500px&#x27;&#125;&#125;&gt;&lt;/div&gt; &lt;/div&gt; &lt;/&gt;&#125;export default Home","categories":[{"name":"前端","slug":"前端","permalink":"http://example.com/categories/%E5%89%8D%E7%AB%AF/"}],"tags":[]},{"title":"react","slug":"React","date":"2023-07-31T09:32:40.486Z","updated":"2023-07-31T09:33:03.332Z","comments":true,"path":"2023/07/31/React/","link":"","permalink":"http://example.com/2023/07/31/React/","excerpt":"","text":"React基础React介绍目标任务: 了解什么是React以及它的特点 React是什么 一个专注于构建用户界面的 JavaScript 库，和vue和angular并称前端三大框架，不夸张的说，react引领了很多新思想，世界范围内是最流行的js前端框架，最新版本已经到了18，加入了许多很棒的新特性 React英文文档（https://reactjs.org/） React中文文档 （https://zh-hans.reactjs.org/） React新文档（https://beta.reactjs.org/）（开发中....） React有什么特点1- 声明式UI（JSX）写UI就和写普通的HTML一样，抛弃命令式的繁琐实现 2- 组件化组件是react中最重要的内容，组件可以通过搭积木的方式拼成一个完整的页面，通过组件的抽象可以增加复用能力和提高可维护性 3- 跨平台react既可以开发web应用也可以使用同样的语法开发原生应用（react-native），比如安卓和ios应用，甚至可以使用react开发VR应用，想象力空间十足，react更像是一个 元框架 为各种领域赋能 环境初始化目标任务: 能够独立使用React脚手架创建一个react项目 1. 使用脚手架创建项目 打开命令行窗口 （当然 必须要装node.js） 执行命令 1$ npx create-react-app react-basic 说明： npx create-react-app 是固定命令，create-react-app是React脚手架的名称 react-basic表示项目名称，可以自定义，保持语义化 npx 命令会帮助我们临时安装create-react-app包，然后初始化项目完成之后会自自动删掉，所以不需要全局安装create-react-app 启动项目 123$ yarn startor$ npm start 2. 项目目录说明调整 目录说明 src 目录是我们写代码进行项目开发的目录 package.json 中俩个核心库：react 、react-dom 目录调整 删除src目录下自带的所有文件，只保留app.js根组件和index.js 创建index.js文件作为项目的入口文件，在这个文件中书写react代码即可 入口文件说明 12345678910111213import React from &#x27;react&#x27;; // 核心包import ReactDOM from &#x27;react-dom/client&#x27;; // 专门做渲染import &#x27;./index.css&#x27; // 一般来说是 全局的样式import App from &#x27;./App&#x27;; // 引入根组件 一切组件的根 想一想vue中的app// 渲染组件app 到id为root的dom节点上 这个root 在public/index.html中// 说白了就是把app渲染到了 这个div下const root = ReactDOM.createRoot(document.getElementById(&#x27;root&#x27;));root.render( // 注释严格模式 不然会影响 useEffect的时机 // &lt;React.StrictMode&gt; &lt;App /&gt; // &lt;/React.StrictMode&gt;); JSX基础1. JSX介绍目标任务: 能够理解什么是JSX，JSX的底层是什么 概念：JSX是 JavaScript XML（HTML）的缩写，表示在 JS 代码中书写 HTML 结构作用：在React中创建HTML结构（页面UI结构）优势： 采用类似于HTML的语法，降低学习成本，会HTML就会JSX 充分利用JS自身的可编程能力创建HTML结构 注意：JSX 并不是标准的 JS 语法，是 JS 的语法扩展，浏览器默认是不识别的，脚手架中内置的 @babel&#x2F;plugin-transform-react-jsx 包，用来解析该语法 1234567function App() &#123; // 这就是jsx 是js和html的结合 return ( &lt;div&gt; hello &lt;/div&gt; );&#125; 2. JSX中使用js表达式目标任务: 能够在JSX中使用表达式语法&#123; JS 表达式 &#125; 123456789101112131415// 变量const a1 = &#x27;Hello jsx&#x27;// 方法const getAge = () =&gt; 18function App() &#123; return ( &lt;div&gt; &#123;/* 使用&#123;&#125;来接收任意合法的js表达式 这里就是类似vue的&#123;&#123;&#125;&#125; */&#125; &#123; a1 &#125;+ &#123; getAge() &#125; + &#123;&#x27;&#x27; ? 2:3&#125; &lt;/div&gt; );&#125;export default App; 可以使用的表达式 字符串、数值、布尔值、null、undefined、object（ [] &#x2F; {} ） 1 + 2、’abc’.split(‘’)、[‘a’, ‘b’].join(‘-‘) fn() 特别注意 if 语句&#x2F; switch-case 语句&#x2F; 变量声明语句，这些叫做语句，不是表达式，不能出现在 &#123;&#125; 中！！ 3. JSX列表渲染目标任务: 能够在JSX中实现列表渲染 页面的构建离不开重复的列表结构，比如歌曲列表，商品列表等，我们知道vue中用的是v-for，react这边如何实现呢？ 实现：使用数组的map 方法 123456789101112131415161718192021// 来个列表const songs = [ &#123; id: 1, name: &#x27;痴心绝对&#x27; &#125;, &#123; id: 2, name: &#x27;像我这样的人&#x27; &#125;, &#123; id: 3, name: &#x27;南山南&#x27; &#125;]function App() &#123; return ( &lt;div className=&quot;App&quot;&gt; &lt;ul&gt; &#123;/* 需要重复展示的是谁 就return谁 并且遍历时候需要一个独一无二的key key仅仅在虚拟dom中使用*/&#125; &lt;ul&gt; &#123;songs.map(song =&gt; &lt;li key=&#123;song.id&#125;&gt;&#123;song.name&#125;&lt;/li&gt;)&#125; &lt;/ul&gt; &lt;/ul&gt; &lt;/div&gt; )&#125;export default App 注意点：需要为遍历项添加 key 属性 key 在 HTML 结构中是看不到的，是 React 内部用来进行性能优化时使用 key 在当前列表中要唯一的字符串或者数值（String&#x2F;Number） 如果列表中有像 id 这种的唯一值，就用 id 来作为 key 值 如果列表中没有像 id 这种的唯一值，就可以使用 index（下标）来作为 key 值 4. JSX条件渲染目标任务: 能够在JSX中实现条件渲染 作用：根据是否满足条件生成HTML结构，比如Loading效果实现：可以使用 三元运算符 或 逻辑与(&amp;&amp;)运算符 12345678910111213// 来个布尔值const flag = truefunction App() &#123; return ( &lt;div className=&quot;App&quot;&gt; &#123;/* 条件渲染字符串 */&#125; &#123;flag ? &#x27;react真有趣&#x27; : &#x27;vue真有趣&#x27;&#125; &#123;/* 条件渲染标签/组件 */&#125; &#123;flag ? &lt;span&gt;this is span&lt;/span&gt; : null&#125; &lt;/div&gt; )&#125;export default App 如果是多个分支如何处理？ 写一个函数 函数中写逻辑 模板中调用函数即可 越来越像后端开发了 1234567891011121314151617181920212223const getHx = (type) =&gt; &#123; if (type === 1) &#123; return &lt;h1&gt;虽然~&lt;/h1&gt; &#125; if (type === 2) &#123; return &lt;h2&gt;baby blue&lt;/h2&gt; &#125; if (type === 3) &#123; return &lt;h3&gt;only blue&lt;/h3&gt; &#125;&#125;function App() &#123; return ( &lt;div&gt; &#123;true ? (&lt;div&gt; &lt;span&gt;虽然~&lt;/span&gt; &lt;/div&gt;) : null &#125; &#123; getHx(1)&#125; &#123; getHx(2)&#125; &#123; getHx(3)&#125; &lt;/div&gt; );&#125; 5. JSX样式处理目标任务: 能够在JSX中实现css样式处理 行内样式 - style 123456789function App() &#123; return ( &lt;div className=&quot;App&quot;&gt; &lt;div style=&#123;&#123; color: &#x27;red&#x27; &#125;&#125;&gt;this is a div&lt;/div&gt; &lt;/div&gt; )&#125;export default App 行内样式 - style - 更优写法 12345678910111213const styleObj = &#123; color:&#x27;red&#x27;&#125;function App() &#123; return ( &lt;div className=&quot;App&quot;&gt; &lt;div style=&#123; styleObj &#125;&gt;this is a div&lt;/div&gt; &lt;/div&gt; )&#125;export default App 类名 - className（推荐） 1234.title &#123; font-size: 30px; color: blue;&#125; 类名 - className - 动态类名控制 12345678910import &#x27;./app.css&#x27; // 首先需要导入const showTitle = truefunction App() &#123; return ( &lt;div className=&quot;App&quot;&gt; &lt;div className=&#123; showTitle ? &#x27;title&#x27; : &#x27;&#x27;&#125;&gt;this is a div&lt;/div&gt; &lt;/div&gt; )&#125;export default App 6. JSX注意事项目标任务: 掌握JSX在实际应用时的注意事项 JSX必须有一个根节点，如果没有根节点，可以使用&lt;&gt;&lt;/&gt;（幽灵节点）替代 123456789101112function App() &#123; return ( // 两个并列 就报错 因为只允许存在一个根 /*&lt;div&gt;app&lt;/div&gt; &lt;div&gt;app&lt;/div&gt;*/ // 解决 或者 外层再包一层div &lt;&gt; &lt;div&gt;2&lt;/div&gt; &lt;div&gt;3&lt;/div&gt; &lt;/&gt; );&#125; 所有标签必须形成闭合，成对闭合或者自闭合都可以（标签必须成对） JSX中的语法更加贴近JS语法，属性名采用驼峰命名法 class -&gt; className for -&gt; htmlFor JSX支持多行（换行），如果需要换行，需使用**()**** 包裹，防止bug出现** 阶段小练习 练习说明 拉取准备好的项目模块到本地 ，安装依赖，run起来项目https://gitee.com/react-course-series/react-jsx-demo 按照图示，完成 评论数据渲染 tab内容渲染 评论列表点赞和点踩 三个视图渲染 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118import &#x27;./index.css&#x27;import avatar from &#x27;./images/avatar.png&#x27;// 依赖的数据const state = &#123; // hot: 热度排序 time: 时间排序 tabs: [ &#123; id: 1, name: &#x27;热度&#x27;, type: &#x27;hot&#x27; &#125;, &#123; id: 2, name: &#x27;时间&#x27;, type: &#x27;time&#x27; &#125; ], active: &#x27;hot&#x27;, list: [ &#123; id: 1, author: &#x27;刘德华&#x27;, comment: &#x27;给我一杯忘情水&#x27;, time: new Date(&#x27;2021-10-10 09:09:00&#x27;), // 1: 点赞 0：无态度 -1:踩 attitude: 1 &#125;, &#123; id: 2, author: &#x27;周杰伦&#x27;, comment: &#x27;哎哟，不错哦&#x27;, time: new Date(&#x27;2021-10-11 09:09:00&#x27;), // 1: 点赞 0：无态度 -1:踩 attitude: 0 &#125;, &#123; id: 3, author: &#x27;五月天&#x27;, comment: &#x27;不打扰，是我的温柔&#x27;, time: new Date(&#x27;2021-10-11 10:09:00&#x27;), // 1: 点赞 0：无态度 -1:踩 attitude: -1 &#125; ]&#125;const formatTime = (time) =&gt; &#123; // 格式化时间 return `$&#123;time.getFullYear()&#125;-$&#123;time.getMonth()+1&#125;-$&#123;time.getDay()&#125;`;&#125;function App () &#123; return ( &lt;div className=&quot;App&quot;&gt; &lt;div className=&quot;comment-container&quot;&gt; &#123;/* 评论数 */&#125; &lt;div className=&quot;comment-head&quot;&gt; &lt;span&gt;5 评论&lt;/span&gt; &lt;/div&gt; &#123;/* 排序 */&#125; &lt;div className=&quot;tabs-order&quot;&gt; &lt;ul className=&quot;sort-container&quot;&gt; &#123; state.tabs.map(tab =&gt; &lt;li key=&#123;tab.id&#125; className=&#123;tab.type === state.active ? &#x27;on&#x27; : &#x27;&#x27;&#125;&gt;按&#123;tab.name&#125;排行&lt;/li&gt;) &#125; &lt;/ul&gt; &lt;/div&gt; &#123;/* 添加评论 */&#125; &lt;div className=&quot;comment-send&quot;&gt; &lt;div className=&quot;user-face&quot;&gt; &lt;img className=&quot;user-head&quot; src=&#123;avatar&#125; alt=&quot;&quot; /&gt; &lt;/div&gt; &lt;div className=&quot;textarea-container&quot;&gt; &lt;textarea cols=&quot;80&quot; rows=&quot;5&quot; placeholder=&quot;发条友善的评论&quot; className=&quot;ipt-txt&quot; /&gt; &lt;button className=&quot;comment-submit&quot;&gt;发表评论&lt;/button&gt; &lt;/div&gt; &lt;div className=&quot;comment-emoji&quot;&gt; &lt;i className=&quot;face&quot;&gt;&lt;/i&gt; &lt;span className=&quot;text&quot;&gt;表情&lt;/span&gt; &lt;/div&gt; &lt;/div&gt; &#123;/* 评论列表 */&#125; &lt;div className=&quot;comment-list&quot;&gt; &#123; state.list.map(item =&gt; &#123; return (&lt;div className=&quot;list-item&quot; key=&#123;item.id&#125;&gt; &lt;div className=&quot;user-face&quot;&gt; &lt;img className=&quot;user-head&quot; src=&#123;avatar&#125; alt=&quot;&quot; /&gt; &lt;/div&gt; &lt;div className=&quot;comment&quot;&gt; &lt;div className=&quot;user&quot;&gt;&#123; item.author &#125;&lt;/div&gt; &lt;p className=&quot;text&quot;&gt;&#123; item.comment &#125;&lt;/p&gt; &lt;div className=&quot;info&quot;&gt; &#123;/*如何 格式化时间？ 写一个函数*/&#125; &lt;span className=&quot;time&quot;&gt;&#123; formatTime(item.time) &#125;&lt;/span&gt; &#123;/*点赞 like代表yes hated代表no*/&#125; &lt;span className=&#123; item.attitude === 1 ? &#x27;like liked&#x27; : &#x27;like&#x27; &#125;&gt; &lt;i className=&quot;icon&quot; /&gt; &lt;/span&gt; &lt;span className=&#123; item.attitude === -1 ? &#x27;hate hated&#x27; : &#x27;hate&#x27; &#125;&gt; &lt;i className=&quot;icon&quot; /&gt; &lt;/span&gt; &lt;span className=&quot;reply btn-hover&quot;&gt;删除&lt;/span&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;) &#125;) &#125; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; )&#125;export default App React组件基础组件概念 函数组件目标任务: 能够独立使用函数完成react组件的创建和渲染 概念 使用 JS 的函数（或箭头函数）创建的组件，就叫做函数组件 组件定义与渲染 12345678910111213141516// 定义函数组件function HelloFn () &#123; return &lt;div&gt;这是我的第一个函数组件!&lt;/div&gt;&#125;// 定义类组件function App () &#123; return ( &lt;div className=&quot;App&quot;&gt; &#123;/* 渲染函数组件 */&#125; &lt;HelloFn /&gt; &lt;HelloFn&gt;&lt;/HelloFn&gt; &lt;/div&gt; )&#125;export default App 约定说明 组件的名称必须首字母大写，react内部会根据这个来判断是组件还是普通的HTML标签 函数组件必须有返回值，表示该组件的 UI 结构（html模板）；如果不需要渲染任何内容，则返回 null 组件就像 HTML 标签一样可以被渲染到页面中。组件表示的是一段结构内容，对于函数组件来说，渲染的内容是函数的返回值就是对应的内容 使用函数名称作为组件标签名称，可以成对出现也可以自闭合 类组件目标任务: 能够独立完成类组件的创建和渲染 使用 ES6 的 class 创建的组件，叫做类（class）组件 组件定义与渲染 1234567891011121314151617181920// 引入Reactimport React from &#x27;react&#x27;// 定义类组件class HelloC extends React.Component &#123; render () &#123; return &lt;div&gt;这是我的第一个类组件!&lt;/div&gt; &#125;&#125;function App () &#123; return ( &lt;div className=&quot;App&quot;&gt; &#123;/* 渲染类组件 */&#125; &lt;HelloC /&gt; &lt;HelloC&gt;&lt;/HelloC&gt; &lt;/div&gt; )&#125;export default App 约定说明 类名称也必须以大写字母开头 类组件应该继承 React.Component 父类，从而使用父类中提供的方法或属性 类组件必须提供 render 方法render 方法必须有返回值，表示该组件的 UI 结构 函数组件的事件绑定目标任务: 能够独立绑定任何事件并能获取到事件对象e 1. 如何绑定事件 语法on + 事件名称 &#x3D; { 事件处理程序 } ，比如：&lt;div onClick=&#123; onClick &#125;&gt;&lt;/div&gt; 注意点react事件采用驼峰命名法，比如：onMouseEnter、onFocus 样例 1234567891011// 函数组件function HelloFn () &#123; // 定义事件回调函数 const clickHandler = () =&gt; &#123; console.log(&#x27;事件被触发了&#x27;) &#125; return ( // 绑定事件 &lt;button onClick=&#123;clickHandler&#125;&gt;click me!&lt;/button&gt; )&#125; 12345678910class HelloClassCom extends React.Component&#123; // 标准写法 clickB = () =&gt; &#123; console.log(&#x27;被点击&#x27;) &#125; // 这里的this就是指的当前组件 render() &#123; return &lt;button onClick=&#123;this.clickB&#125;&gt;这是一个类组件 继承React.Component 并且需要在render中获取写&lt;/button&gt; &#125;&#125; 2. 获取事件对象 获取事件对象e只需要在 事件的回调函数中 补充一个形参e即可拿到 1234567891011// 函数组件function HelloFn () &#123; // 定义事件回调函数 const clickHandler = (e) =&gt; &#123; console.log(&#x27;事件被触发了&#x27;, e) &#125; return ( // 绑定事件 &lt;button onClick=&#123;clickHandler&#125;&gt;click me!&lt;/button&gt; )&#125; 3. 传递额外参数 解决思路: 改造事件绑定为箭头函数 在箭头函数中完成参数的传递 12345678910111213141516171819202122232425262728293031323334353637383940414243import React from &quot;react&quot;// 如何获取额外的参数？// onClick=&#123; onDel &#125; -&gt; onClick=&#123; () =&gt; onDel(id) &#125; 这是只需要一个参数的写法// 注意: 一定不要在模板中写出函数调用的代码 onClick = &#123; onDel(id) &#125; bad!!!!!!// 如果既需要e，也要参数 (e) =&gt; xxx(e, xxx) 别忘了调用的方法要声明econst TestComponent = () =&gt; &#123; const list = [ &#123; id: 1001, name: &#x27;react&#x27; &#125;, &#123; id: 1002, name: &#x27;vue&#x27; &#125; ] const onDel = (e, id) =&gt; &#123; console.log(e, id) &#125; return ( &lt;ul&gt; &#123;list.map(item =&gt;（ &lt;li key=&#123;item.id&#125;&gt; &#123;item.name&#125; &lt;button onClick=&#123;(e) =&gt; onDel(e, item.id)&#125;&gt;x&lt;/button&gt; &lt;/li&gt; ))&#125; &lt;/ul&gt; )&#125;function App () &#123; return ( &lt;div&gt; &lt;TestComponent /&gt; &lt;/div&gt; )&#125;export default App 类组件的事件绑定 类组件中的事件绑定，整体的方式和函数组件差别不大唯一需要注意的 因为处于class类语境下 所以定义事件回调函数以及定它写法上有不同 定义的时候: class Fields语法 使用的时候: 需要借助this关键词获取 1234567891011121314151617181920212223242526272829303132333435import React from &quot;react&quot;class CComponent extends React.Component &#123; // class Fields clickHandler = (e, num) =&gt; &#123; // 这里的this指向的是正确的当前的组件实例对象 // 可以非常方便的通过this关键词拿到组件实例身上的其他属性或者方法 console.log(this) &#125; clickHandler1 () &#123; // 这里的this 不指向当前的组件实例对象而指向undefined 存在this丢失问题 console.log(this) &#125; render () &#123; return ( &lt;div&gt; &lt;button onClick=&#123;(e) =&gt; this.clickHandler(e, &#x27;123&#x27;)&#125;&gt;click me&lt;/button&gt; &lt;button onClick=&#123;this.clickHandler1&#125;&gt;click me&lt;/button&gt; &lt;/div&gt; ) &#125;&#125;function App () &#123; return ( &lt;div&gt; &lt;CComponent /&gt; &lt;/div&gt; )&#125;export default App 组件状态目标任务: 能够为组件添加状态和修改状态的值 一个前提：在React hook出来之前，函数式组件是没有自己的状态的，所以我们统一通过类组件来讲解 1. 初始化状态 通过class的实例属性state来初始化 ** state的值是一个对象结构，表示一个组件可以有多个数据状态 **123456789class Counter extends React.Component &#123; // 初始化状态 state = &#123; count: 0 &#125; render() &#123; return &lt;button&gt;计数器&lt;/button&gt; &#125;&#125; 2. 读取状态 通过this.state来获取状态 12345678910class Counter extends React.Component &#123; // 初始化状态 state = &#123; count: 0 &#125; render() &#123; // 读取状态 return &lt;button&gt;计数器&#123;this.state.count&#125;&lt;/button&gt; &#125;&#125; 3. 修改状态 语法this.setState(&#123; 要修改的部分数据 &#125;) setState方法作用 修改state中的数据状态 更新UI 思想 数据驱动视图，也就是只要修改数据状态，那么页面就会自动刷新，无需手动操作dom 注意事项 不要直接修改state中的值，必须通过setState方法进行修改 1234567891011121314151617// 这就是class fields写法class Counter extends React.Component &#123; // 定义数据 state = &#123; count: 0 &#125; // 定义修改数据的方法 setCount = () =&gt; &#123; this.setState(&#123; count: this.state.count + 1 &#125;) &#125; // 使用数据 并绑定事件 render () &#123; return &lt;button onClick=&#123;this.setCount&#125;&gt;&#123;this.state.count&#125;&lt;/button&gt; &#125;&#125; this问题说明 这里我们作为了解内容，随着js标准的发展，主流的写法已经变成了class fields（即以上写法 创建方法直接使用箭头函数），无需考虑太多this问题 js小知识&amp;&amp;： 只有前面的成立了 才会执行后面 短路运算map: 遍历 React的状态不可变目标任务: 能够理解不可变的意义并且知道在实际开发中如何修改状态概念：不要直接修改状态的值，而是基于当前状态创建新的状态值1. 错误的直接修改 （全部都不行） 1234567891011121314151617181920state = &#123; count : 0, list: [1,2,3], person: &#123; name:&#x27;jack&#x27;, age:18 &#125;&#125;// 直接修改简单类型Numberthis.state.count++++this.state.countthis.state.count += 1this.state.count = 1// 直接修改数组this.state.list.push(123)this.state.list.spice(1,1)// 直接修改对象this.state.person.name = &#x27;rose&#x27; 2. 基于当前状态创建新值（一定要写一个方法 其中修改对象内容） 123456789this.setState(&#123; count: this.state.count + 1 list: [...this.state.list, 4], person: &#123; ...this.state.person, // 覆盖原来的属性 就可以达到修改对象中属性的目的 name: &#x27;rose&#x27; &#125;&#125;) 通过filter方法 使得 不等于2的数组赋值给原来的数组 表单处理目标任务: 能够使用受控组件的方式获取文本框的值 使用React处理表单元素，一般有俩种方式： 受控组件 （推荐使用） 非受控组件 （了解） 1. 受控表单组件 什么是受控组件？ input框自己的状态被React组件状态控制React组件的状态的地方是在state中，input表单元素也有自己的状态是在value中，React将state与表单元素的值（value）绑定到一起，由state的值来控制表单元素的值，从而保证单一数据源特性 实现步骤以获取文本框的值为例，受控组件的使用步骤如下： 在组件的state中声明一个组件的状态数据 将状态数据设置为input标签元素的value属性的值 为input添加change事件，在事件处理程序中，通过事件对象e获取到当前文本框的值（即用户当前输入的值） 调用setState方法，将文本框的值作为state状态的最新值 这里其实就是 vue中的V-model 双向绑定代码落地 12345678910111213141516171819202122232425262728293031import React from &#x27;react&#x27;class Counter extends React.Component&#123; // 声明用来控制 input value的react组件自己的状态 state = &#123; // 这里需要是state message : &#x27;this is msg&#x27; &#125; inputChange = (e) =&gt; &#123; // 这个方法 要做的事 将用户在输入框中输入的 数据 赋值给stata.message // 如果获取输入框的值？ 通过e 这个事件元素获取 this.setState(&#123; message : e.target.value &#125;) &#125; render() &#123; // value是为了和 自定义的属性绑定 onChange是当改变以后调用这个方法 使得用户键入赋值给message // 对于像 &lt;input type=&quot;text&quot;&gt; 这样的元素，change 事件在控件失去焦点前都不会触发。试一下在下面的输入框输入一些文字，然后点击输入框外的地方来触发事件。 return &lt;input type=&quot;text&quot; value=&#123;this.state.message&#125; onChange=&#123;this.inputChange&#125;&gt;&lt;/input&gt; &#125;&#125;function App() &#123; return ( &lt;div&gt; &lt;Counter/&gt; &lt;/div&gt; );&#125;export default App; 2. 非受控表单组件 什么是非受控组件？非受控组件就是通过手动操作dom的方式获取文本框的值，文本框的状态不受react组件的state中的状态控制，直接通过原生dom获取输入框的值 实现步骤 导入createRef 函数 调用createRef函数，创建一个ref对象，存储到名为msgRef的实例属性中 为input添加ref属性，值为msgRef 在按钮的事件处理程序中，通过msgRef.current即可拿到input对应的dom元素，而其中msgRef.current.value拿到的就是文本框的值 代码落地 1234567891011121314151617181920212223242526272829import React, &#123; createRef &#125; from &#x27;react&#x27;class InputComponent extends React.Component &#123; // 使用createRef产生一个存放dom的对象容器 msgRef = createRef() changeHandler = () =&gt; &#123; console.log(this.msgRef.current.value) &#125; render() &#123; return ( &lt;div&gt; &#123;/* ref绑定 获取真实dom */&#125; &lt;input ref=&#123;this.msgRef&#125; /&gt; &lt;button onClick=&#123;this.changeHandler&#125;&gt;click&lt;/button&gt; &lt;/div&gt; ) &#125;&#125;function App () &#123; return ( &lt;div className=&quot;App&quot;&gt; &lt;InputComponent /&gt; &lt;/div&gt; )&#125;export default App 阶段小练习 练习说明 拉取项目模板到本地，安装依赖，run起来项目https://gitee.com/react-course-series/react-component-demo 完成tab点击切换激活状态交互 完成发表评论功能注意：生成独立无二的id 可以使用 uuid 包 yarn add uuid 12import &#123; v4 as uuid &#125; from &#x27;uuid&#x27;uuid() // 得到一个独一无二的id 完成删除评论功能 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184import &#x27;./index.css&#x27;import avatar from &#x27;./images/avatar.png&#x27;import React from &#x27;react&#x27;import &#123;v4 as uuid&#125; from &quot;uuid&quot;;// 时间格式化function formatDate (time) &#123; return `$&#123;time.getFullYear()&#125;-$&#123;time.getMonth()&#125;-$&#123;time.getDate()&#125;`&#125;class App extends React.Component &#123; state = &#123; // hot: 热度排序 time: 时间排序 tabs: [ &#123; id: 1, name: &#x27;热度&#x27;, type: &#x27;hot&#x27; &#125;, &#123; id: 2, name: &#x27;时间&#x27;, type: &#x27;time&#x27; &#125; ], active: &#x27;hot&#x27;, list: [ &#123; id: 1, author: &#x27;刘德华&#x27;, comment: &#x27;给我一杯忘情水&#x27;, time: new Date(&#x27;2021-10-10 09:09:00&#x27;), // 1: 点赞 0：无态度 -1:踩 attitude: 1 &#125;, &#123; id: 2, author: &#x27;周杰伦&#x27;, comment: &#x27;哎哟，不错哦&#x27;, time: new Date(&#x27;2021-10-11 09:09:00&#x27;), // 1: 点赞 0：无态度 -1:踩 attitude: 0 &#125;, &#123; id: 3, author: &#x27;五月天&#x27;, comment: &#x27;不打扰，是我的温柔&#x27;, time: new Date(&#x27;2021-10-11 10:09:00&#x27;), // 1: 点赞 0：无态度 -1:踩 attitude: -1 &#125; ], msg:&#x27;&#x27; &#125; switchTab = (type) =&gt; &#123; // 这里需要的是 更改active的值 这个值是控制样式的关键 this.setState(&#123; // 只需要判断 原本的类型是什么 取相反即可 // active : this.state.active === &quot;hot&quot; ? &#x27;time&#x27; : &#x27;hot&#x27; // 也可以采用传值的方式 active : type &#125;) &#125; getTextarea = (e) =&gt; &#123; this.setState(&#123; // 这里需要的获取数据 更新到展示列的数组 msg: e.target.value &#125;) &#125; submitText = () =&gt; &#123; this.setState(&#123; list : [...this.state.list, &#123; id: uuid(), // uuid 这里不用纠结 一般来说 都是向后台获取数据 需要加入依赖 npm add uuid author: &#x27;dt 陶喆&#x27;, comment: this.state.msg, time: new Date(&#x27;2023-90-88 34:91:97&#x27;), // 1: 点赞 0：无态度 -1:踩 attitude: -1 &#125;], msg: &#x27;&#x27; // 这是为了每次执行完 清空 &#125;) &#125; deleteById = (id) =&gt; &#123; // 根据id 直接请出去数组 this.setState(&#123; list : this.state.list.filter(item =&gt; item.id !== id) &#125;) &#125; goodman = (item) =&gt; &#123; this.setState(&#123; list : this.state.list.map(i =&gt; &#123; if (item.id === i.id) &#123; return&#123; ...i, attitude : item.attitude === 1 ? 0 : 1 &#125; &#125;else &#123; return &#123; item &#125; &#125; &#125;) &#125;) &#125; render () &#123; return ( &lt;div className=&quot;App&quot;&gt; &lt;div className=&quot;comment-container&quot;&gt; &#123;/* 评论数 */&#125; &lt;div className=&quot;comment-head&quot;&gt; &lt;span&gt;5 评论&lt;/span&gt; &lt;/div&gt; &#123;/* 排序 */&#125; &lt;div className=&quot;tabs-order&quot;&gt; &lt;ul className=&quot;sort-container&quot;&gt; &#123; // map遍历了所有 当点击哪个 type类型就是哪个 this.state.tabs.map(tab =&gt; ( &lt;li onClick=&#123;() =&gt; this.switchTab(tab.type)&#125; key=&#123;tab.id&#125; className=&#123;tab.type === this.state.active ? &#x27;on&#x27; : &#x27;&#x27;&#125; &gt;按&#123;tab.name&#125;排序&lt;/li&gt; )) &#125; &lt;/ul&gt; &lt;/div&gt; &#123;/* 添加评论 */&#125; &lt;div className=&quot;comment-send&quot;&gt; &lt;div className=&quot;user-face&quot;&gt; &lt;img className=&quot;user-head&quot; src=&#123;avatar&#125; alt=&quot;&quot; /&gt; &lt;/div&gt; &lt;div className=&quot;textarea-container&quot;&gt; &#123;/* 发表评论 就是往数组里添加一列数据 首先我们需要有一个接收数据的数据 类似vue中的响应式对象 要把这个对象绑定*/&#125; &lt;textarea cols=&quot;80&quot; rows=&quot;5&quot; placeholder=&quot;发条友善的评论&quot; className=&quot;ipt-txt&quot; value=&#123;this.state.msg&#125; onChange=&#123;this.getTextarea&#125; /&gt; &lt;button className=&quot;comment-submit&quot; onClick=&#123;this.submitText&#125;&gt;发表评论&lt;/button&gt; &lt;/div&gt; &lt;div className=&quot;comment-emoji&quot;&gt; &lt;i className=&quot;face&quot;&gt;&lt;/i&gt; &lt;span className=&quot;text&quot;&gt;表情&lt;/span&gt; &lt;/div&gt; &lt;/div&gt; &#123;/* 评论列表 */&#125; &lt;div className=&quot;comment-list&quot;&gt; &#123; this.state.list.map(item =&gt; ( &lt;div className=&quot;list-item&quot; key=&#123;item.id&#125;&gt; &lt;div className=&quot;user-face&quot;&gt; &lt;img className=&quot;user-head&quot; src=&#123;avatar&#125; alt=&quot;&quot; /&gt; &lt;/div&gt; &lt;div className=&quot;comment&quot;&gt; &lt;div className=&quot;user&quot;&gt;&#123;item.author&#125;&lt;/div&gt; &lt;p className=&quot;text&quot;&gt;&#123;item.comment&#125;&lt;/p&gt; &lt;div className=&quot;info&quot;&gt; &lt;span className=&quot;time&quot;&gt;&#123;formatDate(item.time)&#125;&lt;/span&gt; &lt;span className=&#123;item.attitude === 1 ? &#x27;like liked&#x27; : &#x27;like&#x27;&#125; onClick=&#123;() =&gt; this.goodman(item)&#125;&gt; &lt;i className=&quot;icon&quot; /&gt; &lt;/span&gt; &lt;span className=&#123;item.attitude === -1 ? &#x27;hate hated&#x27; : &#x27;hate&#x27;&#125;&gt; &lt;i className=&quot;icon&quot; /&gt; &lt;/span&gt; &lt;span className=&quot;reply btn-hover&quot; onClick=&#123;() =&gt; this.deleteById(item.id)&#125;&gt;删除&lt;/span&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; )) &#125; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;) &#125; &#125; export default App React组件通信组件通信的意义目标任务: 了解为什么需要组件通信 组件是独立且封闭的单元，默认情况下组件只能使用自己的数据（state）组件化开发的过程中，完整的功能会拆分多个组件，在这个过程中不可避免的需要互相传递一些数据为了能让各组件之间可以进行互相沟通，数据传递，这个过程就是组件通信 父子关系 - 最重要的 props 兄弟关系 - 自定义事件模式产生技术方法 eventBus &#x2F; 通过共同的父组件通信 其它关系 - mobx &#x2F; redux &#x2F; zustand 父传子实现目标任务: 实现父子通信中的父传子，把父组件中的数据传给子组件vue中 父传子 使用的自定义属性和 props &lt;组件A c&#x3D;”xxxxx”&gt; 组件A中使用props定义属性 接收父传子 自然是子组件定义 父组件实现步骤 父组件提供要传递的数据 - state 给子组件标签添加属性值为 state中的数据 子组件中通过 props 接收父组件中传过来的数据 类组件使用this.props获取props对象 函数式组件直接通过参数获取props对象 代码实现 1234567891011121314151617181920212223242526272829303132333435363738import React from &#x27;react&#x27;// 定义子组件// 函数// 函数式 接收父传递的数据 只需要 props参数 即可function FSon(props) &#123; return &lt;div&gt;函数 子组件 &#123;props.msg&#125;&lt;/div&gt;&#125;// 类组件class CSon extends React.Component&#123; render() &#123; // 类组件接收 return &lt;div&gt;类组件 &#123;this.props.msg&#125;&lt;/div&gt; &#125;&#125;// 定义一个父组件class App extends React.Component&#123; // 定义父组件中的参数 用于传递给子组件 state = &#123; msg : &quot;给子组件&quot; &#125; render() &#123; return (&lt;div&gt; &#123;/* 传递信息 使用自定义属性 */&#125; &lt;FSon msg=&#123;this.state.msg&#125;/&gt; &lt;CSon msg=&#123;this.state.msg&#125;/&gt; &lt;/div&gt;) &#125;&#125;// function App() &#123;// return (// &lt;div&gt;//// &lt;/div&gt;// );// &#125;export default App; props说明目标任务: 知道props传递时的一些注意事项 1. props是只读对象（readonly）根据**单项数据流(只能是父传子)**的要求，子组件只能读取props中的数据，不能进行修改 2. props可以传递任意数据数字、字符串、布尔值、数组、对象、函数、JSX(传递一个jsx过去 相当于传递了页面 也就是vue中的插槽) 1234567891011121314151617181920class App extends React.Component &#123; state = &#123; message: &#x27;this is message&#x27; &#125; render() &#123; return ( &lt;div&gt; &lt;div&gt;父组件&lt;/div&gt; &lt;FSon msg=&#123;this.state.message&#125; age=&#123;20&#125; isMan=&#123;true&#125; cb=&#123;() =&gt; &#123; console.log(1) &#125;&#125; // 传递函数 子可以直接执行 child=&#123;&lt;span&gt;this is child&lt;/span&gt;&#125; // 传递jsx 传递模板 子组件会直接渲染 /&gt; &lt;CSon msg=&#123;this.state.message&#125; /&gt; &lt;/div&gt; ) &#125;&#125; 解构赋值父传递了多个数据 子可以使用解构赋值的方式 将元素获取出来 子传父实现目标任务: 实现父子通信中的子传父 口诀： 父组件给子组件传递回调函数，子组件调用 实现步骤 父组件提供一个回调函数 - 用于接收数据 将函数作为属性的值，传给子组件 子组件通过props调用 回调函数 将子组件中的数据作为参数传递给回调函数 代码实现 12345678910111213141516171819202122232425262728293031323334353637383940414243import React from &#x27;react&#x27;// 子组件function Son(props) &#123; function handleClick() &#123; // 调用父组件传递过来的回调函数 并注入参数 props.changeMsg(&#x27;this is newMessage&#x27;) &#125; return ( &lt;div&gt; &#123;props.msg&#125; &lt;button onClick=&#123;handleClick&#125;&gt;change&lt;/button&gt; &lt;/div&gt; )&#125;class App extends React.Component &#123; state = &#123; message: &#x27;this is message&#x27; &#125; // 提供回调函数 changeMessage = (newMsg) =&gt; &#123; console.log(&#x27;子组件传过来的数据:&#x27;,newMsg) this.setState(&#123; message: newMsg &#125;) &#125; render() &#123; return ( &lt;div&gt; &lt;div&gt;父组件&lt;/div&gt; &lt;Son msg=&#123;this.state.message&#125; // 传递给子组件 changeMsg=&#123;this.changeMessage&#125; /&gt; &lt;/div&gt; ) &#125;&#125;export default App 兄弟组件通信目标任务: 实现兄弟组件之间的通信 核心思路： 通过状态提升机制，利用共同的父组件实现兄弟通信 实现步骤 将共享状态提升到最近的公共父组件中，由公共父组件管理这个状态 提供共享状态 提供操作共享状态的方法 要接收数据状态的子组件通过 props 接收数据 要传递数据状态的子组件通过props接收方法，调用方法传递数据 代码实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import React from &#x27;react&#x27;// 子组件Afunction SonA(props) &#123; return ( &lt;div&gt; SonA &#123;props.msg&#125; &lt;/div&gt; )&#125;// 子组件Bfunction SonB(props) &#123; return ( &lt;div&gt; SonB &lt;button onClick=&#123;() =&gt; props.changeMsg(&#x27;new message&#x27;)&#125;&gt;changeMsg&lt;/button&gt; &lt;/div&gt; )&#125;// 父组件class App extends React.Component &#123; // 父组件提供状态数据 state = &#123; message: &#x27;this is message&#x27; &#125; // 父组件提供修改数据的方法 changeMsg = (newMsg) =&gt; &#123; this.setState(&#123; message: newMsg &#125;) &#125; render() &#123; return ( &lt;&gt; &#123;/* 接收数据的组件 */&#125; &lt;SonA msg=&#123;this.state.message&#125; /&gt; &#123;/* 修改数据的组件 */&#125; &lt;SonB changeMsg=&#123;this.changeMsg&#125; /&gt; &lt;/&gt; ) &#125;&#125;export default App 跨组件通信Context目标任务: 了解Context机制解决的问题和使用步骤 上图是一个react形成的嵌套组件树，如果我们想从App组件向任意一个下层组件传递数据，该怎么办呢？目前我们能采取的方式就是一层一层的props往下传，显然很繁琐那么，Context 提供了一个无需为每层组件手动添加 props，就能在组件树间进行数据传递的方法 实现步骤1- 创建Context对象 导出 Provider 和 Consumer对象 1const &#123; Provider, Consumer &#125; = createContext() 2- 使用Provider包裹上层组件提供数据 123&lt;Provider value=&#123;this.state.message&#125;&gt; &#123;/* 根组件 */&#125;&lt;/Provider&gt; 3- 需要用到数据的组件使用Consumer包裹获取数据 123&lt;Consumer &gt; &#123;value =&gt; /* 基于 context 值进行渲染*/&#125;&lt;/Consumer&gt; 代码实现 1234567891011121314151617181920212223242526272829303132333435363738import React, &#123; createContext &#125; from &#x27;react&#x27;// 1. 创建Context对象 const &#123; Provider, Consumer &#125; = createContext()// 3. 消费数据function ComC() &#123; return ( &lt;Consumer &gt; &#123;value =&gt; &lt;div&gt;&#123;value&#125;&lt;/div&gt;&#125; &lt;/Consumer&gt; )&#125;function ComA() &#123; return ( &lt;ComC/&gt; )&#125;// 2. 提供数据class App extends React.Component &#123; state = &#123; message: &#x27;this is message&#x27; &#125; render() &#123; return ( &lt;Provider value=&#123;this.state.message&#125;&gt; &lt;div className=&quot;app&quot;&gt; &lt;ComA /&gt; &lt;/div&gt; &lt;/Provider&gt; ) &#125;&#125;export default App 阶段小练习 要求：App为父组件用来提供列表数据 ，ListItem为子组件用来渲染列表数据 123456// 列表数据[ &#123; id: 1, name: &#x27;超级好吃的棒棒糖&#x27;, price: 18.8, info: &#x27;开业大酬宾，全场8折&#x27; &#125;, &#123; id: 2, name: &#x27;超级好吃的大鸡腿&#x27;, price: 34.2, info: &#x27;开业大酬宾，全场8折&#x27; &#125;, &#123; id: 3, name: &#x27;超级无敌的冰激凌&#x27;, price: 14.2, info: &#x27;开业大酬宾，全场8折&#x27; &#125;] 完整代码 12345678910111213141516171819202122232425262728293031323334353637import React from &#x27;react&#x27;class ComA extends React.Component &#123; render() &#123; return &lt;div&gt; &lt;h3&gt;&#123;this.props.item.name&#125;&lt;/h3&gt; &lt;p&gt;&#123;this.props.item.price&#125;&lt;/p&gt; &lt;p&gt;&#123;this.props.item.info&#125;&lt;/p&gt; &lt;button onClick=&#123;() =&gt; this.props.deleteById(this.props.item.id)&#125;&gt;删除&lt;/button&gt; &lt;/div&gt; &#125;&#125;function ComB() &#123; return &lt;div&gt;comb&lt;/div&gt;&#125;// 定义一个父组件class App extends React.Component&#123; // 定义父组件中的参数 用于传递给子组件 state = &#123; // 列表数据 list : [ &#123; id: 1, name: &#x27;超级好吃的棒棒糖&#x27;, price: 18.8, info: &#x27;开业大酬宾，全场8折&#x27; &#125;, &#123; id: 2, name: &#x27;超级好吃的大鸡腿&#x27;, price: 34.2, info: &#x27;开业大酬宾，全场8折&#x27; &#125;, &#123; id: 3, name: &#x27;超级无敌的冰激凌&#x27;, price: 14.2, info: &#x27;开业大酬宾，全场8折&#x27; &#125; ] &#125; deleteById = (id) =&gt; &#123; this.setState(&#123; list : this.state.list.filter(i =&gt; i.id !== id) &#125;) &#125; render() &#123; return (&lt;div&gt; &#123;/* 这里遍历数组 并且父传子 将数组的每一个数据 传递给子组件 */&#125; &#123;this.state.list.map(item =&gt; &lt;ComA key=&#123;item.id&#125; item=&#123;item&#125; deleteById=&#123;this.deleteById&#125;/&gt;)&#125; &lt;/div&gt;) &#125;&#125; React组件进阶children属性目标任务: 掌握props中children属性的用法 children属性是什么 表示该组件的子节点，只要组件内部有子节点，props中就有该属性 children可以是什么 普通文本 普通标签元素 函数 &#x2F; 对象 JSX123456789101112131415161718192021function ComA(props) &#123; // props.children() return &lt;div className= &#x27;title&#x27;&gt;&#123;props.children&#125;&lt;/div&gt;&#125;class App extends React.Component&#123; state = &#123; msg : &quot;给子组件&quot; &#125; changeWhat = () =&gt; &#123; alert(&quot;open&quot;) &#125; render() &#123; return &lt;div&gt; &#123;/* 在子组件中间的 ‘醒目留言’ 会自动传递到 props的children属性 */&#125; &lt;ComA&gt;醒目留言&lt;/ComA&gt; &lt;ComA&gt;&lt;div&gt;可以传递标签&lt;/div&gt;&lt;/ComA&gt; &lt;ComA&gt;&#123;this.changeWhat&#125;&lt;/ComA&gt; &lt;ComA&gt;&#123;&lt;div&gt;&#123;false? 100 : 2000&#125;&lt;/div&gt;&#125;&lt;/ComA&gt; &lt;/div&gt; &#125;&#125; 他的目的在于高阶组件 props校验-场景和使用目标任务: 掌握组件props的校验写法，增加组件的健壮性 对于组件来说，props是由外部传入的，我们其实无法保证组件使用者传入了什么格式的数据，如果传入的数据格式不对，就有可能会导致组件内部错误，有一个点很关键 - 组件的使用者可能报错了也不知道为什么，看下面的例子 面对这样的问题，如何解决？ props校验 实现步骤 安装属性校验包：yarn add prop-types 导入prop-types 包 使用 组件名.propTypes = &#123;&#125; 给组件添加校验规则 核心代码 1234567891011import PropTypes from &#x27;prop-types&#x27;const List = props =&gt; &#123; const arr = props.colors const lis = arr.map((item, index) =&gt; &lt;li key=&#123;index&#125;&gt;&#123;item.name&#125;&lt;/li&gt;) return &lt;ul&gt;&#123;lis&#125;&lt;/ul&gt;&#125;List.propTypes = &#123; colors: PropTypes.array&#125; props校验-规则说明目标任务: 掌握props常见的规则 四种常见结构 常见类型：array、bool、func、number、object、string React元素类型：element 必填项：isRequired 特定的结构对象：shape({}) 核心代码 123456789// 常见类型optionalFunc: PropTypes.func,// 必填 只需要在类型后面串联一个isRequiredrequiredFunc: PropTypes.func.isRequired,// 特定结构的对象optionalObjectWithShape: PropTypes.shape(&#123; color: PropTypes.string, fontSize: PropTypes.number&#125;) 官网文档更多阅读：https://reactjs.org/docs/typechecking-with-proptypes.html props校验-默认值目标任务: 掌握如何给组件的props提供默认值 通过 defaultProps 可以给组件的props设置默认值，在未传入props的时候生效 1. 函数组件（一般直接就这样写） 直接使用函数参数默认值 12345678910function List(&#123;pageSize = 10&#125;) &#123; return ( &lt;div&gt; 此处展示props的默认值：&#123; pageSize &#125; &lt;/div&gt; )&#125;// 不传入pageSize属性&lt;List /&gt; 2. 类组件 使用类静态属性声明默认值，static defaultProps = &#123;&#125; 12345678910111213class List extends Component &#123; static defaultProps = &#123; pageSize: 10 &#125; render() &#123; return ( &lt;div&gt; 此处展示props的默认值：&#123;this.props.pageSize&#125; &lt;/div&gt; ) &#125;&#125;&lt;List /&gt; 生命周期 - 概述目标任务: 能够说出组件生命周期一共几个阶段 组件的生命周期是指组件从被创建到挂载到页面中运行起来，再到组件不用时卸载的过程，注意，只有类组件才有生命周期（类组件 实例化 函数组件 不需要实例化） http://projects.wojtekmaj.pl/react-lifecycle-methods-diagram/ 生命周期 - 挂载阶段目标任务: 能够说出在组件挂载阶段执行的钩子函数和执行时机 钩子 函数 触发时机 作用 constructor（用的不多） 创建组件时，最先执行，初始化的时候只执行一次 1. 初始化state 2. 创建 Ref 3. 使用 bind 解决 this 指向问题等 render 每次组件渲染都会触发 渲染UI（注意： 不能在里面调用setState() ） componentDidMount（例如vue的mounted） 组件挂载（完成DOM渲染）后执行，初始化的时候执行一次 1. 发送网络请求 2.DOM操作 生命周期 - 更新阶段目标任务: 能够说出组件的更新阶段的钩子函数以及执行时机 钩子函数 触发时机 作用 render 每次组件渲染都会触发 渲染UI（与 挂载阶段 是同一个render） componentDidUpdate 组件更新后（DOM渲染完毕） DOM操作，可以获取到更新后的DOM内容，不要直接调用setState 生命周期 - 卸载阶段目标任务: 能够说出组件的销毁阶段的钩子函数以及执行时机 钩子函数 触发时机 作用 componentWillUnmount 组件卸载（从页面中消失） 执行清理工作（比如：清理定时器等） 阶段小练习 - todoMVC案例仓库地址：https://gitee.com/react-course-series/react-todo-mvc1- 克隆项目到本地 1$ git clone https://gitee.com/react-course-series/react-todo-mvc.git 2- 安装必要依赖 1$ npm i 3- 开启mock接口服务，保持窗口不关闭 ！！！！！ 12# 启动mock服务$ npm run mock-serve 4- 另起一个bash窗口开启前端服务 1$ npm start 5- 切换到todo-test分支 1$ git checkout todo-test **接口文档 ** 接口作用 接口地址 接口方法 接口参数 获取列表 http://localhost:3001/data GET 无 删除 http://localhost:3001/data/:id DELETE id 搜索 http://localhost:3001/data/?name=keyword GET name（以name字段搜索） 实现功能 功能 核心思路 表格数据渲染 组件使用 删除功能 获取当前id 调用接口 搜索功能 用的依旧是列表接口，多传一个name参数 清除搜索功能 清空搜索参数 重新获取列表 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990import &#123; Input, Table, Space, Popconfirm, Button &#125; from &#x27;antd&#x27;import React from &#x27;react&#x27;import &#x27;./App.css&#x27;import axios from &quot;axios&quot;;const &#123; Search &#125; = Inputclass App extends React.Component &#123; state = &#123; list: [], columns: [ &#123; title: &#x27;任务编号&#x27;, dataIndex: &#x27;id&#x27;, key: &#x27;id&#x27;, &#125;, &#123; title: &#x27;任务名称&#x27;, dataIndex: &#x27;name&#x27;, key: &#x27;name&#x27;, &#125;, &#123; title: &#x27;任务描述&#x27;, dataIndex: &#x27;des&#x27;, key: &#x27;des&#x27;, &#125;, &#123; title: &#x27;操作&#x27;, dataIndex: &#x27;do&#x27;, key: &#x27;do&#x27;, render: (text, record) =&gt; ( &lt;Space size=&quot;middle&quot;&gt; &lt;Popconfirm title=&quot;确定要删除吗?&quot; onConfirm=&#123;() =&gt; this.handleDelete(record.id)&#125;&gt; &lt;a href=&quot;#&quot;&gt;删除&lt;/a&gt; &lt;/Popconfirm&gt; &lt;/Space&gt; ), &#125;, ] &#125; // 搜索 // 值得一说的 点击搜索框中的 x号 也会回调这个函数 不过他的value数据是空的 onSearch =async (value) =&gt; &#123; const res = await axios.get(`http://localhost:3001/data/?q=$&#123;value&#125;`) this.setState(&#123; list: res.data &#125;) &#125; // 删除 handleDelete = async (id) =&gt; &#123; await axios.delete(`http://localhost:3001/data/$&#123;id&#125;`) this.loadList() &#125; // 加载列表 这就需要借助钩子 // async是一个加在函数前的修饰符，被async定义的函数会默认返回一个Promise对象resolve的值。因此对async函数可以直接then，返回值就是then方法传入的函数。 // await 修饰的如果是Promise对象：可以获取Promise中返回的内容（resolve或reject的参数），且取到值后语句才会往下执行； loadList = async () =&gt; &#123; const res = await axios.get(&#x27;http://localhost:3001/data&#x27;); this.setState(&#123; list: res.data &#125;) &#125; componentDidMount() &#123; this.loadList() &#125; render () &#123; return ( &lt;div className=&quot;container&quot;&gt; &lt;div className=&quot;search-box&quot;&gt; &lt;Search placeholder=&quot;请输入关键词&quot; allowClear enterButton=&quot;搜索&quot; size=&quot;large&quot; onChange=&#123;this.inputChange&#125; value=&#123;this.state.keyword&#125; onSearch=&#123;this.onSearch&#125; /&gt; &lt;/div&gt; &lt;Table bordered dataSource=&#123;this.state.list&#125; columns=&#123;this.state.columns&#125; pagination=&#123;false&#125; /&gt; &lt;/div&gt; ) &#125;&#125;export default App 关于 async awaithttps://zhuanlan.zhihu.com/p/172378607async 就是异步 他声明的函数实际上返回的是promise对象 不用await就需要自己用then接受了 用了await 可以直接获取结果 HooksHooks概念理解本节任务: 能够理解hooks的概念及解决的问题 1. 什么是hooks Hooks的本质：一套能够使函数组件更强大，更灵活的“钩子” React体系里组件分为 类组件 和 函数组件经过多年的实战，函数组件是一个更加匹配React的设计理念 UI = f(data)，也更有利于逻辑拆分与重用的组件表达形式，而先前的函数组件是不可以有自己的状态的，为了能让函数组件可以拥有自己的状态，所以从react v16.8开始，Hooks应运而生 注意点： 有了hooks之后，为了兼容老版本，class类组件并没有被移除，俩者都可以使用 有了hooks之后，不能在把函数成为无状态组件了，因为hooks为函数组件提供了状态 hooks只能在函数组件中使用 2. Hooks解决了什么问题 Hooks的出现解决了俩个问题 1. 组件的状态逻辑复用 2.class组件自身的问题 组件的逻辑复用在hooks出现之前，react先后尝试了 mixins混入，HOC高阶组件，render-props等模式但是都有各自的问题，比如mixin的数据来源不清晰，高阶组件的嵌套问题等等 class组件自身的问题class组件就像一个厚重的‘战舰’ 一样，大而全，提供了很多东西，有不可忽视的学习成本，比如各种生命周期，this指向问题等等，而我们更多时候需要的是一个轻快灵活的’快艇’ useState1. 基础使用本节任务: 能够学会useState的基础用法作用 useState为函数组件提供状态（state） 使用步骤 导入 useState 函数 调用 useState 函数，并传入状态的初始值 从useState函数的返回值中，拿到状态和修改状态的方法 在JSX中展示状态 调用修改状态的方法更新状态 代码实现 1234567891011import &#123; useState &#125; from &#x27;react&#x27;function App() &#123; // 参数：状态初始值比如,传入 0 表示该状态的初始值为 0 // 返回值：数组,包含两个值：1 状态值（state） 2 修改该状态的函数（setState） const [count, setCount] = useState(0) return ( &lt;button onClick=&#123;() =&gt; &#123; setCount(count + 1) &#125;&#125;&gt;&#123;count&#125;&lt;/button&gt; )&#125;export default App 2. 状态的读取和修改本节任务: 能够理解useState下状态的读取和修改读取状态 该方式提供的状态，是函数内部的局部变量，可以在函数内的任意位置使用 修改状态 setCount是一个函数，参数表示最新的状态值 调用该函数后，将使用新值替换旧值 修改状态后，由于状态发生变化，会引起视图变化 注意事项 修改状态的时候，一定要使用新的状态替换旧的状态，不能直接修改旧的状态，尤其是引用类型123456789function App() &#123; const [count, setCount] = useState(0) return ( &lt;div&gt; &#123;/* 这个方式是 新值替换旧值 也就是说 传入的参数会替代原本的参数 */&#125; &lt;button onClick=&#123;() =&gt; setCount(12)&#125;&gt;&#123;count&#125;&lt;/button&gt; &lt;/div&gt; );&#125; 3. 组件的更新过程本节任务: 能够理解使用hook之后组件的更新情况函数组件使用 useState hook 后的执行过程，以及状态值的变化 组件第一次渲染 从头开始执行该组件中的代码逻辑 调用 useState(0) 将传入的参数作为状态初始值，即：0 渲染组件，此时，获取到的状态 count 值为： 0 组件第二次渲染 点击按钮，调用 setCount(count + 1) 修改状态，因为状态发生改变，所以，该组件会重新渲染 组件重新渲染时，会再次执行该组件中的代码逻辑 再次调用 useState(0)，此时 React 内部会拿到最新的状态值而非初始值，比如，该案例中最新的状态值为 1(所以说 是在再次渲染的时候将最新的状态值给予) 再次渲染组件，此时，获取到的状态 count 值为：1 注意：useState 的初始值(参数)只会在组件第一次渲染时生效。也就是说，以后的每次渲染（调用setCount 整个app中代码都会执行），useState 获取到都是最新的状态值，React 组件会记住每次最新的状态值 1234567891011import &#123; useState &#125; from &#x27;react&#x27;function App() &#123; const [count, setCount] = useState(0) // 在这里可以进行打印测试 console.log(count) return ( &lt;button onClick=&#123;() =&gt; &#123; setCount(count + 1) &#125;&#125;&gt;&#123;count&#125;&lt;/button&gt; )&#125;export default App 4. 使用规则本节任务: 能够记住useState的使用规则 这个useState类时 vue中的ref 只是这里有两个返回值 useState 函数可以执行多次，每次执行互相独立，每调用一次为函数组件提供一个状态 123456function List()&#123; // 以字符串为初始值 const [name, setName] = useState(&#x27;cp&#x27;) // 以数组为初始值 const [list,setList] = useState([])&#125; useState 注意事项 只能出现在函数组件或者其他hook函数中 不能嵌套在if&#x2F;for&#x2F;其它函数中（react按照hooks的调用顺序识别每一个hook） 123456789let num = 1function List()&#123; num++ if(num / 2 === 0)&#123; const [name, setName] = useState(&#x27;cp&#x27;) &#125; const [list,setList] = useState([])&#125;// 俩个hook的顺序不是固定的，这是不可以的！！！ 可以通过开发者工具查看hooks状态 useEffect1. 理解函数副作用本节任务: 能够理解副作用的概念 什么是副作用 副作用是相对于主作用来说的，一个函数除了主作用，其他的作用就是副作用。对于 React 组件来说，主作用就是根据数据（state&#x2F;props）渲染 UI，除此之外都是副作用（比如，手动修改 DOM） 副作用其实就是： 本来你函数中只是几个增加的运算 但是又加了一点 赋值操作 其他等等 这些除了主作用外的其他的处理 就是副作用常见的副作用 数据请求 ajax发送 手动修改dom localstorage操作 useEffect函数的作用就是为react函数组件提供副作用处理的！ 2. 基础使用本节任务: 能够学会useEffect的基础用法并且掌握默认的执行执行时机可以写多个作用 为react函数组件提供副作用处理 使用步骤 导入 useEffect 函数 调用 useEffect 函数，并传入回调函数 在回调函数中编写副作用处理（dom操作） 修改数据状态 检测副作用是否生效 当通过修改状态更新组件时 useEffect 也会不断调用代码实现 123456789101112131415import &#123; useEffect, useState &#125; from &#x27;react&#x27;function App() &#123; const [count, setCount] = useState(0) useEffect(()=&gt;&#123; // dom操作 document.title = `当前已点击了$&#123;count&#125;次` &#125;) return ( &lt;button onClick=&#123;() =&gt; &#123; setCount(count + 1) &#125;&#125;&gt;&#123;count&#125;&lt;/button&gt; )&#125;export default App 3. 依赖项控制执行时机本节任务: 能够学会使用依赖项控制副作用的执行时机 1. 不添加依赖项 组件首次渲染执行一次，以及不管是哪个状态更改引起组件更新时都会重新执行 组件初始渲染 组件更新 （不管是哪个状态引起的更新） 123useEffect(()=&gt;&#123; console.log(&#x27;副作用执行了&#x27;)&#125;) 2. 添加空数组 组件只在首次渲染时执行一次 123useEffect(()=&gt;&#123; console.log(&#x27;副作用执行了&#x27;)&#125;,[]) 3. 添加特定依赖项 副作用函数在首次渲染时执行，在依赖项发生变化时重新执行 123456789101112131415function App() &#123; const [count, setCount] = useState(0) const [name, setName] = useState(&#x27;zs&#x27;) useEffect(() =&gt; &#123; console.log(&#x27;副作用执行了&#x27;) &#125;, [count]) return ( &lt;&gt; &lt;button onClick=&#123;() =&gt; &#123; setCount(count + 1) &#125;&#125;&gt;&#123;count&#125;&lt;/button&gt; &lt;button onClick=&#123;() =&gt; &#123; setName(&#x27;cp&#x27;) &#125;&#125;&gt;&#123;name&#125;&lt;/button&gt; &lt;/&gt; )&#125; 注意事项useEffect 回调函数中用到的数据（比如，count）就是依赖数据，就应该出现在依赖项数组中，如果不添加依赖项就会有bug出现 4. 清理副作用 如果想要清理副作用 可以在副作用函数中的末尾return一个新的函数，在新的函数中编写清理副作用的逻辑注意执行时机为： 组件卸载时自动执行 组件更新时，下一个useEffect副作用函数执行之前自动执行 12345678910111213141516171819202122import &#123; useEffect, useState &#125; from &quot;react&quot;const App = () =&gt; &#123; const [count, setCount] = useState(0) useEffect(() =&gt; &#123; const timerId = setInterval(() =&gt; &#123; setCount(count + 1) &#125;, 1000) return () =&gt; &#123; // 用来清理副作用的事情 clearInterval(timerId) &#125; &#125;, [count]) return ( &lt;div&gt; &#123;count&#125; &lt;/div&gt; )&#125;export default App 阶段小练习 - 自定义hook需求描述：自定义一个hook函数，实现获取滚动距离Y const [y] = useWindowScroll() 123456789101112131415import &#123; useState, useEffect &#125; from &quot;react&quot;export function useWindowScroll () &#123; const [y, setY] = useState(0) useEffect(()=&gt;&#123; const scrollHandler = () =&gt; &#123; const h = document.documentElement.scrollTop // 获取当前页面的底部 setY(h) &#125;) window.addEventListener(&#x27;scroll&#x27;, scrollHandler) return () =&gt; window.removeEventListener(&#x27;scroll&#x27;, scrollHandler) &#125;) return [y]&#125; 需求描述： 自定义hook函数，可以自动同步到本地LocalStorage const [message, setMessage] = useLocalStorage(key，defaultValue) message可以通过自定义传入默认初始值 每次修改message数据的时候 都会自动往本地同步一份 12345678910import &#123; useEffect, useState &#125; from &#x27;react&#x27;export function useLocalStorage (key, defaultValue) &#123; const [message, setMessage] = useState(defaultValue) // 每次只要message变化 就会自动同步到本地ls useEffect(() =&gt; &#123; window.localStorage.setItem(key, message) &#125;, [message, key]) return [message, setMessage]&#125; Hooks进阶useState - 回调函数的参数本节任务: 能够理解useState回调函数作为参数的使用场景 使用场景参数只会在组件的初始渲染中起作用，后续渲染时会被忽略。如果初始 state 需要通过计算才能获得，则可以传入一个函数，在函数中计算并返回初始的 state，此函数只在初始渲染时被调用 语法 123const [name, setName] = useState(()=&gt;&#123; // 编写计算逻辑 return &#x27;计算之后的初始值&#x27;&#125;) 语法规则 回调函数return出去的值将作为 name 的初始值 回调函数中的逻辑只会在组件初始化的时候执行一次 语法选择 如果就是初始化一个普通的数据 直接使用 useState(普通数据) 即可 如果要初始化的数据无法直接得到需要通过计算才能获取到，使用useState(()=&gt;&#123;&#125;) 来个需求 1234567891011121314151617181920212223import &#123; useState &#125; from &#x27;react&#x27;function Counter(props) &#123; const [count, setCount] = useState(() =&gt; &#123; return props.count &#125;) return ( &lt;div&gt; &lt;button onClick=&#123;() =&gt; setCount(count + 1)&#125;&gt;&#123;count&#125;&lt;/button&gt; &lt;/div&gt; )&#125;function App() &#123; return ( &lt;&gt; &lt;Counter count=&#123;10&#125; /&gt; &lt;Counter count=&#123;20&#125; /&gt; &lt;/&gt; )&#125;export default App useEffect - 发送网络请求本节任务: 能够掌握使用useEffect hook发送网络请求 使用场景 如何在useEffect中发送网络请求，并且封装同步 async await操作 语法要求 不可以直接在useEffect的回调函数外层直接包裹 await ，因为异步会导致清理函数无法立即返回 1234useEffect(async ()=&gt;&#123; const res = await axios.get(&#x27;http://geek.itheima.net/v1_0/channels&#x27;) console.log(res)&#125;,[]) 正确写法在内部单独定义一个函数，然后把这个函数包装成同步 12345useEffect(()=&gt;&#123; async function fetchData()&#123; const res = await axios.get(&#x27;http://geek.itheima.net/v1_0/channels&#x27;) console.log(res) &#125; &#125;,[]) useRef本节任务: 能够掌握使用useRef获取真实dom或组件实例的方法 使用场景 在函数组件中获取真实的dom元素对象或者是组件对象 使用步骤 导入 useRef 函数 执行 useRef 函数并传入null，返回值为一个对象 内部有一个current属性存放拿到的dom对象（组件实例） 通过ref 绑定 要获取的元素或者组件 获取dom 12345678910111213import &#123; useEffect, useRef &#125; from &#x27;react&#x27;function App() &#123; const h1Ref = useRef(null) useEffect(() =&gt; &#123; console.log(h1Ref) &#125;,[]) return ( &lt;div&gt; &lt;h1 ref=&#123; h1Ref &#125;&gt;this is h1&lt;/h1&gt; &lt;/div&gt; )&#125;export default App 获取组件实例 函数组件由于没有实例，不能使用ref获取，如果想获取组件实例，必须是类组件 12345678910class Foo extends React.Component &#123; sayHi = () =&gt; &#123; console.log(&#x27;say hi&#x27;) &#125; render()&#123; return &lt;div&gt;Foo&lt;/div&gt; &#125;&#125; export default Foo 123456789101112import &#123; useEffect, useRef &#125; from &#x27;react&#x27;import Foo from &#x27;./Foo&#x27;function App() &#123; const h1Foo = useRef(null) useEffect(() =&gt; &#123; console.log(h1Foo) &#125;, []) return ( &lt;div&gt; &lt;Foo ref=&#123; h1Foo &#125; /&gt;&lt;/div&gt; )&#125;export default App useContext本节任务: 能够掌握hooks下的context使用方式 实现步骤 使用createContext 创建Context对象 在顶层组件通过Provider 提供数据 在底层组件通过useContext函数获取数据 代码实现 123456789101112131415161718192021222324import &#123; createContext, useContext &#125; from &#x27;react&#x27;// 创建Context对象const Context = createContext()function Foo() &#123; return &lt;div&gt;Foo &lt;Bar/&gt;&lt;/div&gt;&#125;function Bar() &#123; // 底层组件通过useContext函数获取数据 const name = useContext(Context) return &lt;div&gt;Bar &#123;name&#125;&lt;/div&gt;&#125;function App() &#123; return ( // 顶层组件通过Provider 提供数据 &lt;Context.Provider value=&#123;&#x27;this is name&#x27;&#125;&gt; &lt;div&gt;&lt;Foo/&gt;&lt;/div&gt; &lt;/Context.Provider&gt; )&#125;export default App 阶段小练习-todoMvc-hook版案例仓库地址：https://gitee.com/react-course-series/react-tomvc-hook 项目开发步骤： 切换到todo-test分支 1$ git checkout todo-test 打开 app.js已有基础样板代码，在这个基础上编写业务逻辑即可 接口文档 接口作用 接口地址 接口方法 接口参数 获取列表 http://localhost:3001/data GET 无 删除 http://localhost:3001/data/:id DELETE id 搜索 http://localhost:3001/data/?name=keyword GET name（以name字段搜索） 实现功能 功能 核心思路 表格数据渲染 elementPlus el-table组件使用 删除功能 获取当前id 调用接口 搜索功能 用的依旧是列表接口，多传一个name参数 清除搜索功能 清空搜索参数 重新获取列表 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485import &#123; Input, Table, Space, Popconfirm &#125; from &#x27;antd&#x27;import React, &#123;useEffect, useState&#125; from &#x27;react&#x27;import &#x27;./App.css&#x27;import axios from &quot;axios&quot;;const &#123; Search &#125; = Inputfunction App () &#123; // 获取列表 const [list, setList] = useState([]) const loadList = async () =&gt; &#123; const res = await axios.get(&#x27;http://localhost:3001/data&#x27;) console.log(res.data) setList(res.data) &#125; useEffect(() =&gt; &#123; loadList() &#125;, []) // 删除 const del = async (id) =&gt; &#123; await axios.delete(`http://localhost:3001/data/$&#123;id&#125;`) loadList() &#125; // 搜索 const onSearch = async (value) =&gt; &#123; const res = await axios.get(`http://localhost:3001/data/?q=$&#123;value&#125;`) setList(res.data) &#125; const columns = [ &#123; title: &#x27;任务编号&#x27;, dataIndex: &#x27;id&#x27;, key: &#x27;id&#x27;, &#125;, &#123; title: &#x27;任务名称&#x27;, dataIndex: &#x27;name&#x27;, key: &#x27;name&#x27;, &#125;, &#123; title: &#x27;任务描述&#x27;, dataIndex: &#x27;des&#x27;, key: &#x27;des&#x27;, &#125;, &#123; title: &#x27;操作&#x27;, dataIndex: &#x27;do&#x27;, key: &#x27;do&#x27;, render: (text, record) =&gt; ( &lt;Space size=&quot;middle&quot;&gt; &lt;Popconfirm title=&quot;确定要删除吗?&quot; onConfirm=&#123;() =&gt; del(record.id)&#125;&gt; &lt;a href=&quot;#&quot;&gt;删除&lt;/a&gt; &lt;/Popconfirm&gt; &lt;/Space&gt; ), &#125;, ] return ( &lt;div className=&quot;container&quot;&gt; &lt;div className=&quot;search-box&quot;&gt; &lt;Search placeholder=&quot;请输入关键词&quot; allowClear enterButton=&quot;搜索&quot; size=&quot;large&quot; onSearch=&#123;onSearch&#125; /&gt; &lt;/div&gt; &lt;Table bordered dataSource=&#123;list&#125; columns=&#123;columns&#125; rowKey=&#123;(record) =&gt; record.id&#125; pagination=&#123;false&#125; /&gt; &lt;/div&gt; )&#125;export default App 路由前置知识1. 单页应用 只有一个html文件 主流的开发模式变成了通过路由进行页面切换 优势: 避免整体页面刷新 用户体验变好前端负责事情变多了 开发的难度变大 2. 路由的本质 概念来源于后端 : 一个路径表示匹配一个服务器资源 &#x2F;a.html -&gt; a对应的文件资源 &#x2F;b.html -&gt; b对应的文件资源共同的思想: 一对一的关系前端的路由: 一个路径path对应唯一的一个组件comonent 当我们访问一个path 自动把path对应的组件进行渲染 1234567891011121314const routes = [ &#123; path:&#x27;/home&#x27;, component: Home &#125;, &#123; path:&#x27;/about&#x27;, component: About &#125;, &#123; path:&#x27;/article&#x27;, component: Article &#125;] 准备项目环境 create-react-app -&gt; cra -&gt; webpackvite: 可以实现cra同等能力 但是速度更快的打包工具 [尤大]使用vite新增一个React项目，然后安装一个v6版本的react-router-dom npm install react-router-dom@6 1234567891011# 创建react项目$ yarn create vite react-router --template react# 安装所有依赖包$ yarn# 启动项目$ yarn dev# 安装react-router包$ yarn add react-router-dom@6 基础使用 需求: 准备俩个按钮，点击不同按钮切换不同组件内容的显示实现步骤： 导入必要的路由router内置组件 准备俩个React组件 按照路由的规则进行路由配置 声明两个组件 123456789function Home() &#123; return ( &lt;div className=&quot;App&quot;&gt; home &lt;/div&gt; );&#125;export default Home; 123456789function About() &#123; return ( &lt;div className=&quot;App&quot;&gt; about &lt;/div&gt; );&#125;export default About; 123456789101112131415161718192021222324import &#x27;./App.css&#x27;;import &#123;BrowserRouter, Link, Route, Routes&#125; from &quot;react-router-dom&quot;;import Home from &quot;./Home&quot;;import About from &quot;./About&quot;;function App() &#123; return ( // BrowserRouter声明一个非hash模式的路由 &lt;BrowserRouter&gt; &#123;/* 指定跳转到哪里 类似a标签 */&#125; &lt;Link to=&#123;&quot;/&quot;&#125;&gt;首页&lt;/Link&gt; &lt;Link to=&#123;&quot;/about&quot;&#125;&gt;关于&lt;/Link&gt; &#123;/* 路由出口 */&#125; &lt;Routes&gt; &#123;/* 指定路由路径和组件的具体关系 path代表路径 element代表组件 成对出现 */&#125; &lt;Route path=&#123;&#x27;/&#x27;&#125; element=&#123;&lt;Home/&gt;&#125;&gt;&lt;/Route&gt; &lt;Route path=&#123;&#x27;/about&#x27;&#125; element=&#123;&lt;About/&gt;&#125;&gt;&lt;/Route&gt; &lt;/Routes&gt; &lt;/BrowserRouter&gt; );&#125;export default App; 核心内置组件说明1. BrowerRouter 作用: 包裹整个应用，一个React应用只需要使用一次 模式 实现方式 路由url表现 HashRouter 监听url hash值实现 http://localhost:3000/#/about BrowerRouter h5的 history.pushState API实现 http://localhost:3000/about 2. Link 作用: 用于指定导航链接，完成声明式的路由跳转 类似于 这里to属性用于指定路由地址，表示要跳转到哪里去，Link组件最终会被渲染为原生的a链接 3. Routes 作用: 提供一个路由出口，组件内部会存在多个内置的Route组件，满足条件的路由会被渲染到组件内部类比 router-view 4. Route 作用: 用于定义路由路径和渲染组件的对应关系 [element：因为react体系内 把组件叫做react element] 其中path属性用来指定匹配的路径地址，element属性指定要渲染的组件，图中配置的意思为: 当url上访问的地址为 &#x2F;about 时，当前路由发生匹配，对应的About组件渲染 编程式导航 声明式 【 Link to】 vs 编程式 【调用路由方法进行路由跳转】概念: 通过js编程的方式进行路由页面跳转，比如说从首页跳转到关于页实现步骤： 导入一个 useNavigate 钩子函数 执行 useNavigate 函数 得到 跳转函数 在事件中执行跳转函数完成路由跳转 编程式 跳转 1234567891011121314import &#123;useNavigate&#125; from &quot;react-router-dom&quot;;const Home = () =&gt; &#123; // 这个函数用来跳转 就像是vue中的 router() const navigate = useNavigate(); return ( &lt;div&gt; home &lt;button onClick=&#123;() =&gt; navigate(&#x27;/about&#x27;, &#123;replace:true&#125;)&#125;&gt;login&lt;/button&gt; &lt;/div&gt; )&#125;export default Home; 注: 如果在跳转时不想添加历史记录，可以添加额外参数replace 为true 1navigate(&#x27;/about&#x27;, &#123; replace: true &#125; ) 路由传参 场景：跳转路由的同时，有时候要需要传递参数 1. searchParams传参路由传参路由取参 123456789101112131415import &#123;useNavigate&#125; from &quot;react-router-dom&quot;;const Home = () =&gt; &#123; // 这个函数用来跳转 就像是vue中的 router() const navigate = useNavigate(); return ( &lt;div&gt; home &lt;button onClick=&#123;() =&gt; navigate(&#x27;/about?id=89098809809809&#x27;, &#123;replace:true&#125;)&#125;&gt;login&lt;/button&gt; &lt;/div&gt; )&#125;export default Home; 1234567891011121314import &#123;useSearchParams&#125; from &quot;react-router-dom&quot;;function About() &#123; // 解构对象 这就类似 vue中的route() let [params] = useSearchParams(); let id = params.get(&#x27;id&#x27;); return ( &lt;div className=&quot;App&quot;&gt; id = &#123; id &#125; &lt;/div&gt; );&#125;export default About; 2. params传参必须先有一个占位符 123456789101112131415161718192021222324import &#x27;./App.css&#x27;;import &#123;BrowserRouter, Link, Route, Routes&#125; from &quot;react-router-dom&quot;;import Home from &quot;./Home&quot;;import About from &quot;./About&quot;;function App() &#123; return ( // BrowserRouter声明一个非hash模式的路由 &lt;BrowserRouter&gt; &#123;/* 指定跳转到哪里 类似a标签 */&#125; &lt;Link to=&#123;&quot;/&quot;&#125;&gt;首页&lt;/Link&gt; &lt;Link to=&#123;&quot;/about&quot;&#125;&gt;关于&lt;/Link&gt; &#123;/* 路由出口 */&#125; &lt;Routes&gt; &#123;/* 指定路由路径和组件的具体关系 path代表路径 element代表组件 成对出现 */&#125; &lt;Route path=&#123;&#x27;/&#x27;&#125; element=&#123;&lt;Home/&gt;&#125;&gt;&lt;/Route&gt; &#123;/* 路由参数 如果使用这种风格的 必须要占位 */&#125; &lt;Route path=&#123;&#x27;/about/:id&#x27;&#125; element=&#123;&lt;About/&gt;&#125;&gt;&lt;/Route&gt; &lt;/Routes&gt; &lt;/BrowserRouter&gt; );&#125;export default App; 路由传参 路由取参 嵌套路由 场景：在我们做的很多的管理后台系统中，通常我们都会设计一个Layout组件，在它内部实现嵌套路由 实现步骤： App.js中定义嵌套路由声明 Layout组件内部通过 指定二级路由出口 1- App.js组件中定义路由嵌套关系 1234567891011121314151617181920212223import &#x27;./App.css&#x27;;import &#123;BrowserRouter, Route, Routes&#125; from &quot;react-router-dom&quot;;import Layout from &quot;./Layout&quot;;import Login from &quot;./Login&quot;;import Board from &quot;./Board&quot;;function App() &#123; return ( &lt;BrowserRouter&gt; &lt;Routes&gt; &#123;/* 一级路由 */&#125; &lt;Route path=&#123;&#x27;/&#x27;&#125; element=&#123;&lt;Layout/&gt;&#125;&gt; &#123;/* 不用写/ 和vue一样 另外注意路由出口 也就是这个组件要放在哪里 我们是在Layout中写入 出口当然在Layout中 */&#125; &lt;Route path=&quot;board&quot; element=&#123;&lt;Board/&gt;&#125;&gt;&lt;/Route&gt; &lt;/Route&gt; &lt;Route path=&#123;&#x27;/login&#x27;&#125; element=&#123;&lt;Login/&gt;&#125;&gt;&lt;/Route&gt; &lt;/Routes&gt; &lt;/BrowserRouter&gt; );&#125;export default App; 2- Layout.js组件中使用 Outlet 组件添加二级路由出口 123456789101112131415import &#123; Outlet &#125; from &#x27;react-router-dom&#x27;const Layout = () =&gt; &#123; return ( &lt;div&gt; layout &#123; /* 二级路由的path等于 一级path + 二级path */ &#125; &lt;Link to=&quot;/board&quot;&gt;board&lt;/Link&gt; &lt;Link to=&quot;/article&quot;&gt;article&lt;/Link&gt; &#123; /* 二级路由出口 */ &#125; &lt;Outlet/&gt; &lt;/div&gt; )&#125;export default Layout 默认二级路由 场景: 应用首次渲染完毕就需要显示的二级路由实现步骤: 给默认二级路由标记index属性 把原本的路径path属性去掉 123456&lt;Routes&gt; &lt;Route path=&quot;/&quot; element=&#123;&lt;Layout/&gt;&#125;&gt; &lt;Route index element=&#123; &lt;Board/&gt; &#125; /&gt; &lt;Route path=&quot;article&quot; element=&#123; &lt;Article/&gt; &#125; /&gt; &lt;/Route&gt;&lt;/Routes&gt; 1234567891011121314import &#123; Outlet &#125; from &#x27;react-router-dom&#x27;const Layout = () =&gt; &#123; return ( &lt;div&gt; layout &#123; /* 默认二级不再具有自己的路径 */ &#125; &lt;Link to=&quot;/&quot;&gt;board&lt;/Link&gt; &lt;Link to=&quot;/article&quot;&gt;article&lt;/Link&gt; &#123; /* 二级路由出口 */ &#125; &lt;Outlet/&gt; &lt;/div&gt; )&#125; 404路由配置 场景：当url的路径在整个路由配置中都找不到对应的path，使用404兜底组件进行渲染 1- 准备一个NotFound组件 12345const NotFound = () =&gt; &#123; return &lt;div&gt;this is NotFound&lt;/div&gt;&#125;export default NotFound 123456789&lt;BrowserRouter&gt; &lt;Routes&gt; &lt;Route path=&quot;/&quot; element=&#123;&lt;Layout /&gt;&#125;&gt; &lt;Route index element=&#123;&lt;Board /&gt;&#125; /&gt; &lt;Route path=&quot;article&quot; element=&#123;&lt;Article /&gt;&#125; /&gt; &lt;/Route&gt; &lt;Route path=&quot;*&quot; element=&#123;&lt;NotFound /&gt;&#125;&gt;&lt;/Route&gt; &lt;/Routes&gt;&lt;/BrowserRouter&gt; 尝试访问一个不存在的路径，查看效果~ 集中式路由配置 *** 场景: 当我们需要路由权限控制点时候, 对路由数组做一些权限的筛选过滤，所谓的集中式路由配置就是用一个数组统一把所有的路由对应关系写好替换 本来的Roues组件 快速使用使用步骤 引入 createBrowerRouter 方法和 RouterProvider组件 使用 createBrowerRouter 配置路由 path 和组件的对应关系生成 router 实例 渲染 RouterProvider 组件并传入 router 实例 代码实现src&#x2F;router&#x2F;index.jsx 1234567891011121314151617import Login from &quot;../Login.jsx&quot;;import &#123;createBrowserRouter&#125; from &#x27;react-router-dom&#x27;const router = createBrowserRouter([ &#123; path: &#x27;/&#x27;, element: &lt;div&gt;this is layout&lt;/div&gt;, &#125;, &#123; path: &#x27;/login&#x27;, element: &lt;Login/&gt; &#125;, &#123; path: &#x27;/about&#x27;, element: &lt;div&gt;this is about&lt;/div&gt;, &#125;,])export default router src&#x2F;main.jsx 12345678import ReactDOM from &#x27;react-dom/client&#x27;import &#123;RouterProvider&#125; from &#x27;react-router-dom&#x27;import router from &quot;./router/index.jsx&quot;;ReactDOM.createRoot(document.getElementById(&#x27;root&#x27;)).render( &lt;RouterProvider router=&#123;router&#125; /&gt;) 查看效果在浏览器中，输入 &#x2F;login 和 &#x2F; 路径查看页面内容是否切换 编程式导航概念: 通过 js 编程的方式进行路由页面跳转，比如说从首页跳转到关于页实现步骤 导入一个 useNavigate 钩子函数 执行 useNavigate 函数 得到 跳转函数 在事件中执行跳转函数完成路由跳转 我们以从登录路由 跳转到 关于路由 作为例子 1234567891011import &#123; useNavigate &#125; from &#x27;react-router-dom&#x27;const Login = () =&gt; &#123; const navigate = useNavigate() return ( &lt;div&gt; &lt;button onClick=&#123;() =&gt; navigate(&#x27;/about&#x27;)&#125;&gt;go about&lt;/button&gt; &lt;/div&gt; )&#125;export default Login 注: 如果在跳转时想要替换记录，可以添加额外参数 replace 为 true 1navigate(&#x27;/&#x27;, &#123; replace: true &#125;) 路由传参场景：跳转路由的同时，有时候要需要传递参数 1. searchParams 传参查询字符串传参的方式比较简单，参数的形式以问号拼接到地址后面路由传参 12345678910import &#123; useNavigate &#125; from &#x27;react-router-dom&#x27;const Login = () =&gt; &#123; const navigate = useNavigate() return ( &lt;div&gt; &lt;button onClick=&#123;() =&gt; navigate(&#x27;/about?id=1001&#x27;)&#125;&gt;go index&lt;/button&gt; &lt;/div&gt; )&#125;export default Login 路由取参 123456789import &#123; useSearchParams &#125; from &#x27;react-router-dom&#x27;const About = () =&gt; &#123; const [params] = useSearchParams() let id = params.get(&#x27;id&#x27;) return &lt;div&gt;this is about &#123;id&#125;&lt;/div&gt;&#125;export default About 2. params 传参params 方式传参要求会多一些，需要我们在路由表配置的位置添加一个 参数占位参数占位 12345678const router = createBrowserRouter([ &#123; path: &#x27;/about/:id&#x27;, element: &lt;About /&gt;, &#125;,])export default router 路由传参 12345678910import &#123; useNavigate &#125; from &#x27;react-router-dom&#x27;const Login = () =&gt; &#123; const navigate = useNavigate() return ( &lt;div&gt; &lt;button onClick=&#123;() =&gt; navigate(&#x27;/about/1001&#x27;)&#125;&gt;go index&lt;/button&gt; &lt;/div&gt; )&#125;export default Login 路由取参 123456789import &#123; useParams &#125; from &#x27;react-router-dom&#x27;const About = () =&gt; &#123; const params = useParams() let id = params.id return &lt;div&gt;this is about &#123;id&#125;&lt;/div&gt;&#125;export default About 嵌套路由场景：在我们做的很多的管理后台系统中，通常我们都会设计一个 Layout 组件，在它内部实现嵌套路由实现步骤 准备二级路由组件 Board 和 Article 在路由表中通过 children属性 进行二级路由配置 通过内置组件 Outlet 渲染二级路由组件 使用内置组件 Link 进行声明式导航配置 代码实现1- 准备组件 12345const Article = () =&gt; &#123; return &lt;div&gt;this is article&lt;/div&gt;&#125;export default Article 1234const Board = () =&gt; &#123; return &lt;div&gt;this is Board&lt;/div&gt;&#125;export default Board 2- children 属性配置 1234567891011121314151617181920import Board from &#x27;../page/Board&#x27;import Article from &#x27;../page/Article&#x27;const router = createBrowserRouter([ &#123; path: &#x27;/&#x27;, element: &lt;Layout /&gt;, children: [ &#123; path: &#x27;board&#x27;, element: &lt;Board /&gt;, &#125;, &#123; path: &#x27;article&#x27;, element: &lt;Article /&gt;, &#125;, ], &#125;,])export default router 3- 配置二级路由出口 1234567891011121314151617import &#123; Outlet, Link &#125; from &#x27;react-router-dom&#x27;const Layout = () =&gt; &#123; return ( &lt;div&gt; this is Layout &lt;div&gt; &lt;Link to=&quot;/board&quot;&gt;面板&lt;/Link&gt; &lt;Link to=&quot;/article&quot;&gt;文章&lt;/Link&gt; &lt;/div&gt; &#123;/* 二级路由渲染出口 */&#125; &lt;Outlet /&gt; &lt;/div&gt; )&#125;export default Layout 默认二级路由场景: 应用首次渲染完毕就需要显示的二级路由实现步骤 给默认二级路由标记 index 属性 把原本的路径 path 属性去掉123456789101112131415161718const router = createBrowserRouter([ &#123; path: &#x27;/&#x27;, element: &lt;Layout /&gt;, children: [ &#123; index: true, element: &lt;Board /&gt;, &#125;, &#123; path: &#x27;article&#x27;, element: &lt;Article /&gt;, &#125;, ], &#125;,])export default router 写了这个 就不需要在写原本的路径了 而且当访问原本的路径会报错 404 路由配置场景：当 url 的路径在整个路由配置中都找不到对应的 path，使用 404 兜底组件进行渲染1- 准备一个 NotFound 组件2- 路由表末尾配置尝试访问一个不存在的路径，查看效果~ 12345const Article = () =&gt; &#123; return &lt;div&gt;this is article&lt;/div&gt;&#125;export default Article 1234const Board = () =&gt; &#123; return &lt;div&gt;this is Board&lt;/div&gt;&#125;export default Board 1234567891011121314151617181920import Board from &#x27;../page/Board&#x27;import Article from &#x27;../page/Article&#x27;const router = createBrowserRouter([ &#123; path: &#x27;/&#x27;, element: &lt;Layout /&gt;, children: [ &#123; path: &#x27;board&#x27;, element: &lt;Board /&gt;, &#125;, &#123; path: &#x27;article&#x27;, element: &lt;Article /&gt;, &#125;, ], &#125;,])export default router 1234567891011121314151617import &#123; Outlet, Link &#125; from &#x27;react-router-dom&#x27;const Layout = () =&gt; &#123; return ( &lt;div&gt; this is Layout &lt;div&gt; &lt;Link to=&quot;/board&quot;&gt;面板&lt;/Link&gt; &lt;Link to=&quot;/article&quot;&gt;文章&lt;/Link&gt; &lt;/div&gt; &#123;/* 二级路由渲染出口 */&#125; &lt;Outlet /&gt; &lt;/div&gt; )&#125;export default Layout 123456789101112131415161718const router = createBrowserRouter([ &#123; path: &#x27;/&#x27;, element: &lt;Layout /&gt;, children: [ &#123; index: true, element: &lt;Board /&gt;, &#125;, &#123; path: &#x27;article&#x27;, element: &lt;Article /&gt;, &#125;, ], &#125;,])export default router 12345const NotFound = () =&gt; &#123; return &lt;div&gt;this is NotFound&lt;/div&gt;&#125;export default NotFound 123456789101112131415161718192021222324252627282930const router = createBrowserRouter([ &#123; path: &#x27;/&#x27;, element: &lt;Layout /&gt;, children: [ &#123; index: true, element: &lt;Board /&gt;, &#125;, &#123; path: &#x27;article&#x27;, element: &lt;Article /&gt;, &#125;, ], &#125;, &#123; path: &#x27;/login&#x27;, element: &lt;Login /&gt;, &#125;, &#123; path: &#x27;/about/:id&#x27;, element: &lt;About /&gt;, &#125;, &#123; path: &#x27;*&#x27;, element: &lt;NotFound /&gt;, &#125;,])export default router 路由大练习 需求如下: 使用vite创建一个新的项目 在项目中全局安装组件库 antDesign 文档地址: https://ant.design/ 准备一个一级路由 使用antDesign中的布局组件实现如下图Layout页面的布局方式 Layout组件地址: [https://ant.design/components/layout-cn/](https://ant.design/components/layout-cn/) Menu组件地址: [https://ant.design/components/menu-cn/](https://ant.design/components/menu-cn/) 用 &lt;4.2.0 的写法即可 准备三个二级路由，分别是数据概览，内容管理，发布文章，在Layout组件内部的主体部分实现点击左侧菜单切换二级路由内容显示 将数据概览路由作为默认渲染的二级路由 设置404一级路由页，在无法匹配的时候做 兜底显示 [404页面UI不做要求] 在内容管理组件中增加一个按钮 点击可以跳转到 发布文章路由 并且传递一个参数 id &#x3D; 100 Mobx配置开发环境Mobx是一个独立的响应式的库，可以独立于任何UI框架存在，但是通常大家习惯把它和React进行绑定使用，用Mobx来做响应式数据建模，React作为UI视图框架渲染内容，我们环境的配置需要三个部分 一个create-react-app创建好的React项目环境 mobx框架本身 一个用来链接mobx和React的中间件 npm install mobx –savenpm install mobx-react –save基础使用 123456789101112131415import &#123; makeAutoObservable &#125; from &#x27;mobx&#x27;class CounterStore &#123; count = 0 // 定义数据 constructor() &#123; makeAutoObservable(this) // 响应式处理 &#125; // 定义修改数据的方法 addCount = () =&gt; &#123; this.count++ &#125;&#125;const counter = new CounterStore()export default counter 1234567891011import counter from &quot;./store/counter.jsx&quot;;import &#123;observer&#125; from &quot;mobx-react&quot;;const App = () =&gt; &#123; return ( &lt;div&gt; &#123;counter.count&#125; &lt;button onClick=&#123;counter.addCount&#125;&gt;xxxxxx&lt;/button&gt; &lt;/div&gt; )&#125;export default observer(App)","categories":[{"name":"前端","slug":"前端","permalink":"http://example.com/categories/%E5%89%8D%E7%AB%AF/"}],"tags":[]},{"title":"设计模式","slug":"设计模式","date":"2023-07-31T09:31:50.301Z","updated":"2023-07-31T09:32:06.090Z","comments":true,"path":"2023/07/31/设计模式/","link":"","permalink":"http://example.com/2023/07/31/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"前言关于设计模式 独立于语言，是一门单独的知识， 接口（interface）和API（项目对外提供 供外部访问的接口） 在代码中加入设计模式 可以提高代码的健壮性 可读性等等提高 并不是所有的地方使用设计模式都适合 要根据项目的不同来选择用不用设计模式 用哪个 而不是滥用 在java很多处都用了设计模式 java.util.Iterator(使用了迭代器模式) spring创建对象的方法使用了单例模式 项目中使用设计模式要考虑 是否适用 在实际的使用中 往往很多的设计模式是综合使用的 并不会单一的出现 设计模式的目标之一就是**提高程序的可复用性 解耦合等 ** 因此 他考虑的是如何使得程序复用 因此不能将示例作品当作成品应该当作扩展和变更的基础 从以下角度看设计模式： 有哪些功能可以被扩展 扩展功能是必须修改哪些地方 有哪些类不需要修改 使用设计模式的代码或许不同 但是角色都是一样的不要使用具体类来编程，优先使用抽象类或者接口来编程 理论的知识与实际结合还有很长一段距离，切记多多上手！ 当项目中同样类型的代码越写越多 就需要考虑设计模式了例如聚合搜索项目中 按照传入类型来判断具体的查询 每次都要写ifelse来判断具体的type 如果接口越来越多 就需要判断多个type 返回接口 这样很麻烦 随着项目越来越大 接入接口越来越多 ifelse也会越写越大 这样的话 屎山近在眼前 这个时候我们使用了适配器模式进行修改https://blog.csdn.net/a745233700/category_9280776.html Iterator 迭代器模式 (实际开发中使用并不多)迭代器 类似 数组循环 从下标0 一直i++下去 寻找元素迭代器模式： 提供一种方法顺序访问一个聚合对象各个元素，而又无需暴露该聚合对象的内部表示迭代器模式的特点：实现Iterator接口 通过next()方法获取元素 具体实现： Iterator接口 — 迭代器接口 Aggregate — 表示集合的接口 其中提供创建Iterator接口 只有继承他才有集合的能力 BookShelf — 书架类 实现Aggregate接口 实现接口方法返回一个迭代器类 并且他有书籍数组 Book — 书的实体类 BookShelfIterator — 书架遍历迭代器实体类 实现了迭代器接口 代码实现 1234567/** * 迭代器接口 */public interface Iterator &#123; public abstract boolean hasNext(); public abstract Object next();&#125; 1234567/** * 集合接口 其中有迭代器方法 代表可遍历集合 */public interface Aggregate &#123; public abstract Iterator getIterator();&#125; 1234567891011121314151617181920212223242526/** * 书架类 实现集合接口 拥有迭代器的能力 */public class BookShelf implements Aggregate&#123; private Book[] books; private int last = 0; public BookShelf(int maxsize) &#123; this.books = new Book[maxsize]; &#125; public Book getBookAt(int index) &#123; return books[index]; &#125; public void appendBook(Book book) &#123; this.books[last] = book; last++; &#125; public int getLength() &#123; return last; &#125; @Override public Iterator getIterator() &#123; return new BookShelfIterator(this); &#125;&#125; 12345678910111213141516171819202122232425/** * 书架的具体迭代器 * @author luhumu */public class BookShelfIterator implements Iterator&#123; private BookShelf bookShelf; private int index; public BookShelfIterator(BookShelf bookShelf) &#123; this.bookShelf = bookShelf; this.index = 0; &#125; @Override public boolean hasNext() &#123; return index &lt; bookShelf.getLength(); &#125; @Override public Object next() &#123; Book book = bookShelf.getBookAt(index); index++; return book; &#125;&#125; 12345678910111213141516171819public class Book &#123; private String name; public Book(String name) &#123; this.name = name; &#125; public Book() &#123; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125; 123456789101112public static void main(String[] args) &#123; BookShelf bookShelf = new BookShelf(3); bookShelf.appendBook(new Book(&quot;A long time goodbye&quot;)); bookShelf.appendBook(new Book(&quot;bbbbb&quot;)); bookShelf.appendBook(new Book(&quot;ccccc&quot;)); Iterator iterator = bookShelf.getIterator(); while (iterator.hasNext()) &#123; Book book = (Book) iterator.next(); System.out.println(book.getName()); &#125; &#125;// 如果我们在这里遍历循环（for） 一定会暴露BookShelf中的方法 而使用迭代器不用 他内部的next已经实现了 迭代器中的角色 迭代器角色 Iterator接口 具体的迭代器角色 BookShelfIterator实现类 Aggregate 集合接口 具体的集合接口 BookShelf类 对于集合接口和迭代器接口是一个对应的关系 只要对方变了 另一个就得做出相应改变 为什么非要使用迭代器这种设计模式呢？ 对于以上程序来说 可以使用for循环的方式来获取元素 使用迭代器可以使得遍历和实现分离开 实现解耦合 对于以上代码来说 无论BookShelf如何变化 只要返回的BookShelfIterator类没有问题（可以根据BookShelf实例化） 代码就可以运行 健壮性强 迭代器模式： 一个脱离出来可复用的迭代器，他可以从集合中一个个取出元素（迭代器是可以从后向前取数据 具体看如何实现next方法） 提供一种方法顺序访问一个聚合对象各个元素，而又无需暴露该聚合对象的内部表示例如以上的main方法中 使用迭代器遍历 没有暴露集合中的方法 因为迭代器的next封装了 java中list的Iterator 主要有几大模块 Collection，集合方法 用于对自定义的数据结构添加通常方法: add()、remove()、 iterator() Iterable，提供获取迭代器， 这个接口类会被Collection继承 Iterator&lt;E&gt; iterator(); Iterator，提供两个方法的定义；hasNext() next() 最大的特点就是可复用 适配器 Adapter要点：将一个类的接口转换为用户希望的另外一个接口，适配器模式使得原本由于接口不兼容而不能一起工作的那些类可以一起工作系统想要接入的接口满足一种规范 只有满足这个规范的才能接入 此时如何将原本不符合的转为符合规范接口 实现的就是适配器例如：在我聚合搜索的项目中 假设所有结果有新的接入 例如说视频，文档等等 但是这些接入的接口需要满足我们项目的要求 不是所有的接口都适合我们项目 这些接口需要有分页等等 但是这些接口的方法不满足 这个时候就需要引入一个适配器接口 他用来将这些接入接口转换为项目适配的接口 生活中的例子：手机充电 不能直接把电给他 手机接收不你 当然我们也没办法手搓雷电 就像排插 我们没办法把手机直接放在排插中获取电（因为电压会直接把手机搞爆炸） 这个时候需要一个适配器 将电压转为手机可以接受的电压 这个适配器就是充电器又例如： 人没办法用手直接获取100度的热水（无情铁手当我没说） 这个时候就需要用杯子来装 使得人获取到热水 这个杯子充当了适配器的角色 适配器不仅仅可以适配接口 还可以适配一些属性信息 案例场景模拟营销系统中，大部分常见的是 裂变、拉客例如： 邀请一个用户注册 或者下单 平台就会有返利金 多邀多得 总而言之 这样就会有很多的订单（内部订单 开户 外部订单等等）请求进入项目 一般来说一个系统就会接受各种各样的MQ消息或者接口 这就意味着项目需要对每一个类型的订单都做对应的处理（每一个都需要 controller dto vo service等等） 如果一个个去开发就会耗费很大的成本 也会对后期的拓展有影响 此时就希望有一个系统可以将所有外来的订单转为我们想要的配置代码示例：没有使用设计模式之前三个对应mq的消息的映射类 12345678910111213141516171819202122232425262728293031323334/** * 注册开户MQ */public class Create_account &#123; private String number; // 编号 private String address; // 开户地 private Date accountDate; // 开户时间 private String desc; // 开户描述 // ...get set toString()&#125;/** * 内部订单MQ */public class OrderMq &#123; private String uid; // 用户id private String sku; // 商品 private String orderId; // 订单id private Date createOrderTime; // 下单时间 // ...get set toString()&#125;/** * 内部订单MQ */public class POPOrderDelivered &#123; private String uid; // 用户id private String orderId; // 订单id private Date orderTime; // 下单时间 private String sku; // 商品 private String skuName; // 商品名称 private BigDecimal decimal; // 金额 // ...get set toString()&#125; 多少个mq就有多少对应的处理类 java接收mq消息 并执行业务 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * 对应mq的开户处理类 */public class CreateAccountService &#123; public void onMessage(String message) &#123; // 这里只是模拟 将mq消息转为对应对象 Create_account account = JSON.parseObject(message, Create_account.class); // 执行业务 createAccount(account.getNumber()); &#125; public Long createAccount(String number) &#123; System.out.println(&quot;创建成功&quot; + number); return 1L; &#125;&#125;/** * 对应mq的订单处理类 */public class OrderService &#123; public void onMessage(String message) &#123; // 这里只是模拟 将mq消息转为对应对象 OrderMq orderMq = JSON.parseObject(message, OrderMq.class); // 执行业务 queryUserOrderCount(orderMq.getUserId()); &#125; public Long queryUserOrderCount(String userId) &#123; System.out.println(&quot;查询用户订单&quot;); return 10L; &#125;&#125;/** * 对应mq的订单处理类 */public class POPOrderService &#123; public void onMessage(String message) &#123; // 这里只是模拟 将mq消息转为对应对象 POPOrderDelivered orderMq = JSON.parseObject(message, POPOrderDelivered.class); // 执行业务 isFirstOrder(orderMq.getUId()); &#125; public Boolean isFirstOrder(String uId) &#123; System.out.println(&quot;查询用户订单是否是首单&quot;); return true; &#125;&#125; 如果只是这样业务不做扩展 那么这么写没什么问题 但是如果考虑到扩展 可能有多个消息加入 就有需要些对应消息的实体类和service方法 使用适配器重构代码mq的适配 接口的适配 123456789/** * 统一的MQ消息体 */public class RebateInfo &#123; private String userId; // 用户id private String bizId; // 业务ID private Date bizTime; // 业务时间 private String desc; // 业务描述&#125; 123456789101112131415161718192021222324252627282930/** * 适配器类 使得所有的mq消息转为我们规定的实体类 */public class MQAdapter &#123; public static RebateInfo filter(String strJson, Map&lt;String, String&gt; link) &#123; return filter(JSON.parseObject(strJson, Map.class), link); &#125; /** * * @param obj 使用map来接收json 来做映射 * @param link * @return * @throws NoSuchMethodException * @throws InvocationTargetException * @throws IllegalAccessException */ public static RebateInfo filter(Map obj, Map&lt;String, String&gt; link) throws NoSuchMethodException, InvocationTargetException, IllegalAccessException &#123; RebateInfo rebateInfo = new RebateInfo(); for (String key : link.keySet()) &#123; Object val = obj.get(key); // 通过反射将值给rebateInfo 使得不同的mq消息转为RebateInfo对象 // 也可以使用代理类把映射的配置交给他 RebateInfo.class .getMethod(&quot;set&quot; + key.substring(0, 1).toUpperCase() + key.substring(1), String.class) .invoke(rebateInfo, val.toString()); &#125; return rebateInfo; &#125;&#125; 接口适配 就是我们聚合搜索的doseach 所有接入接口都要实现这个接口而在这里 不使用设计模式版本中 两个service的方法 判断逻辑使用方法都不同 此时我们统一了实体类 需要使得这些接口也和实体类适配 所以有一个适配接口 对应实体类 123456/** * 统一适配接口 只有实现了这个接口的业务处理类 才能执行 */public interface OrderAdapterService &#123; Boolean isFirst(String uId);&#125; 1234567891011121314/** * 对应mq的订单处理类 */public class OrderService implements OrderAdapterService &#123; public Long queryUserOrderCount(String userId) &#123; System.out.println(&quot;查询用户订单&quot;); return 10L; &#125; @Override public Boolean isFirst(String uId) &#123; return queryUserOrderCount(uId) &lt;= 1; &#125;&#125; 1234567891011121314/** * 对应mq的订单处理类 */public class POPOrderService implements OrderAdapterService&#123; public Boolean isFirstOrder(String uId) &#123; System.out.println(&quot;查询用户订单是否是首单&quot;); return true; &#125; @Override public Boolean isFirst(String uId) &#123; return isFirstOrder(uId); &#125;&#125; 适配器接口成为了统一的包装 外部使用只需要传入参数 不需要关心内部具体逻辑聚合搜索中我们对于请求也做了接口适配器 doSearch() 对于接入接口只需要实现此接口 项目才能使用此接入接口的方法 适配器模式中的角色 适配 Adapter 被适配 Adaptee 请求者 Client 对象 Target 适配器使得代码： 干净整洁容易维护 减少大量重复的判断的使用 代码更加易于维护和拓展 对于MQ这样的多种消息体中不同的属性同类的值，进行适配加上代理类，可以使用简单的配置方式接入对方提供的MQ消息，而不需要大量重复的开发，非常利于拓展 模板方法模式 Template Method（实际中很少用）定义一个操作中的算法的骨架，而将一些步骤延迟到子类中。模板方法使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤。 模板模式的核心设计思路是通过在抽象类中定义抽象方法的执行顺序，并将抽象方法设定为只有子类实现，但不设计 独立访问的方法。简单说也就是把你安排的明明白白的。就像西游记的99十一难，基本每一关都是；师傅被掳走、打妖怪、妖怪被收走（抽象类定义执行顺序），具体什么妖怪你自己定义，怎么打你想办法，最后收走还是弄死看你本事（实现类实现具体方法 但是流程一定是和抽象类一样的），我只定义执行顺序和基本策略，具体的每一难由观音（抽象类）来安排。 模板方法模式的角色 抽象类（实现模板方法 并且创建模板方法中使用的方法–子类实现） 具体类 业务场景爬虫各类电商商品 生成海报爬取分为三个步骤：模拟登录 爬取信息 生成海报严格按照步骤来 代码实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/** * 定义抽象类 * 定义具体方法 以及 模板方法 * 这个类是模板方法模式的灵魂 */public abstract class NetMall &#123; /** * 用户id */ String uId; /** * 用户密码 */ String uPwd; public NetMall(String uId, String uPwd) &#123; this.uId = uId; this.uPwd = uPwd; &#125; // protected 修饰的类和属性,对于本类、本包及其子类可见。 protected abstract Boolean login(String uId, String uPwd); /** * 爬取数据 */ protected abstract Map&lt;String, String&gt; reptile(String url); /** * 生成海报 */ protected abstract String createBase64(Map&lt;String, String&gt; goodsInfo); /** * 模板方法 * 严格按照以下步骤 不过具体实现看子类 * 用户实际执行 就是调用他 */ public String generateGoodsPoster(String skuUrl) &#123; if (!login(uId, uPwd)) &#123; return null; &#125; Map&lt;String, String&gt; reptile = reptile(skuUrl); return createBase64(reptile); &#125;&#125; 12345678910111213141516171819202122232425262728public class JDNetMall extends NetMall &#123; public JDNetMall(String uId, String uPwd) &#123; super(uId, uPwd); &#125; public Boolean login(String uId, String uPwd) &#123; logger.info(&quot;模拟京东⽤用户登录 uId：&#123;&#125; uPwd：&#123;&#125;&quot;, uId, uPwd); return true; &#125; public Map&lt;String, String&gt; reptile(String skuUrl) &#123; // 模拟发送请求 String str = HttpClient.doGet(skuUrl); Pattern p9 = Pattern.compile(&quot;(?&lt;=title\\\\&gt;).*(?=&lt;/title)&quot;); Matcher m9 = p9.matcher(str); Map&lt;String, String&gt; map = new ConcurrentHashMap&lt;String, String&gt;(); if (m9.find()) &#123; map.put(&quot;name&quot;, m9.group()); &#125; map.put(&quot;price&quot;, &quot;5999.00&quot;); logger.info(&quot;模拟京东商品爬⾍虫解析：&#123;&#125; | &#123;&#125; 元 &#123;&#125;&quot;, map.get(&quot;name&quot;), map.get(&quot;price&quot;), skuUrl); return map; &#125; public String createBase64(Map&lt;String, String&gt; goodsInfo) &#123; BASE64Encoder encoder = new BASE64Encoder(); logger.info(&quot;模拟⽣生成京东商品base64海海报&quot;); return encoder.encode(JSON.toJSONString(goodsInfo).getBytes()); &#125;&#125; 12345678910111213141516171819202122232425262728public class TaoBaoNetMall extends NetMall &#123; public TaoBaoNetMall(String uId, String uPwd) &#123; super(uId, uPwd); &#125; public Boolean login(String uId, String uPwd) &#123; logger.info(&quot;模拟淘宝用户登录 uId：&#123;&#125; uPwd：&#123;&#125;&quot;, uId, uPwd); return true; &#125; public Map&lt;String, String&gt; reptile(String skuUrl) &#123; // 模拟发送请求 String str = HttpClient.doGet(skuUrl); Pattern p9 = Pattern.compile(&quot;(?&lt;=title\\\\&gt;).*(?=&lt;/title)&quot;); Matcher m9 = p9.matcher(str); Map&lt;String, String&gt; map = new ConcurrentHashMap&lt;String, String&gt;(); if (m9.find()) &#123; map.put(&quot;name&quot;, m9.group()); &#125; map.put(&quot;price&quot;, &quot;5999.00&quot;); logger.info(&quot;模拟淘宝商品爬虫解析：&#123;&#125; | &#123;&#125; 元 &#123;&#125;&quot;, map.get(&quot;name&quot;), map.get(&quot;price&quot;), skuUrl); return map; &#125; public String createBase64(Map&lt;String, String&gt; goodsInfo) &#123; BASE64Encoder encoder = new BASE64Encoder(); logger.info(&quot;模拟⽣生成京东商品base64海海报&quot;); return encoder.encode(JSON.toJSONString(goodsInfo).getBytes()); &#125;&#125; 1234567public static void main(String[] args) &#123; // 多态实现实现这个类 NetMall netMall = new JDNetMall(&quot;1000001&quot;,&quot;*******&quot;); // generateGoodsPoster中的login等方法就是JDNetMall的 String base64 = netMall.generateGoodsPoster(&quot;https://item.jd.com/100008348542.html&quot;); logger.info(&quot;测试结果：&#123;&#125;&quot;, base64); &#125; 优点抽象类做了执行顺序 意味着子类只需要专注具体方法的实现 不需要关注其他 这就非常利于扩展和迭代 工厂方法 Factory Method （最常见的一种模式）定义一个创建对象的接口，让其子类自己决定实例化哪一个工厂类，工厂模式使其创建过程延迟到子类进行 用模板方法模式来构建生成实例的工厂（接口提供了生成实例的模板方法而具体创建什么 由实现类决定）父类决定实例的生成方式 但并不决定要生成的具体的类 具体类的处理交给子类负责 业务场景模拟积分兑换中的发放多种商品类型 假设有一下三种类型的商品接口： 优惠券—- CouponResult sendCoupon(String uId,String CouponNumber, String uuid); 实物商品— Boolean deliverGoods(DeliverReq req); 第三方爱奇艺兑换卡—- void grantToken(String bindMobileNumber, String cardId); 分析以上接口： 返回值不同 参数不同 随着业务的增加 商品接口会越来越多 没用设计模式之前 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657/** * 入参对象 */public class AwardReq &#123; private int awardType; public int getAwardType() &#123; return awardType; &#125; public void setAwardType(int awardType) &#123; this.awardType = awardType; &#125;&#125;/** * 出参对象 */public class AwardRes &#123; private String gId; private String gName; public AwardRes(String gId, String gName) &#123; this.gId = gId; this.gName = gName; &#125;&#125;/** * 接口处理 用来接收用户请求 */public class PrizeController &#123; /** * 获取入参对象 根据其中类型应该给哪一种商品 * @param req * @return */ public AwardRes awardToUser(AwardReq req) &#123; AwardRes res = null; if (req.getAwardType() == 1) &#123; CouponService couponService = new CouponService(); // 进行业务逻辑 返回结果 return new AwardRes(&quot;1&quot;,&quot;优惠券&quot;); &#125;else if (req.getAwardType() == 2) &#123; GoodsService goodsService = new GoodsService(); // 进行业务逻辑 返回结果 return new AwardRes(&quot;2&quot;, &quot;实物&quot;); &#125; else if (req.getAwardType() == 3) &#123; IQiYiCardService iQiYiCardService = new IQiYiCardService(); // 进行业务逻辑 返回结果 return new AwardRes(&quot;3&quot;, &quot;爱奇艺优惠券&quot;); &#125;else &#123; // 发放失败 return new AwardRes(&quot;0&quot;, &quot;失败&quot;); &#125; &#125;&#125; 使用ifelse判断返回 如果说项目需要扩展 就会爆炸 非常难维护 工厂模式优化 1234567891011121314/** * 这里使用了适配器模式 所有具体接口 都需要实现此接口 不然无法接入系统 保证出参一致 * */public interface ICommodity &#123; /** * 发奖接口 * @param uId 用户ID * @param commodityId 奖品ID * @param bizId 业务id * @param extMap 扩展字段 */ void sendCommodity(String uId, String commodityId, String bizId, Map&lt;String, String&gt; extMap);&#125; 12345678910111213141516171819202122232425262728293031323334353637/** * 优惠券 */public class CouponCommodityService implements ICommodity&#123; @Override public void sendCommodity(String uId, String commodityId, String bizId, Map&lt;String, String&gt; extMap) &#123; CouponService couponService = new CouponService(); // 将参数写入 // 调用方法执行发放 couponService.sendCoupon(uId, commodityId, bizId); &#125;&#125;/** * 商品 */public class GoodsCommodityService implements ICommodity&#123; @Override public void sendCommodity(String uId, String commodityId, String bizId, Map&lt;String, String&gt; extMap) &#123; GoodsService goodsService = new GoodsService(); // 将参数写入 // 调用方法执行发放 goodsService.sendGoods(uId, commodityId, bizId); &#125;&#125;/** * 爱奇艺券 */public class IQiYiCommodityService implements ICommodity&#123; @Override public void sendCommodity(String uId, String commodityId, String bizId, Map&lt;String, String&gt; extMap) &#123; IQiYiCardService iQiYiCardService = new IQiYiCardService(); // 将参数写入 // 调用方法执行发放 iQiYiCardService.sendCoupon(uId, commodityId, bizId); &#125;&#125; 12345678910111213/** * 工厂类 只提供创建类的方法 而具体是实例化哪一个 由子类决定 */public class StoreFactory &#123; public ICommodity getCommodityService(Integer commodityType) &#123; if (null == commodityType) return null; if (1 == commodityType) return new CouponCommodityService(); if (2 == commodityType) return new GoodsCommodityService(); if (3 == commodityType) return new IQiYiCommodityService(); throw new RuntimeException(&quot;不存在此商品&quot;); &#125;&#125; 工厂类： 这个工厂方法模式的核心 他定义了一个方法 这个方法实现类的实例化 但是到底实例化哪一个 交给调用方法者来执行 优点： 避免创建者与具体的产品耦合 满足单一职责 每一个业务逻辑实现都在所属自己的类中完成 满足开闭原则（ocp） 无需更改使用调用方就可以在程序中引入新的产品 单例模式 Singleton （重要）保证一个类仅有一个实例 并提供一个它的全局访问点 初始化行为只在类加载的时候执行一次 单例模式不允许外部创建 也就是不让外部使用new的方式日常开发示例： 数据库连接池不会反复创建 spring中一个单例模式bean的生成和使用 平时代码中设置全局的一些属性保存 在我们日常发开中大致都会在以上场景中使用单例模式 虽然单例模式并不复杂 但是使用面很广 七种单例模式的实现0 静态类使用 不是静态类1234567891011121314/** * 静态类的使用 * 最简单的单例模式 */public class Singleton_00 &#123; /** * 非常常见 在平常开发中经常使用 * 静态属性 在程序中只有一份 * 在第一次运行的的时候直接初始化此map类 这里也不需要延迟加载 * 不需要维持任何状态下 仅用于全局访问 这种方式更方便 * 如果需要被继承以及需要维持一些特定状态的情况下 更适合是用其他单例模式 */ public static Map&lt;String, String&gt; cache = new ConcurrentHashMap&lt;&gt;();&#125; 1 懒汉模式（线程不安全）123456789101112131415161718192021222324252627/** * 单例模式 * 1 懒汉模式 （线程不安全） */public class Singleton_01 &#123; /** * 类加载的时候 静态属性初始化 懒加载就是先初始化 这里实例化延迟到方法调用getSingleton01() */ private static Singleton_01 singleton01; /** * 之所以是private 是因为单例模式的特点就是 不允许外部直接创建 */ private Singleton_01() &#123; &#125; /** * 提供静态方法 返回对象 也就是单例模式 提供一个全局访问实例点 满足懒加载 * 这个方法是线程不安全的 如果多个线程同时调用此方法 就无法保证单例（只有一个类） 多个实例并存 * @return */ public static Singleton_01 getSingleton01() &#123; if (null != singleton01) return singleton01; singleton01 = new Singleton_01(); return singleton01; &#125;&#125; 懒加载的本质：一个类要创建实例需要先加载并初始化该类，可以简单的理解为，**一个类的实例的最终形态需要先进行初始化，再进行实例化 **所谓懒加载 就是延迟实例化 2 懒汉模式（线程安全）12345678910111213141516171819202122232425262728/** * 单例模式 * 2 懒汉模式 （线程安全） */public class Singleton_02 &#123; /** * 类加载的时候 静态属性初始化 懒加载就是先初始化 这里实例化延迟到方法调用getSingleton02() */ private static Singleton_02 singleton02; /** * 之所以是private 是因为单例模式的特点就是 不允许外部直接创建 */ private Singleton_02() &#123; &#125; /** * 提供静态方法 返回对象 也就是单例模式 提供一个全局访问实例点 满足懒加载 * 这个方法是线程安全的 但是因为方法上加锁 所有的访问都因为需要锁占用导致资源的浪费 * 因此若非特殊情况 不建议此方式实现单例 * @return */ public static synchronized Singleton_02 getSingleton02() &#123; if (null != singleton02) return singleton02; singleton02 = new Singleton_02(); return singleton02; &#125;&#125; 对比不安全的懒汉模式 这里只是加了锁保证线程安全 其余没有区别 3 饿汉模式（线程安全）12345678910111213141516171819202122232425/** * 单例模式 * 2 饿汉模式 （线程安全） */public class Singleton_03 &#123; /** * 类加载的时候 静态属性初始化并且实例化 就像01中的map 这种方式不是懒加载的 * 也就是无论程序中用不用到这个类 都会创建出来 * 这种情况导致的问题就是 假设我们玩塞尔达 打开游戏 我们只是在初始台地 而他把全地图加载好了 那任天堂机子早负荷不了就爆炸了 */ private static Singleton_03 singleton03 = new Singleton_03(); /** * 之所以是private 是因为单例模式的特点就是 不允许外部直接创建 */ private Singleton_03() &#123; &#125; /** * 类加载的时候 就将对象实例化 因为只有一份 这里提供全局访问点 获取的都是同一份对象 */ public static Singleton_03 getSingleton03() &#123; return singleton03; &#125;&#125; 4 使用类的内部类（线程安全） 推荐的方式123456789101112131415161718192021222324252627/** * 单例模式 * 4 使用类的静态内部类 （线程安全） */public class Singleton_04 &#123; /** * 使用静态内部类初始化 只有当这个他被调用的时候 才会初始化 * 这里设置为私有 也就是只有getSingleton04()方法被调用才会被初始化 实例化改对象 * 这样的好处就是支持了懒加载 不会一开始就实例化 */ private static class SingletonHolder &#123; private static Singleton_04 singleton04 = new Singleton_04(); &#125; /** * 之所以是private 是因为单例模式的特点就是 不允许外部直接创建 */ private Singleton_04() &#123; &#125; /** * 提供全局访问点 获取的都是静态内部类中的同一份对象 */ public static Singleton_04 getSingleton04() &#123; return SingletonHolder.singleton04; &#125;&#125; 之所以线程安全 这主要是因为JVM虚拟机可以保证多线程并发访问的正确性也就是一个类的构造方法在多线程环境下可以被正确的加载 5 双重锁校验（线程安全）1234567891011121314151617181920212223242526272829303132/** * 单例模式 * 5 双重锁校验实现单例 （线程安全） */public class Singleton_05 &#123; /** * 类加载的时候 静态属性初始化 */ private static Singleton_05 singleton05; /** * 之所以是private 是因为单例模式的特点就是 不允许外部直接创建 */ private Singleton_05() &#123; &#125; /** * 提供静态方法 返回对象 也就是单例模式 提供一个全局访问实例点 满足懒加载 * 这种方式对比在方法上加锁 是对他的优化 减少了了部分获取实例的耗时 * @return */ public static Singleton_05 getsingleton05() &#123; if (null != singleton05) return singleton05; synchronized (Singleton_05.class) &#123; if (null == singleton05) &#123; singleton05 = new Singleton_05(); &#125; &#125; return singleton05; &#125;&#125; 6 CAS[AtomicReference] （线程安全）使用java并发库的原子类来支持并发访问的数据安全 12345678910111213141516171819202122232425262728293031323334353637/** * 单例模式 * 6 使用CAS （线程安全） */public class Singleton_06 &#123; /** * 通过并发库的原子类AtomicReference */ private static final AtomicReference&lt;Singleton_06&gt; INSTANCE = new AtomicReference&lt;&gt;(); public static Singleton_06 singleton06; /** * 之所以是private 是因为单例模式的特点就是 不允许外部直接创建 */ private Singleton_06() &#123; &#125; /** * 提供静态方法 返回对象 也就是单例模式 提供一个全局访问实例点 满足懒加载 * 这种方式对比在方法上加锁 是对他的优化 减少了了部分获取实例例的耗时 * @return */ public static final Singleton_06 getsingleton06() &#123; while (true) &#123; Singleton_06 instance = INSTANCE.get(); if (null != instance) return instance; INSTANCE.compareAndSet(null, new Singleton_06()); return INSTANCE.get(); &#125; &#125; public static void main(String[] args) &#123; // 输出一致 System.out.println(Singleton_06.getsingleton06()); System.out.println(Singleton_06.getsingleton06()); &#125;&#125; java并发库提供了了很多原子类来支持并发访问的数据安全性 :AtomicInteger AtomicBoolean AtomicLong AtomicReference AtomicReference可以封装引用一个V实例 支持并发访问如上的单例方式就是使用了这样的特点 用CAS的好处就是不需要使用传统加锁方式包装线程安全 而是依赖于CAS的忙等算法 依赖底层硬件支持 来保证线程安全 相对于其他锁的实现没有线程的切换和阻塞也就没有了额外的开销，并且支持较大的并发性 CAS的缺点就是忙等 一直没有获取到将会处于死循环中 7 Effective 枚举 Java作者推荐的枚举单例 （线程安全） 约书亚 布洛克 美国著名程序员 他为Java平台设计并实作了许多的功能 曾担任Google的首席Java架构师 123456789/** * 7 枚举方式 */public enum Singleton_07 &#123; INSTANCE; public void test() &#123; System.out.println(&quot;hi~&quot;); &#125;&#125; 这种写法在功能上与共有域方法相近 但是更简洁 无偿地提供了串行化机制 绝对防止了对此实例化（枚举无法实例化） 即使是在面对复杂的串行化或者反射攻击的时候。虽然这种方法还没有广泛采用，但是单元素的枚举类型已经成为实现 Singleton 的最佳⽅方法。但是在继承场景下是不可用的 总结： 一个很平常的单例模式（但是重要） 但在各种的实现上真的可以看到java的基本功的体现 在平时的开发中如果可以确保此类是全局可用不需要做懒加载 那么直接创建并给外部调用即可 但如果是很多的类 有的需要用户触发才显示 那就必须用懒加载 线程的安全上可以按需选择 原型模式 prototype 使用频率不高用原型实例指定创建对象的种类，并且通过拷贝这些原型创建新的对象。 原型模式主要解决的问题就是创建重复对象，而这部分 对象 内容本身比较复杂，生成过程可能从库或者RPC接口中获取数据的耗时较长，因此采用克隆的方式节省时间。其实这种场景经常出现在我们的身边： CTRL+C CTRL+V 复制粘贴代码 Java多数类中提供的API Object clone(); 细胞的有丝分裂 通过实例原型 生成新的实例 案例场景以前都是统一一套试卷 现在不同了 考试将试题打乱顺序 这样防止了抄袭同一套题目 乱序需要实现一个上机考试抽题的服务 因此构建一个题库题目的场景类信息 用于创建 选择题 问答题 代码示例： 123456789101112131415161718192021222324252627282930public class AnswerQuestion &#123; private String name; // 问题 private String key; // 答案 public AnswerQuestion() &#123; &#125; public AnswerQuestion(String name, String key) &#123; this.name = name; this.key = key; &#125; // ...get/set&#125;public class ChoiceQuestion &#123; private String name; // 题⽬ private Map&lt;String, String&gt; option; // 选项；A、B、C、D private String key; // 答案；B public ChoiceQuestion() &#123; &#125; public ChoiceQuestion(String name, Map&lt;String, String&gt; option, String key) &#123; this.name = name; this.option = option; this.key = key; &#125; // ...get/set&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class QuestionBankController &#123; public String createPaper(String candidate, String number) &#123; List&lt;ChoiceQuestion&gt; choiceQuestionList = new ArrayList&lt;&gt;(); List&lt;AnswerQuestion&gt; answerQuestionList = new ArrayList&lt;&gt;(); Map&lt;String, String&gt; map01 = new HashMap&lt;&gt;(); map01.put(&quot;A&quot;, &quot;JAVA2 EE&quot;); map01.put(&quot;B&quot;, &quot;JAVA2 Card&quot;); map01.put(&quot;C&quot;, &quot;JAVA2 ME&quot;); map01.put(&quot;D&quot;, &quot;JAVA2 HE&quot;); map01.put(&quot;E&quot;, &quot;JAVA2 SE&quot;); Map&lt;String, String&gt; map02 = new HashMap&lt;&gt;(); map02.put(&quot;A&quot;, &quot;JAVA程序的main⽅方法必须写在类⾥⾯&quot;); map02.put(&quot;B&quot;, &quot;JAVA程序中可以有多个main⽅方法&quot;); map02.put(&quot;C&quot;, &quot;JAVA程序中类名必须与⽂文件名⼀一样&quot;); map02.put(&quot;D&quot;, &quot;JAVA程序的main⽅方法中如果只有⼀一条语句句，可以不不⽤用&#123;&#125;(⼤大括号)括起来&quot;); Map&lt;String, String&gt; map03 = new HashMap&lt;&gt;(); map03.put(&quot;A&quot;, &quot;变量量由字⺟母、下划线、数字、$符号随意组成；&quot;); map03.put(&quot;B&quot;, &quot;变量量不不能以数字作为开头；&quot;); map03.put(&quot;C&quot;, &quot;A和a在java中是同⼀一个变量量；&quot;); map03.put(&quot;D&quot;, &quot;不不同类型的变量量，可以起相同的名字；&quot;); Map&lt;String, String&gt; map04 = new HashMap&lt;&gt;(); map04.put(&quot;A&quot;, &quot;STRING&quot;); map04.put(&quot;B&quot;, &quot;x3x;&quot;); map04.put(&quot;C&quot;, &quot;void&quot;); map04.put(&quot;D&quot;, &quot;de$f&quot;); Map&lt;String, String&gt; map05 = new HashMap&lt;&gt;(); map05.put(&quot;A&quot;, &quot;31&quot;); map05.put(&quot;B&quot;, &quot;0&quot;); map05.put(&quot;C&quot;, &quot;1&quot;); map05.put(&quot;D&quot;, &quot;2&quot;); // 加入了非常多的对象 每一道题都需要创建一个对象 choiceQuestionList.add(new ChoiceQuestion(&quot;JAVA所定义的版本中不不包括&quot;, map01, &quot;D&quot;)); choiceQuestionList.add(new ChoiceQuestion(&quot;下列列说法正确的是&quot;, map02, &quot;A&quot;)); choiceQuestionList.add(new ChoiceQuestion(&quot;变量量命名规范说法正确的是&quot;, map03, &quot;B&quot;)); choiceQuestionList.add(new ChoiceQuestion(&quot;以下()不不是合法的标识符&quot;, map04, &quot;C&quot;)); choiceQuestionList.add(new ChoiceQuestion(&quot;表达式(11+3*8)/4%3的值 是&quot;, map05, &quot;D&quot;)); answerQuestionList.add(new AnswerQuestion(&quot;⼩小红⻢马和⼩小⿊黑⻢马⽣生的⼩小⻢马⼏几条 腿&quot;, &quot;4条腿&quot;)); answerQuestionList.add(new AnswerQuestion(&quot;铁棒打头疼还是⽊木棒打头疼&quot;, &quot;头最疼&quot;)); answerQuestionList.add(new AnswerQuestion(&quot;什什么床不不能睡觉&quot;, &quot;⽛牙床&quot;)); answerQuestionList.add(new AnswerQuestion(&quot;为什什么好⻢马不不吃回头草&quot;, &quot;后面的草没了了&quot;));// 输出结果 return &quot;结果&quot;; &#125;&#125; 以上没使用设计模式 每一道题目用对象的方式来封装 有多少道题目就需要new多少个同样的对象以上代码面对过程 不面向对象 不考虑扩展性另外对于以上代码而言 无法乱序 增加乱序也可以但复杂度会大大增高 原型模式模型结构 12345678910111213141516171819202122232425262728293031323334353637383940414243public class QuestionBank implements Cloneable &#123; private String candidate; // 考⽣生 private String number; // 考号 private ArrayList&lt;ChoiceQuestion&gt; choiceQuestionList = new ArrayList&lt;&gt;(); private ArrayList&lt;AnswerQuestion&gt; answerQuestionList = new ArrayList&lt;&gt;(); public QuestionBank append(ChoiceQuestion choiceQuestion) &#123; choiceQuestionList.add(choiceQuestion); return this; &#125; public QuestionBank append(AnswerQuestion answerQuestion) &#123; answerQuestionList.add(answerQuestion); return this; &#125; @Override public Object clone() throws CloneNotSupportedException &#123; QuestionBank questionBank = (QuestionBank) super.clone(); questionBank.choiceQuestionList = (ArrayList&lt;ChoiceQuestion&gt;) choiceQuestionList.clone(); questionBank.answerQuestionList = (ArrayList&lt;AnswerQuestion&gt;) answerQuestionList.clone(); // 题目乱序 Collections.shuffle(questionBank.choiceQuestionList); Collections.shuffle(questionBank.answerQuestionList); // 答案乱序 ArrayList&lt;ChoiceQuestion&gt; choiceQuestionList = questionBank.choiceQuestionList; for (ChoiceQuestion question : choiceQuestionList) &#123; Topic random = TopicRandomUtil.random(question.getOption(), question.getKey()); question.setOption(random.getOption()); question.setKey(random.getKey()); &#125; return questionBank; &#125; public void setCandidate(String candidate) &#123; this.candidate = candidate; &#125; public void setNumber(String number) &#123; this.number = number; &#125;&#125; 通过clone的方式复制对象 总结： 原型模式的使用频率不高 优点：通过clone创建复杂对象、避免重复初始化操作 不需要与其他类耦合 缺点：如果对象中包括了循环引用的克隆 以及类中深度引用对象的克隆 会让模式变得麻烦 **永远不要想硬凑设计模式 否则会引起过度设计 ** 初期是代码的优化 中期是设计模式的使用 后期是对全局服务的搭建 不断加强自身对全局的掌控 也加深对细节的处理 可上可下才是一个程序员最佳的处理方式 建造者模式 builder将一个复杂的构建与其表示 相分离，使得同样的构建过程可以创建不同的表示 建造者模式所完成的内容就是通过将多个简单对象通过一步步的组装构建出一个复杂对象的过程 场景：模拟装修公司对于设计出一些套餐装修服务的场景很多装修公司都会给出自家的套餐服务，一般有；欧式豪华、轻奢田园、现代简约等等，而这些套餐的后面是不同的商品的组合。例如；一级&amp;二级吊顶、多乐士涂料、圣象地板、马可波罗地砖等等，按照不同的套餐的价格选取不同的品牌组合，最终再按照装修面积给出一个整体的报价。这里我们就模拟装修公司想推出一些套餐装修服务，按照不同的价格设定品牌选择组合，以达到使用建造者模式的过程。 代码实现：不使用建造者模式 1234567891011121314/** * 统一的 为所有的装修材料提供了基本的信息 */public interface Matter &#123; String scene(); // 场景；地板、地砖、涂料料、吊顶 String brand(); // 品牌 String model(); // 型号 BigDecimal price(); // 价格 String desc(); // 描述&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889public class LevelOneCeiling implements Matter &#123; @Override public String scene() &#123; return &quot;吊顶&quot;; &#125; @Override public String brand() &#123; return &quot;装修公司自带&quot;; &#125; @Override public String model() &#123; return &quot;一级顶&quot;; &#125; @Override public BigDecimal price() &#123; return new BigDecimal(260); &#125; @Override public String desc() &#123; return &quot;造型只做低一级，只有一个层次的吊顶，一般离顶120-150mm&quot;; &#125;&#125;public class DuluxCoat implements Matter &#123; public String scene() &#123; return &quot;涂料料&quot;; &#125; public String brand() &#123; return &quot;多乐⼠士(Dulux)&quot;; &#125; public String model() &#123; return &quot;第二代&quot;; &#125; public BigDecimal price() &#123; return new BigDecimal(719); &#125; public String desc() &#123; return &quot;多乐⼠士是阿克苏诺贝尔旗下的著名建筑装饰油漆品牌，产品畅销于全球100个国家，每年全球有5000万户家庭使用多乐士油漆。&quot;; &#125;&#125;public class DongPengTile implements Matter &#123; public String scene() &#123; return &quot;地砖&quot;; &#125; public String brand() &#123; return &quot;东鹏瓷砖&quot;; &#125; public String model() &#123; return &quot;10001&quot;; &#125; public BigDecimal price() &#123; return new BigDecimal(102); &#125; public String desc() &#123; return &quot;东鹏瓷砖以品质铸就品牌，科技推动品牌，⼝口碑碑传播品牌为宗旨，2014年年品牌 价值132 .35 亿元，位列列建陶⾏行行业榜⾸首。&quot;; &#125;&#125;public class DerFloor implements Matter &#123; public String scene() &#123; return &quot;地板&quot;; &#125; public String brand() &#123; return &quot;德尔(Der)&quot;; &#125; public String model() &#123; return &quot;A+&quot;; &#125; public BigDecimal price() &#123; return new BigDecimal(119); &#125; public String desc() &#123; return &quot;DER德尔集团是全球领先的专业⽊木地板制造商，北北京2008年年奥运会家装和公装地板供应商&quot;; &#125;&#125; 以上是具体类型 可能有很多 这里只是列举一点 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public String getMatterList(BigDecimal area, Integer level) &#123; List&lt;Matter&gt; list = new ArrayList&lt;Matter&gt;(); // 装修清单 BigDecimal price = BigDecimal.ZERO; // 装修价格// 豪华欧式 if (1 == level) &#123; LevelTwoCeiling levelTwoCeiling = new LevelTwoCeiling(); // 吊顶，二级顶 DuluxCoat duluxCoat = new DuluxCoat(); // 涂料料，多乐⼠ ShengXiangFloor shengXiangFloor = new ShengXiangFloor(); // 地板，圣象 list.add(levelTwoCeiling); list.add(duluxCoat); list.add(shengXiangFloor); price = price.add(area.multiply(new BigDecimal(&quot;0.2&quot;)).multiply(levelTwoCeiling.price())); price = price.add(area.multiply(new BigDecimal(&quot;1.4&quot;)).multiply(duluxCoat.price())); price = price.add(area.multiply(shengXiangFloor.price())); &#125;// 轻奢⽥田园 if (2 == level) &#123; LevelTwoCeiling levelTwoCeiling = new LevelTwoCeiling(); // 吊顶，⼆二级顶 LiBangCoat liBangCoat = new LiBangCoat(); // 涂料料，⽴立邦 MarcoPoloTile marcoPoloTile = new MarcoPoloTile(); // 地砖，⻢马可波罗 list.add(levelTwoCeiling); list.add(liBangCoat); list.add(marcoPoloTile); price = price.add(area.multiply(new BigDecimal(&quot;0.2&quot;)).multiply(levelTwoCeiling.price())); price = price.add(area.multiply(new BigDecimal(&quot;1.4&quot;)).multiply(liBangCoat.price())); price = price.add(area.multiply(marcoPoloTile.price())); &#125;// 现代简约 if (3 == level) &#123; LevelOneCeiling levelOneCeiling = new LevelOneCeiling(); //吊顶，⼆二级顶 LiBangCoat liBangCoat = new LiBangCoat(); //涂料料，⽴立邦 DongPengTile dongPengTile = new DongPengTile(); //地砖，东鹏 list.add(levelOneCeiling); list.add(liBangCoat); list.add(dongPengTile); price = price.add(area.multiply(new BigDecimal(&quot;0.2&quot;)).multiply(levelOneCeiling.price())); price = price.add(area.multiply(new BigDecimal(&quot;1.4&quot;)).multiply(liBangCoat.price())); price = price.add(area.multiply(dongPengTile.price())); &#125; StringBuilder detail = new StringBuilder(&quot;\\r\\n-------------------------------------------------------\\r\\n&quot; + &quot;装修清单&quot; + &quot;\\r\\n&quot; + &quot;套餐等级：&quot; + level + &quot;\\r\\n&quot; + &quot;套餐价格：&quot; + price.setScale(2, BigDecimal.ROUND_HALF_UP) + &quot; 元\\r\\n&quot; + &quot;房屋⾯面积：&quot; + area.doubleValue() + &quot; 平⽶米\\r\\n&quot; + &quot;材料料清单：\\r\\n&quot;); for (Matter matter : list) &#123; detail.append(matter.scene()).append(&quot;：&quot;).append(matter.brand()).append( &quot;、&quot;).append(matter.model()).append(&quot;、平⽶米价格：&quot;).append(matter.price()).append(&quot; 元。\\n&quot;); &#125; return detail.toString(); &#125; 以上根据选用等级来划分 返回的方案如果只是这些 没有问题 但是随着业务的增加 可能有几千种方案 ifelse就会臃肿建造者模式重构代码建造者模式主要解决的问题是在软件系统中，有时候面临着”一个复杂对象”的创建工作，其通常由各个部分的子对象用一定的过程构成；由于需求的变化，这个复杂对象的各个部分经常面临着重大的变化，但是将它们组合在一起的过程却相对稳定。 1234567891011121314/** * 接口类中定义了填充各项物料的方法 */public interface IMenu &#123; IMenu appendCeiling(Matter matter); // 吊顶 IMenu appendCoat(Matter matter); // 涂料料 IMenu appendFloor(Matter matter); // 地板 IMenu appendTile(Matter matter); // 地砖 String getDetail(); // 明细&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * 装修包实现 可以使得用户根据不同的材料生成对应的方案 */public class DecorationPackageMenu implements IMenu &#123; private List&lt;Matter&gt; list = new ArrayList&lt;Matter&gt;(); // 装修清单 private BigDecimal price = BigDecimal.ZERO; // 装修价格 private BigDecimal area; // ⾯面积 private String grade; // 装修等级；豪华欧式、轻奢⽥田园、现代简约 private DecorationPackageMenu() &#123; &#125; public DecorationPackageMenu(Double area, String grade) &#123; this.area = new BigDecimal(area); this.grade = grade; &#125; public IMenu appendCeiling(Matter matter) &#123; list.add(matter); price = price.add(area.multiply(new BigDecimal(&quot;0.2&quot;)).multiply(matter.price())); return this; &#125; public IMenu appendCoat(Matter matter) &#123; list.add(matter); price = price.add(area.multiply(new BigDecimal(&quot;1.4&quot;)).multiply(matter.price())); return this; &#125; public IMenu appendFloor(Matter matter) &#123; list.add(matter); price = price.add(area.multiply(matter.price())); return this; &#125; public IMenu appendTile(Matter matter) &#123; list.add(matter); price = price.add(area.multiply(matter.price())); return this; &#125; public String getDetail() &#123; StringBuilder detail = new StringBuilder(&quot;\\r\\n------------------------------------------------------ -\\r\\n&quot; + &quot;装修清单&quot; + &quot;\\r\\n&quot; + &quot;套餐等级：&quot; + grade + &quot;\\r\\n&quot; + &quot;套餐价格：&quot; + price.setScale(2, BigDecimal.ROUND_HALF_UP) + &quot; 元\\r\\n&quot; + &quot;房屋⾯面积：&quot; + area.doubleValue() + &quot; 平⽶米\\r\\n&quot; + &quot;材料料清单：\\r\\n&quot;); for (Matter matter : list) &#123; detail.append(matter.scene()).append(&quot;：&quot;).append(matter.brand()).append( &quot;、&quot;).append(matter.model()).append(&quot;、平⽶米价格：&quot;).append(matter.price()).append(&quot; 元。\\n&quot;); &#125; return detail.toString(); &#125;&#125; 12345678910111213141516171819202122232425/** * 构建者类 根据不同类型 构建不同的明细 */public class Builder &#123; public IMenu levelOne(Double area) &#123; return new DecorationPackageMenu(area, &quot;豪华欧式&quot;) .appendCeiling(new LevelTwoCeiling()) // 吊顶，⼆二级顶 .appendCoat(new DuluxCoat()) // 涂料料，多乐⼠士 .appendFloor(new ShengXiangFloor()); // 地板，圣象 &#125; public IMenu levelTwo(Double area) &#123; return new DecorationPackageMenu(area, &quot;轻奢⽥田园&quot;) .appendCeiling(new LevelTwoCeiling()) // 吊顶，⼆二级顶 .appendCoat(new LiBangCoat()) // 涂料料，⽴立邦 .appendTile(new MarcoPoloTile()); // 地砖，⻢马可波罗 &#125; public IMenu levelThree(Double area) &#123; return new DecorationPackageMenu(area, &quot;现代简约&quot;) .appendCeiling(new LevelOneCeiling()) // 吊顶，⼆二级顶 .appendCoat(new LiBangCoat()) // 涂料料，⽴立邦 .appendTile(new DongPengTile()); // 地砖，东鹏 &#125;&#125; 目前的代码结构可以让你很方便的 很有调理的进行扩展业务开发。而不是以往一样把所有代码都写到 ifelse 里面 根据装修包 可以装修出多种组合 原始类不变 但是他们组合在一起就会发生变化 总结：什么时候会选择这样的设计模式，当一些基本物料不会变，而其组合经常变化的时候 ，就可以选择这样的设计模式来构建代码。此设计模式满足了单一职责原则以及可复用的技术、建造者独立、易扩展、便于控制细节风险。但同时当出现特别多的物料以及很多的组合后，类的不断扩展也会造成难以维护的问题 抽象工厂模式 Abstract Factory提供一个创建一系列相关依赖对象的接口，而无需指定他们的具体的类 **编程中“抽象”的概念：不考虑具体怎样实现，而是仅关注接口 **例如： 抽象方法 只是定义这个方法 而不是具体实现 忘记方法的具体实现（假装忘记） 使用抽象方法进行编程 在使用工厂模式时，只需要关心降低耦合度的目的是否达到了 因为工厂模式就是为了解耦合 工厂方法模式：工厂类 只提供创建类的方法 而具体是实例化哪一个 由子类决定 只是一个创建具体类的方法抽象工厂模式：抽线工厂 是用来创建工厂的 **抽象工厂是⼀个中心工厂，创建其他工厂的模式 **抽象工厂就像工厂，而工厂方法则像是工厂的一种产品生产线一个抽象工厂类，派生出多个具体工厂类。而每个具体工厂类只能创建一个具体产品类的实例。 同一个功能不同的命令 一个工厂中存放不同的实现工厂例如： 不同系统的回车命令不同 例如win是\\r\\n（&lt;换行&gt;&lt;回车&gt;） Unix是\\n（&lt;换行&gt;） Mac是&lt;回车&gt; 一个换行工厂 里面有三种不同的工厂换行 分别是win Unix Mac工厂 IDEA开发工具的异常 Win&#x2F;Mac 功能一致但是名字不同 **以上这种例子在业务中非常常见 开发业务也会遇到类似问题 需要兼容做处理 **功能一致但是应用实现有区别 场景模拟创建两个接口Shape和Color 实现他们的实体类创建抽象工厂AbstractFactory 用来生成工厂类 ShapeFactory和ColorFactory 这两个类用来创建对应对象定义工厂类生成器 FactoryProducer 来获取具体的AbstractFactory在跟具体工厂来获取具体形状具体颜色的物品根据抽象工厂获取具体需要的工厂 根据工厂创建出自己需要的类 代码实现 123456789101112131415161718192021public interface Color &#123; void fill();&#125;public class Blue implements Color &#123; @Override public void fill() &#123; System.out.println(&quot;Blue&quot;); &#125;&#125;public class Green implements Color &#123; @Override public void fill() &#123; System.out.println(&quot;Green&quot;); &#125;&#125;public class Red implements Color &#123; @Override public void fill() &#123; System.out.println(&quot;red&quot;); &#125;&#125; 123456789101112131415161718192021public interface Shape &#123; void draw();&#125;public class Circle implements Shape &#123; @Override public void draw() &#123; System.out.println(&quot;圆形&quot;); &#125;&#125;public class Rectangle implements Shape &#123; @Override public void draw() &#123; System.out.println(&quot;三角&quot;); &#125;&#125;public class Square implements Shape &#123; @Override public void draw() &#123; System.out.println(&quot;正方形&quot;); &#125;&#125; 1234567891011121314/** * 工厂创建者 */public class FactoryProducer &#123; public static AbstractFactory getFactory(String choice) &#123; if (choice.equals(&quot;shape&quot;)) &#123; return new ShapeFactory(); &#125;else if (choice.equals(&quot;color&quot;)) &#123; return new ColorFactory(); &#125;else &#123; return null; &#125; &#125;&#125; 12345public static void main(String[] args) &#123; AbstractFactory factory = FactoryProducer.getFactory(&quot;color&quot;); Color red = factory.getColor(&quot;red&quot;); red.fill();&#125; 以上使用工厂模式的好处 抽象工厂模式隔离了具体类的生产，使得客户并不需要知道什么被创建。 当一个产品族中的多个对象被设计成一起工作时，它能保证客户端始终只使用同一个产品族中的对象 增加新的具体工厂和产品族很方便，无须修改已有系统，符合“开闭原则” 抽象工厂模式，所要解决的问题就是在一个产品族（多个工厂），存在多个不同类型的产品(Redis集群、操作系统，例如以上颜色有多个类)情况下，接口选择的问题。而这种场景在业务开发中也是非常多见的，只不过可能有时候没有将它们抽象化出来。这个模式满足单一职责、开闭原则、解耦等优点 简单工厂模式不是23中设计模式之一也是用来创建对象实现非常简单一个静态方法 调用者指定具体实例类 桥接模式 bridge**将抽象部分 与 实现部分 分离，使他们都可以独立变化 ****例如：支付平台(wechat,支付宝，同时也是抽象部分)有多种 支付方式（指纹 密码 人脸，实现部分）也有多种 他们可以有多种组合 ** 它是用组合关系代替继承关系来实现，从而降低了抽象和实现这两个可变维度的耦合度。 假如你有一个几何形状（Shape）类， 从它能扩展出两个子类： 圆形（Circle）和方形（Square）。你希望对这样的类层次结构进行扩展以使其包含颜色， 所以你打算创建名为红色（Red）和蓝色（Blue）的形状子类。但是，由于你已有两个子类，所以总共需要创建四个类才能覆盖所有组合，例如蓝色圆形（BlueCircle）和红色方形（RedSquare）。在层次结构中新增形状和颜色将导致代码复杂程度指数增长。例如添加三角形状，你需要新增两个子类，也就是每种颜色一个；此后新增一种新颜色需要新增三个子类，即每种形状一个。如此以往，情况会越来越糟糕。 解决办法：问题的根本原因是我们试图在两个独立的维度——形状与颜色——上扩展形状类。这在处理类继承时是很常见的问题。桥接模式通过将继承改为组合的方式来解决这个问题。具体来说，就是抽取其中一个维度（形状与颜色）并使之成为独立的类层次(一个维度变为实体类)，这样就可以在初始类中引用这个新层次的对象，从而使得一个类不必拥有所有的状态和行为。https://blog.csdn.net/weixin_43956958&#x2F;article&#x2F;details&#x2F;125335780 核心实现也就是在A类中含有B类接⼝，通过构造函数传递B类的实现，这个B类就是设计的 桥支付平台有多种支付方式 1234567891011121314public abstract class A&#123; private B b; public A(B b) &#123; this.b = b; &#125;&#125;public abstract class Pay&#123; // 抽象部分 多个支付平台 prvate PayModel mode; // 这个PayModel（支付方式）有多种 指纹 密码 人脸，实现部分 // 当创建支付平台的时候就指定支付方式 这个PayModel就是桥 public Pay(PayMode mode) &#123; this.mode = mode; &#125; &#125; 开发场景： JDBC多种驱动程序的实现 同品牌类型的台式机和笔记本平板 业务场景：支付的时候可以选择多个平台 也可以选择多个不同的支付方式多支付和多模式的融合 如果给每一个支付都实现一次不同的模式（为微信和支付宝 都分别根据支付模式实现）显而易见过于麻烦 如果多加了一个平台 有需要为他实现所有对于这种多对多可能会出现笛卡尔积 具体实现不使用桥接模式使用ifelse 判断那个平台 那种方式 123456789101112// 伪代码if (1 === type) &#123; if (1 === modeType) &#123; // 指纹 &#125; else if(2 === modeType) &#123; // 人脸 &#125; else if(2 === modeType) &#123; // 密码 &#125;&#125;else if(2 === type) &#123; // 同上&#125; 非常麻烦 又开始写一坨坨山了 加入桥接模式 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public abstract class Pay &#123; /** * 桥接接口 十分重要 桥接的核心 * 它用来使得 抽象部分 和 实现部分 分离 */ protected IPayMode iPayMode; /** * 在创建支付平台的时候用户自己指定对应的支付方式 * @param mode */ public Pay(IPayMode mode) &#123; this.iPayMode = mode; &#125; /** * 具体的支付方式 * @param uId * @param tradeId * @param amount * @return */ public abstract String transfer(String uId, String tradeId, BigDecimal amount);&#125;public class ZfbPay extends Pay&#123; /** * 在创建支付平台的时候指定对应的支付方式 * * @param mode */ public ZfbPay(IPayMode mode) &#123; super(mode); &#125; @Override public String transfer(String uId, String tradeId, BigDecimal amount) &#123; System.out.println(&quot;支付开始&quot;); System.out.println(&quot;支付校验&quot;); System.out.println(&quot;支付&quot;); System.out.println(&quot;成功&quot;); return &quot;111&quot;; &#125;&#125;public class WxPay extends Pay&#123; /** * 在创建支付平台的时候指定对应的支付方式 * * @param mode */ public WxPay(IPayMode mode) &#123; super(mode); &#125; @Override public String transfer(String uId, String tradeId, BigDecimal amount) &#123; System.out.println(&quot;支付开始&quot;); System.out.println(&quot;支付校验&quot;); System.out.println(&quot;支付&quot;); System.out.println(&quot;成功&quot;); return &quot;000&quot;; &#125;&#125; 123456789101112131415161718192021222324252627282930/** * 支付方式接口 */public interface IPayMode &#123; boolean security(String uId);&#125;public class PayCypher implements IPayMode&#123; @Override public boolean security(String uId) &#123; System.out.println(&quot;密码支付&quot;); return true; &#125;&#125;public class PayFaceMode implements IPayMode&#123; @Override public boolean security(String uId) &#123; System.out.println(&quot;人脸支付&quot;); return true; &#125;&#125;public class PayFingerprintMode implements IPayMode&#123; @Override public boolean security(String uId) &#123; System.out.println(&quot;指纹支付&quot;); return true; &#125;&#125; 1234public static void main(String[] args) &#123; Pay wxPay = new WxPay(new PayFaceMode()); wxPay.transfer(&quot;123&quot;,&quot;321&quot;,new BigDecimal(12)); &#125; 使用场景： 当一个对象有多个变化因素的时候，考虑依赖于抽象的实现，而不是具体的实现。如手机品牌有2种变化因素，一个是品牌，一个是功能。 当多个变化因素在多个对象间共享时，考虑将这部分变化的部分抽象出来再聚合&#x2F;合成进来， 当我们考虑一个对象的多个变化因素可以动态变化的时候，考虑使用桥接模式，如上面例子中的手机品牌是变化的，手机的功能也是变化的，所以将他们分离出来，独立的变化。 优点： 抽象与实现分离，扩展能力强 符合开闭原则 符合合成复用原则 其实现细节对客户透明 策略模式 strategy定义一系列的算法，把他们一个个封装起来，并且使得他们可以互相替换 （说白了 将每一种实现都转变为类 用户根据选择不同类去执行处理）策略模式是解决过多 if-else（或者 switch-case） 代码块的方法之一，提高代码的可维护性、可扩展性和可读性。 策略模式是将每一个算法封装到拥有共同接口的不同类中，使得算法可以在不影响客户端的情况下发生变化。（也可以理解为可供程序运行时选择的（不同的类&#x3D;&#x3D;不同的解决方案&#x3D;&#x3D;不同的算法）） 一个类就是一个解决方式 例子：诸葛亮给赵云三个锦囊 让他遇到危险的时候打开锦囊接口 三种不同实现 代码实现 不用策略模式 123456789public void jinNang() &#123; if (1 == choice) &#123; return &quot;第一个锦囊&quot; &#125;else if (2 == choice) &#123; return &quot;第二个锦囊&quot; &#125;else if (3 == choice) &#123; return &quot;第三个锦囊&quot; &#125;&#125; 使用策略模式 12345678910111213141516171819202122232425262728293031323334353637383940/** * 策略接口 */public interface IStrategy &#123; //每个锦囊妙计都是一个可执行的算法(程序) public void operate();&#125;/** * 具体妙计接口 */public class GivenGreenLight implements IStrategy&#123; @Override public void operate() &#123; System.out.println(&quot;吴国太开发绿灯，放行&quot;); &#125;&#125;/** * 具体妙计实现 也就是不同的算法（业务实现） */public class BlockEnemy implements IStrategy&#123; @Override public void operate() &#123; System.out.println(&quot;孙夫人断后，挡住追兵&quot;); &#125;&#125;/** * 具体妙计 */public class BackDoor implements IStrategy&#123; @Override public void operate() &#123; System.out.println(&quot;乔国老帮忙，吴国太给孙权施压&quot;); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839/** * 策略接口封装 * 用了装妙计的锦囊 */public class Context &#123; //构造函数，你要使用那个妙计 private IStrategy strategy; public Context(IStrategy strategy) &#123; this.strategy = strategy; &#125; //使用计谋 public void operate()&#123; this.strategy.operate(); &#125;&#125;public static void main(String[] args) &#123; Context context; // 根据不同的算法执行 //刚到吴国的时候拆开一个 System.out.println(&quot;-----刚到吴国的时候拆开一个------&quot;); context = new Context(new BackDoor());//拿到妙计 context.operate();//拆开执行 System.out.println(&quot;\\n\\n\\n\\n\\n&quot;); //刘备乐不思蜀，拆开第二个 System.out.println(&quot;-----刘备乐不思蜀，拆开第二个-----&quot;); context = new Context(new GivenGreenLight()); context.operate();//执行妙计 System.out.println(&quot;\\n\\n\\n\\n\\n&quot;); //孙夫人退兵 System.out.println(&quot;-----孙夫人退兵----------------&quot;); context = new Context(new BlockEnemy()); context.operate();//执行妙计 System.out.println(&quot;\\n\\n\\n\\n\\n&quot;); &#125; 减少了ifelse 但是缺点也很明显客户端必须知道所有的策略类, 并自行决定使用哪一个策略类， 这就意味着客户端必须理解这些算法的区别, 以便适时选择恰当的算法类。 换言之, 策略模式只适用于客户端知道算法或行为的情况。由于策略模式把每个具体的策略实现都单独封装成类, 如果备选的策略很多的话, 那么对象的数目就会很多。 策略模式的角色 策略角色 决定实现策略所必需的接口 以上程序中的 IStrategy 具体的策略 实现策略接口的对象 以上程序中 BlockEnemy BackDoor 负责调用具体策略的角色 他保存了具体策略的实例 以上中的Context 定义一系列的算法，把他们一个个封装起来，并且使得他们可以互相替换 （说白了 将每一种实现都转变为类 用户根据选择不同类去执行处理）用来减少ifelse 组合模式 Composite（复合的）能够使容器与内容具有一致性，创造出递归结构例如： 文件夹中可以存放 文件夹和文件 对于文件夹这个容器来说 文件夹和文件是一样的 都是被我装起来的 将对象组合成树型结构以表示 “部分-整体”的层次结构。 组合模式使得用户对单个对象和组合对象的使用具有一致性 组合模式的主要解决的是一系列简单逻辑节点或者扩展的复杂逻辑节点在不同结构的组织下，对于外部的调用是仍然可以非常简单的 https://baijiahao.baidu.com/s?id=1711869421433076203&wfr=spider&for=pc 组合模式通过一种巧妙的设计方案，可以一致性地处理整个树形结构或者树形结构的一部分，也可以一致性地处理树形结构中的叶子节点（不包含子节点的节点）和容器节点（包含子节点的节点）。 什么是组合模式对于树形结构，当容器对象（如文件夹）的某一个方法被调用时，将遍历整个树形结构，寻找也包含这个方法的成员对象（可以是容器对象，也可以是叶子对象）并调用执行。这是靠递归调用的机制实现的。由于容器对象和叶子对象在功能上的区别（文件夹是来装文件的 不同文件的功能不同），在使用这些对象的代码中必须有区别地对待容器对象和叶子对象，而实际上大多数情况下我们希望一致地处理它们，因为对于这些对象的区别对待将会使得程序非常复杂。组合模式为解决此类问题而诞生，它可以让叶子对象和容器对象的使用具有一致性。 组合模式的UML类图 在组合模式结构图中包含如下几个角色： Component（抽象构件）：它可以是接口或抽象类，为叶子构件和容器构件对象声明接口，在该角色中可以包含所有子类共有行为的声明和实现。在抽象构件中定义了访问及管理它的子构件的方法，如增加子构件、删除子构件、获取子构件等。 Leaf（叶子构件）：它在组合结构中表示叶子节点对象，叶子节点没有子节点，它实现了在抽象构件中定义的行为。对于那些访问及管理子构件的方法，可以通过异常等方式进行处理。 Composite（容器构件）：它在组合结构中表示容器节点对象，容器节点包含子节点，其子节点可以是叶子节点，也可以是容器节点，它提供一个集合用于存储子节点，实现了在抽象构件中定义的行为，包括那些访问及管理子构件的方法，在其业务方法中可以递归调用其子节点的业务方法。 组合模式的关键是定义了一个抽象构件类，它既可以代表叶子，又可以代表容器，而客户端针对该抽象构件类进行编程，无须知道它到底表示的是叶子还是容器，可以对其进行统一处理。同时容器对象与抽象构件类之间还建立一个聚合关联关系，在容器对象中既可以包含叶子，也可以包含容器，以此实现递归组合，形成一个树形结构。如果不使用组合模式，客户端代码将过多地依赖于容器对象复杂的内部实现结构，容器对象内部实现结构的变化将引起客户代码的频繁变化，带来了代码维护复杂、可扩展性差等弊端。组合模式的引入将在一定程度上解决这些问题。 业务场景：文件夹下 有不同的文件和文件夹 他其中又有新的文件和文件夹如何用代码表示？ 不使用组合模式 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * 音乐文件 */public class MusicFile &#123; private String name; public MusicFile(String name) &#123; this.name = name; &#125; public void print() &#123; System.out.println(name); &#125;&#125;/** * 视频文件 */public class VideoFile &#123; private String name; public VideoFile(String name) &#123; this.name = name; &#125; public void print() &#123; System.out.println(name); &#125;&#125;/** * img文件 */public class ImageFile &#123; private String name; public ImageFile(String name) &#123; this.name = name; &#125; public void print() &#123; System.out.println(name); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * 文件夹 */public class Folder &#123; private String name; //音乐 private List&lt;MusicFile&gt; musicList = new ArrayList&lt;MusicFile&gt;(); //视频 private List&lt;VideoFile&gt; videoList = new ArrayList&lt;VideoFile&gt;(); //图片 private List&lt;ImageFile&gt; imageList = new ArrayList&lt;ImageFile&gt;(); //文件夹 private List&lt;Folder&gt; folderList = new ArrayList&lt;Folder&gt;(); public Folder(String name) &#123; this.name = name; &#125; public void addFolder(Folder folder) &#123; folderList.add(folder); &#125; public void addImage(ImageFile image) &#123; imageList.add(image); &#125; public void addVideo(VideoFile video) &#123; videoList.add(video); &#125; public void addMusic(MusicFile music) &#123; musicList.add(music); &#125; public void print() &#123; for (MusicFile music : musicList) &#123; music.print(); &#125; for (VideoFile video : videoList) &#123; video.print(); &#125; for (ImageFile image : imageList) &#123; image.print(); &#125; for (Folder folder : folderList) &#123; folder.print(); &#125; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637public static void main(String[] args) &#123; /** * 创建文件 并且把他们加入到各自的文件夹中 */ MusicFile m1 = new MusicFile(&quot;尽头.mp3&quot;); MusicFile m2 = new MusicFile(&quot;飘洋过海来看你.mp3&quot;); MusicFile m3 = new MusicFile(&quot;曾经的你.mp3&quot;); MusicFile m4 = new MusicFile(&quot;take me to your heart.mp3&quot;); VideoFile v1 = new VideoFile(&quot;战狼2.mp4&quot;); VideoFile v2 = new VideoFile(&quot;理想.avi&quot;); VideoFile v3 = new VideoFile(&quot;琅琊榜.avi&quot;); ImageFile i1 = new ImageFile(&quot;敦煌.png&quot;); ImageFile i2 = new ImageFile(&quot;baby.jpg&quot;); ImageFile i3 = new ImageFile(&quot;girl.jpg&quot;); Folder aa = new Folder(&quot;aa&quot;); aa.addImage(i3); Folder bb = new Folder(&quot;bb&quot;); bb.addMusic(m4); bb.addVideo(v3); Folder top = new Folder(&quot;top&quot;); top.addFolder(aa); top.addFolder(bb); top.addMusic(m1); top.addMusic(m2); top.addMusic(m3); top.addVideo(v1); top.addVideo(v2); top.addImage(i1); top.addImage(i2); top.print(); &#125; 使用以上方式的缺点： 文件夹的实现方式十分麻烦 代码很冗余 文件夹必须对不同类型文件作出处理 由于没有抽象层 客户端必须对每种类型做出实现 无法统一处理 可扩展性差 使用组合模式 1234567891011121314151617public abstract class AbstractFile &#123; public void add(AbstractFile file)&#123; throw new UnsupportedOperationException(); &#125; public void remove(AbstractFile file)&#123; throw new UnsupportedOperationException(); &#125; public AbstractFile getChild(int i)&#123; throw new UnsupportedOperationException(); &#125; public void print()&#123; throw new UnsupportedOperationException(); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071/** * 文件夹 * 重大变化 使用组合模式 使得文件夹对于所有的文件（文件夹 文件） 一致 */public class Folder extends AbstractFile &#123; private String name; // 所有文件都是这个 AbstractFile的子类 private List&lt;AbstractFile&gt; files = new ArrayList&lt;AbstractFile&gt;(); public Folder(String name) &#123; this.name = name; &#125; @Override public void add(AbstractFile file) &#123; files.add(file); &#125; @Override public void remove(AbstractFile file) &#123; files.remove(file); &#125; @Override public AbstractFile getChild(int i) &#123; return files.get(i); &#125; @Override public void print() &#123; for (AbstractFile file : files) &#123; file.print(); &#125; &#125;&#125;public class ImageFile extends AbstractFile &#123; private String name; public ImageFile(String name) &#123; this.name = name; &#125; public void print() &#123; System.out.println(name); &#125;&#125;public class MusicFile extends AbstractFile &#123; private String name; public MusicFile(String name) &#123; this.name = name; &#125; public void print() &#123; System.out.println(name); &#125;&#125;public class VideoFile extends AbstractFile &#123; private String name; public VideoFile(String name) &#123; this.name = name; &#125; public void print() &#123; System.out.println(name); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334public static void main(String[] args) &#123; AbstractFile m1 = new MusicFile(&quot;尽头.mp3&quot;); AbstractFile m2 = new MusicFile(&quot;飘洋过海来看你.mp3&quot;); AbstractFile m3 = new MusicFile(&quot;曾经的你.mp3&quot;); AbstractFile m4 = new MusicFile(&quot;take me to your heart.mp3&quot;); AbstractFile v1 = new VideoFile(&quot;战狼2.mp4&quot;); AbstractFile v2 = new VideoFile(&quot;理想.avi&quot;); AbstractFile v3 = new VideoFile(&quot;琅琊榜.avi&quot;); AbstractFile i1 = new ImageFile(&quot;敦煌.png&quot;); AbstractFile i2 = new ImageFile(&quot;baby.jpg&quot;); AbstractFile i3 = new ImageFile(&quot;girl.jpg&quot;); AbstractFile aa = new Folder(&quot;aa&quot;); aa.add(i3); AbstractFile bb = new Folder(&quot;bb&quot;); bb.add(m4); bb.add(v3); AbstractFile top = new Folder(&quot;top&quot;); top.add(aa); top.add(bb); top.add(m1); top.add(m2); top.add(m3); top.add(v1); top.add(v2); top.add(i1); top.add(i2); top.print(); &#125; 对于以上代码而言 文件夹类 的实现代码减少 并且更加简洁 对于文件夹类而言 文件夹和文件 都是一致的 这就实现了 对于容器和具体 统一处理 在以下情况下可以考虑使用组合模式： 在具有整体和部分的层次结构中，希望通过一种方式忽略整体与部分的差异，客户端可以一致地对待它们。 在一个使用面向对象语言开发的系统中需要处理一个树形结构。 在一个系统中能够分离出叶子对象和容器对象，而且它们的类型不固定，需要增加一些新的类型。 装饰器模式 Decorater动态的给一个对象添加一些额外的职责，就增加功能来说，装饰器比生成子类更灵活 有一个蛋糕 只给她加奶油 就成为了奶油蛋糕 给他加草莓就是草莓蛋糕 等等无论是奶油蛋糕 还是 草莓蛋糕 本质都是蛋糕没有变 只是加强了原本蛋糕 代码中就是 有一个蛋糕类 给他加了新功能 奶油 使得在原有的基础上更加强大 装饰器核心： 不改变原有类的基础上新增功能 业务场景：一个男的 普通男人 因为有 勇气 智慧 成长为优秀男性 代码实现 12345678910111213141516171819// 男人public interface Man &#123; public void getManDesc();&#125;// 普通男人public class NormalMan implements Man&#123; private String name = null; public NormalMan(String name) &#123; this.name = name; &#125; @Override public void getManDesc() &#123; System.out.print(name + &quot;: &quot;); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495/** * 十分关键 装饰者模式的核心 * 装饰抽象类 继承接口 * 具体的装饰类 通过实现方法 进行功能增强 * 这个装饰器的增强方式调用原本的方法 使得原来的功能不改变 */public abstract class AttachedPropertiesDecorator implements Man&#123; private Man man; public AttachedPropertiesDecorator(Man man) &#123; this.man = man; &#125; public void getManDesc() &#123; man.getManDesc(); &#125;&#125;// 小车装饰者public class CarDecoratorImpl extends AttachedPropertiesDecorator&#123; private String car = &quot;有车&quot;; // new 这个装饰类对象的时候 需要传入具体man 为谁装饰 增强功能 public CarDecoratorImpl(Man man) &#123; super(man); &#125; /** * 功能增强方法 */ public void addCar() &#123; System.out.print(car + &quot; &quot;); &#125; @Override public void getManDesc() &#123; super.getManDesc(); // 原本的方法不变 还可以用 addCar(); // 功能增强 &#125;&#125;// 房子装饰者public class HouseDecoratorImpl extends AttachedPropertiesDecorator&#123; private String house = &quot;有房&quot;; public HouseDecoratorImpl(Man man) &#123; super(man); &#125; public void addHouse() &#123; System.out.print(house + &quot; &quot;); &#125; @Override public void getManDesc() &#123; super.getManDesc(); addHouse(); &#125;&#125;// 存款装饰者public class DepositDecoratorImpl extends AttachedPropertiesDecorator&#123; private String deposit = &quot;有存款&quot;; public DepositDecoratorImpl(Man man) &#123; super(man); &#125; public void addDeposit() &#123; System.out.print(deposit + &quot; &quot;); &#125; @Override public void getManDesc() &#123; super.getManDesc(); addDeposit(); &#125;&#125;// 品质装饰者public class QualityDecoratorImpl extends AttachedPropertiesDecorator&#123; private String quality = &quot;有好品质&quot;; public QualityDecoratorImpl(Man man) &#123; super(man); &#125; public void addQuality() &#123; System.out.print(quality + &quot; &quot;); &#125; @Override public void getManDesc() &#123; super.getManDesc(); &#125;&#125; 12345678910111213141516171819202122232425public static void main(String[] args) &#123; Man man = new NormalMan(&quot;张三&quot;); Man man1 = new CarDecoratorImpl(man); Man man2 = new HouseDecoratorImpl(man1); Man man3 = new DepositDecoratorImpl(man2); System.out.println(&quot;层层装饰:&quot;); man3.getManDesc(); System.out.println(); System.out.println(&quot;重复装饰（有两个&#x27;有存款&#x27;）:&quot;); Man man4 = new DepositDecoratorImpl(man3); man4.getManDesc(); System.out.println(); System.out.println(&quot;任意修饰:&quot;); Man man5 = new QualityDecoratorImpl(man1); man5.getManDesc(); System.out.println(); System.out.println(&quot;直接得到修饰结果:&quot;); Man man6 = new HouseDecoratorImpl(new DepositDecoratorImpl(new NormalMan(&quot;李四&quot;))); man6.getManDesc(); System.out.println(); &#125; 通过 new A(new B(new C)); 的方式使得功能达到多个增强 在java中 io流也用了这种方式 模式中的角色： 增强功能时的核心角色 为这个角色来增加功能 例如例子中的蛋糕 和以上的Man接口 实现了核心角色的具体 例如以上的NormalMan 装饰物 继承核心角色接口 并将核心角色包含 例如以上的AttachedPropertiesDecorator 他的作用是让让原本功能可以调用 并且加上新的功能 使得用户可以调用原本的也可以调用增强功能 具体的装饰类 例如以上的QualityDecoratorImpl CarDecoratorImpl等等 访问者模式 visitor主要将数据结构与数据操作分离如何理解这句话？访问者要解决的核心事项是，在一个稳定的数据结构下，例如用户信息、雇员信息等 增加易变的业务逻辑，为了了增强扩展性，将这两部分的业务解耦的一种设计模式说白了就是 同一个事物不同视角下访问信息不同 比如一个美女手里拿着冰激凌 小朋友会注意冰激凌 大朋友可就不好说了同一份数据 不同人需要的不一样 案例场景模拟在本案例中我们模拟校园中的学生和老师对于不同用户的访问视角例如 家长看重老师的能力和学生的成绩 校长看重学生的人数和升学 老师和学生是一个固定信息的内容 可是想要不同视角的用户获取他们想要的信息，就需要使用访问者模式 使得实体和业务解耦 代码实现定义固定信息内容 老师和学生类 12345678910111213141516171819202122232425262728293031323334353637383940414243// 基础⽤用户信息public abstract class User &#123; public String name; // 姓名 public String identity; // 身份；重点班、普通班 | 特级教师、普通教师、实习教师 public String clazz; // 班级 public User(String name, String identity, String clazz) &#123; this.name = name; this.identity = identity; this.clazz = clazz; &#125; // 核心访问⽅方法 public abstract void accept(Visitor visitor); // 最终访问的是 visitor的visit方法&#125;public class Student extends User &#123; public Student(String name, String identity, String clazz) &#123; super(name, identity, clazz); &#125; public void accept(Visitor visitor) &#123; visitor.visit(this); // 这个this 很精妙 传入的谁 就执行谁的访问 &#125; public int ranking() &#123; return (int) (Math.random() * 100); &#125;&#125;public class Teacher extends User &#123; public Teacher(String name, String identity, String clazz) &#123; super(name, identity, clazz); &#125; public void accept(Visitor visitor) &#123; visitor.visit(this); // 这个this 很精妙 传入的谁 就执行谁的访问 &#125; // 升本率 public double entranceRatio() &#123; return BigDecimal.valueOf(Math.random() * 100).setScale(2, BigDecimal.ROUND_HALF_UP).doubleValue(); &#125;&#125; 123456789101112131415161718192021222324252627282930313233public interface Visitor &#123; // 访问学⽣生信息 void visit(Student student); // 访问⽼老老师信息 void visit(Teacher teacher);&#125;public class Parent implements Visitor &#123; private Logger logger = LoggerFactory.getLogger(Parent.class); public void visit(Student student) &#123; logger.info(&quot;学⽣生信息 姓名：&#123;&#125; 班级：&#123;&#125; 排名：&#123;&#125;&quot;, student.name, student.clazz, student.ranking()); &#125; public void visit(Teacher teacher) &#123; logger.info(&quot;⽼老老师信息 姓名：&#123;&#125; 班级：&#123;&#125; 级别：&#123;&#125;&quot;, teacher.name, teacher.clazz, teacher.identity); &#125;&#125;public class Principal implements Visitor &#123; private Logger logger = LoggerFactory.getLogger(Principal.class); public void visit(Student student) &#123; logger.info(&quot;学⽣生信息 姓名：&#123;&#125; 班级：&#123;&#125;&quot;, student.name, student.clazz); &#125; public void visit(Teacher teacher) &#123; logger.info(&quot;老师信息 姓名：&#123;&#125; 班级：&#123;&#125; 升学率：&#123;&#125;&quot;, teacher.name, teacher.clazz, teacher.entranceRatio()); &#125;&#125; 123456789101112131415161718192021222324252627public class DataView &#123; List&lt;User&gt; userList = new ArrayList&lt;User&gt;(); public DataView() &#123; userList.add(new Student(&quot;谢⻜飞机&quot;, &quot;重点班&quot;, &quot;⼀一年年⼀一班&quot;)); userList.add(new Student(&quot;windy&quot;, &quot;重点班&quot;, &quot;⼀一年年⼀一班&quot;)); userList.add(new Student(&quot;⼤大⽑毛&quot;, &quot;普通班&quot;, &quot;⼆二年年三班&quot;)); userList.add(new Student(&quot;Shing&quot;, &quot;普通班&quot;, &quot;三年年四班&quot;)); userList.add(new Teacher(&quot;BK&quot;, &quot;特级教师&quot;, &quot;⼀一年年⼀一班&quot;)); userList.add(new Teacher(&quot;娜娜Goddess&quot;, &quot;特级教师&quot;, &quot;⼀一年年⼀一班&quot;)); userList.add(new Teacher(&quot;dangdang&quot;, &quot;普通教师&quot;, &quot;⼆二年年三班&quot;)); userList.add(new Teacher(&quot;泽东&quot;, &quot;实习教师&quot;, &quot;三年年四班&quot;)); &#125; // 展示 public void show(Visitor visitor) &#123; for (User user : userList) &#123; user.accept(visitor); &#125; &#125;&#125;public static void main(String[] args) &#123; DataView dataView = new DataView(); dataView.show(new Parent()); // 家⻓访问视角 dataView.show(new Principal()); // 校长访问视角 &#125; 角色： Visitor 抽象访问者角色，为该对象结构中具体元素角色声明一个访问操作接口。该操作接口的名字和参数标识了发送访问请求给具体访问者的具体元素角色，这样访问者就可以通过该元素角色的特定接口直接访问它。 ConcreteVisitor.具体访问者角色，实现Visitor声明的接口。 Element 定义一个接受访问操作(accept())，它以一个访问者(Visitor)作为参数。 ConcreteElement 具体元素，实现了抽象元素(Element)所定义的接受操作接口。 ObjectStructure 结构对象角色，这是使用访问者模式必备的角色。它具备以下特性：能枚举它的元素；可以提供一个高层接口以允许访问者访问它的元素；如有需要，可以设计成一个复合对象或者一个聚集（如一个列表或无序集合）。 责任链模式 chain of responsibility避免请求发送者与接收者耦合在一起，让多个对象都有可能接收到请求，将这些对象连接成一条链，沿着这条链传递请求，直到有对象处理他为止 将多个对象组成一条责任链，然后按照他们在责任链上的顺序一个一个地找出到底改由谁负责处理这种情况是有的 请求进入 暂时不知道应该由谁来处理 这种时候就需要推卸责任了。 解决方式就是以上的责任链例如：请假 你需要上报班长 班长报给老师 老师可能报给校长 校长在审批 主要实现 就是抽象出一个链路 将所有的实现放在链路中 业务场景模拟618的业务上线 123456789101112public class AuthInfo &#123; private String code; private String info = &quot;&quot;; public AuthInfo(String code, String... infos) &#123; this.code = code; for (String str : infos) &#123; this.info = this.info.concat(str); &#125; &#125;// ...get/set&#125; 1234567891011121314151617181920212223242526272829303132333435363738public abstract class AuthLink &#123; protected SimpleDateFormat f = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);// 时间格式化 protected String levelUserId; // 级别⼈人员ID protected String levelUserName; // 级别⼈人员姓名 private AuthLink next; // 责任链 public AuthLink(String levelUserId, String levelUserName) &#123; this.levelUserId = levelUserId; this.levelUserName = levelUserName; &#125; /** * 链路可以返回下一个链 * @return */ public AuthLink next() &#123; return next; &#125; /** * 加入到链路中 * @param next * @return */ public AuthLink appendNext(AuthLink next) &#123; this.next = next; return this; &#125; /** * 执行的方法 * @param uId * @param orderId * @param authDate * @return */ public abstract AuthInfo doAuth(String uId, String orderId, Date authDate);&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class Level1AuthLink extends AuthLink &#123; public Level1AuthLink(String levelUserId, String levelUserName) &#123; super(levelUserId, levelUserName); &#125; public AuthInfo doAuth(String uId, String orderId, Date authDate) &#123; Date date = new Date(); if (null == date) &#123; return new AuthInfo(&quot;0001&quot;, &quot;单号：&quot;, orderId, &quot; 状态：待⼀一级审批负责人&quot;, levelUserName); &#125; AuthLink next = super.next(); if (null == next) &#123; return new AuthInfo(&quot;0000&quot;, &quot;单号：&quot;, orderId, &quot; 状态：⼀一级审批完 成负责⼈人&quot;, &quot; 时间：&quot;, f.format(date), &quot; 审批⼈人：&quot;, levelUserName); &#125; return next.doAuth(uId, orderId, authDate); &#125;&#125;public class Level2AuthLink extends AuthLink &#123; public Level2AuthLink(String levelUserId, String levelUserName) &#123; super(levelUserId, levelUserName); &#125; public AuthInfo doAuth(String uId, String orderId, Date authDate) &#123; Date date = new Date(); if (null == date) &#123; return new AuthInfo(&quot;0002&quot;, &quot;单号：&quot;, orderId, &quot; 状态：待2级审批负责人&quot;, levelUserName); &#125; AuthLink next = super.next(); if (null == next) &#123; return new AuthInfo(&quot;0000&quot;, &quot;单号：&quot;, orderId, &quot; 状态：2级审批完 成负责⼈人&quot;, &quot; 时间：&quot;, f.format(date), &quot; 审批⼈人：&quot;, levelUserName); &#125; return next.doAuth(uId, orderId, authDate); &#125;&#125;public class Level3AuthLink extends AuthLink &#123; public Level3AuthLink(String levelUserId, String levelUserName) &#123; super(levelUserId, levelUserName); &#125; public AuthInfo doAuth(String uId, String orderId, Date authDate) &#123; Date date = new Date(); if (null == date) &#123; return new AuthInfo(&quot;0003&quot;, &quot;单号：&quot;, orderId, &quot; 状态：待3级审批负责人&quot;, levelUserName); &#125; AuthLink next = super.next(); if (null == next) &#123; return new AuthInfo(&quot;0000&quot;, &quot;单号：&quot;, orderId, &quot; 状态：3级审批完 成负责⼈人&quot;, &quot; 时间：&quot;, f.format(date), &quot; 审批⼈人：&quot;, levelUserName); &#125; return next.doAuth(uId, orderId, authDate); &#125;&#125; 12345678910111213141516171819202122public static void main(String[] args) &#123; AuthLink authLink = new Level3AuthLink(&quot;1000013&quot;, &quot;王⼯工&quot;) .appendNext(new Level2AuthLink(&quot;1000012&quot;, &quot;张经理理&quot;) .appendNext(new Level1AuthLink(&quot;1000011&quot;, &quot;段总&quot;))); logger.info(&quot;测试结果：&#123;&#125;&quot;, JSON.toJSONString(authLink.doAuth(&quot;⼩小傅哥&quot;, &quot;1000998004813441&quot;, new Date()))); // 模拟三级负责⼈人审批 AuthService.auth(&quot;1000013&quot;, &quot;1000998004813441&quot;); logger.info(&quot;测试结果：&#123;&#125;&quot;, &quot;模拟三级负责⼈人审批，王⼯工&quot;); logger.info(&quot;测试结果：&#123;&#125;&quot;, JSON.toJSONString(authLink.doAuth(&quot;⼩小傅哥&quot;, &quot;1000998004813441&quot;, new Date()))); // 模拟⼆二级负责⼈人审批 AuthService.auth(&quot;1000012&quot;, &quot;1000998004813441&quot;); logger.info(&quot;测试结果：&#123;&#125;&quot;, &quot;模拟⼆二级负责⼈人审批，张经理理&quot;); logger.info(&quot;测试结果：&#123;&#125;&quot;, JSON.toJSONString(authLink.doAuth(&quot;⼩小傅哥&quot;, &quot;1000998004813441&quot;, new Date()))); // 模拟⼀一级负责⼈人审批 AuthService.auth(&quot;1000011&quot;, &quot;1000998004813441&quot;); logger.info(&quot;测试结果：&#123;&#125;&quot;, &quot;模拟⼀一级负责⼈人审批，段总&quot;); logger.info(&quot;测试结果：&#123;&#125;&quot;, JSON.toJSONString(authLink.doAuth(&quot;⼩小傅哥&quot;, &quot;1000998004813441&quot;, new Date()))); &#125; 通过把不同的责任节点进行组装 构成一条完整业务的责任链 外观模式 facade为子系统的一组接口提供一个一致的界面，外观模式定义了一个高层接口 这个接口使得这一子系统更加容易使用 外观模式也叫门面模式 是一种通过为多个复杂的子系统提供一个一致的接口，而使这些子系统更加容易被访问的模式。该模式对外有一个统一接口，外部应用程序不用关心内部子系统的具体细节，这样会大大降低应用程序的复杂度，提高了程序的可维护性。 引入外观角色之后，用户只需要直接与外观角色交互，用户与子系统之间的复杂关系由外观角色来实现，从而降低了系统的耦合度。 例子：电源总开关 可以控制所有电器 代码实现 12345678910111213141516171819202122232425262728293031323334353637383940/** * 外观模式的核心 * 为子系统的接口 提供一个一致的界面 用户只需要面对这个门面就可以了 */public class GeneralSwitchFacade &#123; private Light lights[] = new Light[4]; private Fan fan; private AirConditioner ac; private Television tv; public GeneralSwitchFacade() &#123; lights[0] = new Light(&quot;左前&quot;); lights[1] = new Light(&quot;右前&quot;); lights[2] = new Light(&quot;左后&quot;); lights[3] = new Light(&quot;右后&quot;); fan = new Fan(); ac = new AirConditioner(); tv = new Television(); &#125; public void on() &#123; lights[0].on(); lights[1].on(); lights[2].on(); lights[3].on(); fan.on(); ac.on(); tv.on(); &#125; public void off() &#123; lights[0].off(); lights[1].off(); lights[2].off(); lights[3].off(); fan.off(); ac.off(); tv.off(); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class Light &#123; private String position; public Light(String position) &#123; this.position = position; &#125; public void on() &#123; System.out.println(this.position + &quot;灯打开！&quot;); &#125; public void off() &#123; System.out.println(this.position + &quot;灯关闭！&quot;); &#125;&#125;//子系统角色public class Television &#123; public void on() &#123; System.out.println(&quot;电视机打开！&quot;); &#125; public void off() &#123; System.out.println(&quot;电视机关闭！&quot;); &#125;&#125;//子系统角色public class Fan &#123; public void on() &#123; System.out.println(&quot;风扇打开！&quot;); &#125; public void off() &#123; System.out.println(&quot;风扇关闭！&quot;); &#125;&#125;//子系统角色public class AirConditioner &#123; public void on() &#123; System.out.println(&quot;空调打开！&quot;); &#125; public void off() &#123; System.out.println(&quot;空调关闭！&quot;); &#125;&#125; 1234567891011//客户端 public class Client &#123; public static void main(String args[]) &#123; GeneralSwitchFacade gsf=new GeneralSwitchFacade(); gsf.on(); System.out.println(&quot;-----------------------&quot;); gsf.off(); &#125; &#125; 外观（Facade）模式包含以下主要角色。 外观（Facade）角色：为多个子系统对外提供一个共同的接口。 子系统（Sub System）角色：实现系统的部分功能，客户可以通过外观角色访问它。 客户（Client）角色：通过一个外观角色访问各个子系统的功能。 优点： 1、减少系统相互依赖。 2、提高灵活性。 3、提高了安全性。缺点： 不符合开闭原则，如果要改东西很麻烦，继承重写都不合适。使用场景： 1、为复杂的模块或子系统提供外界访问的模块。 2、子系统相对独立。 3、预防低水平人员带来的风险。 中介者模式 mediator用一个中介对象来封装一系列的对象交互，中介者使各对象不需要显式的相互引用，从而使其耦合松散，而且可以独立的改变他们之间的交互例如：房屋中介 屋主和找房的 只需要通过中介就可以 而不是自己去寻找合适对象 中介者模式要解决的就是复杂功能应用之间的重复调用，在这中间添加一层中介者包装服务，对外提供 简单、通用、易扩展的服务能力 中介者模式通过中介者对象来封装一系列的对象交互，将对象间复杂的关系网状结构变成结构简单的以中介者为核心的星形结构，对象间一对多的关联转变为一对一的关联，简化对象间的关系，便于理解；各个对象之间的关系被解耦，每个对象不再和它关联的对象直接发生相互作用，而是通过中介者对象来与关联的对象进行通讯，使得对象可以相对独立地使用，提高了对象的可复用和系统的可扩展性。 在中介者模式中，中介者类处于核心地位，它封装了系统中所有对象类之间的关系，除了简化对象间的关系，还可以对对象间的交互进行进一步的控制。https://blog.csdn.net/a745233700/article/details/120293022 业务：找房子 出租房子 如何没有中介 出租房子需要一个个询问找房子的人 找房子的需要一个个寻找出租房子的加入设计模式 他们只需要面对中介 代码实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public abstract class Person &#123; public Mediator mediator; public String name; public Person(String name, Mediator mediator) &#123; this.mediator = mediator; this.name = name; &#125;&#125;public class Tenant extends Person&#123; Tenant(String name, Mediator mediator) &#123; super(name, mediator); &#125; /** * @desc 与中介者联系 * @param message * @return void */ public void constact(String message)&#123; mediator.constact(message, this); &#125; /** * @desc 获取信息 * @param message * @return void */ public void getMessage(String message)&#123; System.out.println(&quot;租房者:&quot; + name +&quot;,获得信息：&quot; + message); &#125;&#125;public class HouseOwner extends Person&#123; HouseOwner(String name, Mediator mediator) &#123; super(name, mediator); &#125; /** * @desc 与中介者联系 * @param message * @return void */ public void constact(String message)&#123; mediator.constact(message, this); &#125; /** * @desc 获取信息 * @param message * @return void */ public void getMessage(String message)&#123; System.out.println(&quot;房主:&quot; + name +&quot;,获得信息：&quot; + message); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * 抽象处中介的接口 */public abstract class Mediator &#123; //申明一个联络方法 public abstract void constact(String message,Person person);&#125;/** * 具体中介 他使得两个对象进行交互 */public class MediatorStructure extends Mediator&#123; //首先中介结构必须知道所有房主和租房者的信息 private HouseOwner houseOwner; private Tenant tenant; public HouseOwner getHouseOwner() &#123; return houseOwner; &#125; public void setHouseOwner(HouseOwner houseOwner) &#123; this.houseOwner = houseOwner; &#125; public Tenant getTenant() &#123; return tenant; &#125; public void setTenant(Tenant tenant) &#123; this.tenant = tenant; &#125; /** * 主要就在这里 通过中介使得他们交互 * @param message * @param person */ public void constact(String message, Person person) &#123; if(person == houseOwner)&#123; //如果是房主，则租房者获得信息 tenant.getMessage(message); &#125; else&#123; //反之则是获得房主信息 houseOwner.getMessage(message); &#125; &#125;&#125; 123456789101112131415public static void main(String[] args) &#123; //一个房主、一个租房者、一个中介机构 MediatorStructure mediator = new MediatorStructure(); //房主和租房者只需要知道中介机构即可 HouseOwner houseOwner = new HouseOwner(&quot;张三&quot;, mediator); Tenant tenant = new Tenant(&quot;李四&quot;, mediator); //中介结构要知道房主和租房者 mediator.setHouseOwner(houseOwner); mediator.setTenant(tenant); tenant.constact(&quot;听说你那里有三室的房主出租.....&quot;); houseOwner.constact(&quot;是的!请问你需要租吗?&quot;);&#125; 角色：Mediator：抽象中介者，定义了同事对象到中介者对象之间的接口ConcreteMediator：具体中介者，实现抽象中介者的方法，它需要知道所有的具体同事类，同时需要从具体的同事类那里接收信息，并且向其他具体的同事类发送信息Colleague：抽象同事类ConcreteColleague：具体同事类，每个具体同事类都只需要知道自己的行为即可，但是他们都需要认识中介者。 中介者对象封装了对象之间的关联关系，导致中介者对象变得比较庞大复杂，所承担的责任也比较多，维护起来也比较困难，它需要知道每个对象和他们之间的交互细节，如果它出问题，将会导致整个系统都会出问题。 mybatis也使用了这种模式 他就充当了中介 用户可能使用mysql Oracle等等数据库 配置不同 ORM数据库面对mybaits 用户也面对mybatis 观察者模式 Observer定义对象间的一种一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更更新。观察者模式又称为 发布-订阅模式，定义了对象之间一对多依赖关系，当目标对象(被观察者)的状态发生改变时，它的所有依赖者(观察者)都会收到通知。 业务：在气象观测站中，它能够追踪目前的天气状况，包括温度、适度、气压。需要实现一个布告板，能够分别显示目前的状态，气象统计和简单的预报。当气象站中获取最新的测量数据时，三种布告板必须实时更新 代码实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071/** * 被观察者 */public interface Subject &#123; /** * 注册观察者 * @param observer */ public void registerObserver(Observer observer); /** * 删除观察者 * @param observer */ public void removeOberver(Observer observer); /** * 当主题状态发生改变时，这个方法需要被调用，以通知所有观察者 */ public void notifyObserver();&#125;/** * 具体的被观察者 只要他变化 观察者就会知道 */public class WeatherData implements Subject &#123; private List&lt;Observer&gt; observers; private float tempterature; private float pressure; private float humidity; public WeatherData()&#123; observers = new ArrayList&lt;Observer&gt;(); &#125; @Override public void notifyObserver() &#123; // 当他改变的时候 观察者会更新 for(int i = 0; i &lt; observers.size();i++)&#123; Observer observer = observers.get(i); observer.update(tempterature, humidity, pressure); &#125; &#125; @Override public void registerObserver(Observer observer) &#123; observers.add(observer); &#125; @Override public void removeOberver(Observer observer) &#123; int i = observers.indexOf(observer); if(i &gt;= 0)&#123; observers.remove(i); &#125; &#125; /** * 气象站得到更新的观测数据时，通知观察者 */ public void measurementChanged()&#123; notifyObserver(); &#125; public void setMeasurements(float temperature,float humidity,float pressure)&#123; this.tempterature = temperature; this.humidity = humidity; this.pressure = pressure; measurementChanged(); &#125;&#125; 123456789101112131415161718192021222324252627282930public interface Observer &#123; public void update(float temp,float humidity,float pressure);&#125;public interface DisplayElement &#123; public void display();&#125;public class CurrentConditionsDisplay implements Observer,DisplayElement&#123; private float temperature; private float humidity; private Subject weatherData; public CurrentConditionsDisplay(Subject weatherData)&#123; this.weatherData = weatherData; weatherData.registerObserver(this); //注册观察者 &#125; public void update(float temp, float humidity, float pressure) &#123; this.temperature = temp; this.humidity = humidity; display(); &#125; @Override public void display() &#123; System.out.println(&quot;Current conditions:&quot;+temperature+&quot;F degrees and &quot;+humidity+&quot;% humidity&quot;); &#125; &#125; 123456789public static void main(String[] args) &#123; WeatherData weatherData = new WeatherData(); CurrentConditionsDisplay conditionsDisplay = new CurrentConditionsDisplay(weatherData); weatherData.setMeasurements(80, 65, 30.4f); weatherData.setMeasurements(82, 70, 29.2f); weatherData.setMeasurements(78, 78, 40.4f); &#125; 角色：Subject：抽象主题（被观察者），每一个主题可以有多个观察者，并将所有观察者对象的引用保存在一个集合里，被观察者提供一个接口，可以增加和删除观察者角色ConcreteSubject：具体主题，将有关状态存入具体观察者对象，在主题发生改变时，给所有的观察者发出通知Observer：抽象观察者，为所有的具体观察者定义一个更新接口，该接口的作用是在收到主题的通知时能够及时的更新自己ConcreteObserver：具体观察者，实现抽象观察者角色定义的更新接口，以便使本身的状态与主题状态相协调。如果需要，具体观察者角色可以保存一个指向具体主题角色的引用。 备忘录模式 memento在不破坏封装性的情况下，捕获一个对象的内部状态，并在该对象之外保存这个状态（说白了 保存一份时间点 需要的时候回滚到这个时间点）备忘录模式提供了一种恢复状态的机制，在不破坏封装的前提下，捕获对象的内部状态，并保存在该对象之外，保证该对象能够恢复到某个历史状态；备忘录模式将保存的细节封装在备忘录中，除了创建它的创建者之外其他对象都不能访问它，并且实现了即使要改变保存的细节也不影响客户端。但是备忘录模式都是多状态和多备份的，会早用较多的内存，消耗资源。 业务场景：我们以游戏挑战BOSS为实现场景：在挑战BOSS之前，角色的血量、蓝量都是满值，然后存档，在大战BOSS时，由于操作失误导致血量和蓝量大量损耗，所以只好恢复到刚刚开始的存档点，继续进行大战BOSS了。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * 游戏角色类 */public class Role&#123; /** * 血条 */ private int bloodFlow; /** * 蓝条 */ private int magicPoint; public Role(int bloodFlow,int magicPoint)&#123; this.bloodFlow = bloodFlow; this.magicPoint = magicPoint; &#125; public int getBloodFlow() &#123; return bloodFlow; &#125; public void setBloodFlow(int bloodFlow) &#123; this.bloodFlow = bloodFlow; &#125; public int getMagicPoint() &#123; return magicPoint; &#125; public void setMagicPoint(int magicPoint) &#123; this.magicPoint = magicPoint; &#125; /** * @desc 展示角色当前状态 */ public void display()&#123; System.out.println(&quot;用户当前状态:&quot;); System.out.println(&quot;血量:&quot; + getBloodFlow() + &quot;;蓝量:&quot; + getMagicPoint()); &#125; /** * @desc 保持存档、当前状态 * 将当前状态 保存在备忘录中 */ public Memento saveMemento()&#123; return new Memento(getBloodFlow(), getMagicPoint()); &#125; /** * @desc 恢复存档 获取备忘录中的状态写入 */ public void restoreMemento(Memento memento)&#123; this.bloodFlow = memento.getBloodFlow(); this.magicPoint = memento.getMagicPoint(); &#125;&#125; 12345678910111213141516171819202122232425262728/** * 备忘录类 它记录了 用户保存的 时间点 */class Memento &#123; private int bloodFlow; private int magicPoint; public int getBloodFlow() &#123; return bloodFlow; &#125; public void setBloodFlow(int bloodFlow) &#123; this.bloodFlow = bloodFlow; &#125; public int getMagicPoint() &#123; return magicPoint; &#125; public void setMagicPoint(int magicPoint) &#123; this.magicPoint = magicPoint; &#125; public Memento(int bloodFlow,int magicPoint)&#123; this.bloodFlow = bloodFlow; this.magicPoint = magicPoint; &#125;&#125; 123456789101112131415161718/** * 管理员 他负责将数据创建备忘录 */public class Caretaker &#123; Memento memento; public Memento getMemento() &#123; return memento; &#125; /** * 设置备忘录 * @param memento */ public void setMemento(Memento memento) &#123; this.memento = memento; &#125;&#125; 1234567891011121314151617181920212223public static void main(String[] args) &#123; //打BOSS之前：血、蓝全部满值 Role role = new Role(100, 100); System.out.println(&quot;----------大战BOSS之前----------&quot;); role.display(); //保持进度 Caretaker caretaker = new Caretaker(); // 将目前时间点 保存到备忘录中 caretaker.memento = role.saveMemento(); //大战BOSS，快come Over了 role.setBloodFlow(20); role.setMagicPoint(20); System.out.println(&quot;----------大战BOSS----------&quot;); role.display(); //恢复存档 role.restoreMemento(caretaker.getMemento()); System.out.println(&quot;----------恢复----------&quot;); role.display(); &#125; 角色： Originator：原发器，负责创建一个备忘录，用于记录当前对象的内部状态，也可以使用它来利用备忘录恢复内部状态，同时原发器还可以根据需要决定 Memento 存储 Originator 的哪些内部状态。 Memento：备忘录，用于存储 Originator 的内部状态，并且可以防止 Originator 以外的对象访问Memento。在备忘录 Memento 中有两个接口，其中 Caretaker 只能看到备忘录中的窄接口，它只能将备忘录传递给其他对象。Originator可以看到宽接口，允许它访问返回到先前状态的所有数据。 Caretaker: 负责人，对备忘录 Memento 进行管理，保存和提供备忘录，但不能对备忘录的内容进行操作和访问，只能够将备忘录传递给其他对象。 备忘录模式的核心就是备忘录 Memento，在备忘录中存储的就是原发器（也就是以上的saveMemento） Originator 的部分或者所有的状态信息，而这些状态信息是不能够被其他对象所访问的，也就是说我们是不能使用备忘录之外的对象来存储这些状态信息，如果暴漏了内部状态信息就违反了封装的原则，故备忘录除了原发器外其他对象都不可以访问。所以为了实现备忘录模式的封装，我们需要对备忘录的访问做些控制：（1）对原发器：可以访问备忘录里的所有信息。（2）对负责人：不可以访问备忘录里面的数据，但是他可以保存备忘录并且可以将备忘录传递给其他对象。（3）其他对象：不可访问也不可以保存，它只负责接收从负责人那里传递过来的备忘录同时恢复原发器的状态。 优点： 实现了信息的封装，用户不需要关心状态的保存细节，窄接口保证了只有发起者才能访问备忘录对象的状态； 简化了原发器，把备忘录对象保存到原发器对象之外，这样原发器角色就不需要对各个备份的状态进行管理。 缺点：在实际应用中，备忘录模式都是多状态和多备份的，每保存一次对象状态都需要消耗一定的系统资源，如果需要保存的原发器类的成员变量太多，就不可避免需要占用大量的内存空间 适用场景： 如果有需要提供回滚操作的需求，使用备忘录模式非常适合，比如jdbc的事务操作，文本编辑器的Ctrl+Z恢复等。 保存一个对象在某一个时刻的全部状态或部分状态，这样以后需要时它能够恢复到先前的状态，实现撤销操作。防止外界对象破坏一个对象历史状态的封装性。 状态模式 state允许对象在内部状态发生改变时改变他的行为，对象看起来好像修改了它的类用类表示状态 状态模式，就是允许对象在内部状态发生改变时改变它的行为，对象看起来就好像修改了它的类，也就是说以状态为原子来改变它的行为，而不是通过行为来改变状态。例如： 房间的状态改变（退订 预定 空闲） 可进行的操作也就改变了一个行为下的多种变更 例如登录和不登陆 看到的网站样式是不同的 而登录和不登陆就是我们通过改变状态 而让整个行为发生了变化 假设订房 房间就会有几种状态 退房 预定 空闲 当状态改变的时候 他的行为也改变 例如 空闲无法退房 代码实现订房 退房 12345678910111213141516171819202122232425/** * 抽象状态接口 预定入住等等 */public interface State &#123; /** * @desc 预订房间 */ public void bookRoom(); /** * @desc 退订房间 */ public void unsubscribeRoom(); /** * @desc 入住 */ public void checkInRoom(); /** * @desc 退房 */ public void checkOutRoom();&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192/** * 房间类 */public class Room &#123; /* * 房间的三个状态 */ State freeTimeState; //空闲状态 State checkInState; //入住状态 State bookedState; //预订状态 State state ; /** * 实例化房间类的时候 需要指定它的状态 */ public Room()&#123; // 将此房价所有状态初始化 有state来决定 freeTimeState = new FreeTimeState(this); checkInState = new CheckInState(this); bookedState = new BookedState(this); state = freeTimeState ; //初始状态为空闲 &#125; /** * @desc 预订房间 */ public void bookRoom()&#123; state.bookRoom(); // 初始是空闲 会调用freeTimeState的bookRoom方法 &#125; /** * @desc 退订房间 */ public void unsubscribeRoom()&#123; state.unsubscribeRoom(); &#125; /** * @desc 入住 */ public void checkInRoom()&#123; state.checkInRoom(); &#125; /** * @desc 退房 */ public void checkOutRoom()&#123; state.checkOutRoom(); &#125; public String toString()&#123; return &quot;该房间的状态是:&quot;+getState().getClass().getName(); &#125; /* * getter和setter方法 */ public State getFreeTimeState() &#123; return freeTimeState; &#125; public void setFreeTimeState(State freeTimeState) &#123; this.freeTimeState = freeTimeState; &#125; public State getCheckInState() &#123; return checkInState; &#125; public void setCheckInState(State checkInState) &#123; this.checkInState = checkInState; &#125; public State getBookedState() &#123; return bookedState; &#125; public void setBookedState(State bookedState) &#123; this.bookedState = bookedState; &#125; public State getState() &#123; return state; &#125; public void setState(State state) &#123; this.state = state; &#125;&#125; 123456789101112131415161718192021222324252627282930/** * @Description: 空闲状态只能预订和入住 */public class FreeTimeState implements State &#123; Room hotelManagement; public FreeTimeState(Room hotelManagement)&#123; this.hotelManagement = hotelManagement; &#125; public void bookRoom() &#123; System.out.println(&quot;您已经成功预订了...&quot;); hotelManagement.setState(hotelManagement.getBookedState()); //状态变成已经预订 &#125; public void checkInRoom() &#123; System.out.println(&quot;您已经成功入住了...&quot;); hotelManagement.setState(hotelManagement.getCheckInState()); //状态变成已经入住 &#125; public void checkOutRoom() &#123; //不需要做操作 &#125; public void unsubscribeRoom() &#123; //不需要做操作 &#125;&#125; 1234567891011121314151617181920212223242526/** * @Description: 入住可以退房 */public class CheckInState implements State &#123; Room hotelManagement; public CheckInState(Room hotelManagement) &#123; this.hotelManagement = hotelManagement; &#125; public void bookRoom() &#123; System.out.println(&quot;该房间已经入住了...&quot;); &#125; public void checkInRoom() &#123; System.out.println(&quot;该房间已经入住了...&quot;); &#125; public void checkOutRoom() &#123; System.out.println(&quot;退房成功....&quot;); hotelManagement.setState(hotelManagement.getFreeTimeState()); //状态变成空闲 &#125; public void unsubscribeRoom() &#123; //不需要做操作 &#125;&#125; 12345678910111213141516171819202122232425262728/** * @Description: 入住状态房间只能退房 */public class BookedState implements State &#123; Room hotelManagement; public BookedState(Room hotelManagement) &#123; this.hotelManagement = hotelManagement; &#125; public void bookRoom() &#123; System.out.println(&quot;该房间已近给预定了...&quot;); &#125; public void checkInRoom() &#123; System.out.println(&quot;入住成功...&quot;); hotelManagement.setState(hotelManagement.getCheckInState()); //状态变成入住 &#125; public void checkOutRoom() &#123; //不需要做操作 &#125; public void unsubscribeRoom() &#123; System.out.println(&quot;退订成功,欢迎下次光临...&quot;); hotelManagement.setState(hotelManagement.getFreeTimeState()); //变成空闲状态 &#125;&#125; 123456789101112131415161718192021public static void main(String[] args) &#123; //有3间房 Room[] rooms = new Room[2]; //初始化 for(int i = 0 ; i &lt; rooms.length ; i++)&#123; rooms[i] = new Room(); &#125; //第一间房 rooms[0].bookRoom(); //预订 开始是空闲状态 预定以后 就会更改类的状态 rooms[0].checkInRoom(); //入住 rooms[0].bookRoom(); //预订 System.out.println(rooms[0]); System.out.println(&quot;---------------------------&quot;); //第二间房 rooms[1].checkInRoom(); rooms[1].bookRoom(); rooms[1].checkOutRoom(); rooms[1].bookRoom(); System.out.println(rooms[1]);&#125; 对于以上来说 room的状态改变 对应的操作也改变了 角色： Context：环境类，可以包括一些内部状态 State：抽象状态类，定义了所有具体状态的共同接口，任何状态都需要实现这个接口，从而实现状态间的互相转换 ConcreteState：具体状态类，处理来自 Context 的请求，每一个 ConcreteState 都提供了它对自己请求的实现，所以，当 Context 改变状态时行为也会跟着改变 所以状态模式适用于：代码中包含大量与对象状态有关的条件语句，以及对象的行为依赖于它的状态，并且可以根据它的状态改变而改变它的相关行为。 享元模式 flyweight享元模式通过共享技术有效地支持细粒度、状态变化小的对象复用，当系统中存在有多个相同的对象，那么只共享一份，不必每个都去实例化一个对象，极大地减少系统中对象的数量。说白了 尽量通过共享实例来避免new出实例比如说一个文本系统，每个字母定一个对象，那么大小写字母一共就是52个，那么就要定义52个对象。如果有一个1M的文本，那么字母是何其的多，如果每个字母都定义一个对象那么内存早就爆了。那么如果要是每个字母都共享一个对象，那么就大大节约了资源。 内部状态：在享元对象内部不随外界环境改变而改变的共享部分。 外部状态：随着环境的改变而改变，不能够共享的状态就是外部状态。 享元模式的核心是享元工厂类，享元工厂类维护了一个对象存储池，当客户端需要对象时，首先从享元池中获取，如果享元池中存在对象实例则直接返回，如果享元池中不存在，则创建一个新的享元对象实例返回给用户，并在享元池中保存该新增对象，这点有些单例的意思。工厂类通常会使用集合类型来保存对象，如 HashMap、Hashtable、Vector 等等，在 Java 中，数据库连接池、线程池等都是用享元模式的应用。 在 Java 中，String 类型就是使用享元模式，String 对象是 final 类型，对象一旦创建就不可改变。而 Java 的字符串常量都是存在字符串常量池中的，JVM 会确保一个字符串常量在常量池中只有一个拷贝。 12345678String a = &quot;hello&quot;;String b = &quot;hello&quot;;if(a == b) System.out.println(&quot;OK&quot;);else System.out.println(&quot;Error&quot;);// 结果是 ok 场景：假如我们有一个绘图的应用程序，通过它我们可以出绘制各种各样的形状、颜色的图形，那么这里形状和颜色就是内部状态了，通过享元模式我们就可以实现该属性的共享了 123public abstract class Shape &#123; public abstract void draw();&#125; 12345678910public class Circle extends Shape&#123; private String color; public Circle(String color)&#123; this.color = color; &#125; public void draw() &#123; System.out.println(&quot;画了一个&quot; + color +&quot;的圆形&quot;); &#125;&#125; 123456789101112131415161718192021//核心类public class FlyweightFactory&#123; /** * 定义个共享池 如果用户需要的 共享池中有就返回 没有就新建 这就是享元模式 */ static Map&lt;String, Shape&gt; shapes = new HashMap&lt;String, Shape&gt;(); public static Shape getShape(String key)&#123; Shape shape = shapes.get(key); //如果shape==null,表示不存在,则新建,并且保持到共享池中 if(shape == null)&#123; shape = new Circle(key); shapes.put(key, shape); &#125; return shape; &#125; public static int getSum()&#123; return shapes.size(); &#125;&#125; 12345678910111213141516171819202122public static void main(String[] args) &#123; Shape shape1 = FlyweightFactory.getShape(&quot;红色&quot;); shape1.draw(); Shape shape2 = FlyweightFactory.getShape(&quot;灰色&quot;); shape2.draw(); Shape shape3 = FlyweightFactory.getShape(&quot;绿色&quot;); shape3.draw(); Shape shape4 = FlyweightFactory.getShape(&quot;红色&quot;); shape4.draw(); Shape shape5 = FlyweightFactory.getShape(&quot;灰色&quot;); shape5.draw(); Shape shape6 = FlyweightFactory.getShape(&quot;灰色&quot;); shape6.draw(); System.out.println(&quot;一共绘制了&quot;+FlyweightFactory.getSum()+&quot;中颜色的圆形&quot;); // 结果是只创建了3个&#125; 适用场景：（1）如果系统中存在大量的相同或者相似的对象，由于这类对象的大量使用，会造成系统内存的耗费，可以使用享元模式来减少系统中对象的数量。（2）对象的大部分状态都可以外部化，可以将这些外部状态传入对象中。 代理模式 proxy为其他对象提供一个代理以控制这个对象的访问 代理模式的设计动机是通过代理对象来访问真实对象，通过建立一个对象代理类，由代理对象控制原对象的引用，从而实现对真实对象的操作。在代理模式中，代理对象主要起到一个中介的作用，用于协调与连接调用者(即客户端)和被调用者(即目标对象)，在一定程度上降低了系统的耦合度，同时也保护了目标对象。但缺点是在调用者与被调用者之间增加了代理对象，可能会造成请求的处理速度变慢， 最常见的就是mybatis dao层 接口和xml 定义接口方法 不写实现了 而是在xml写sql 实际上他就是通过代理来实现的 代理根据接口生成impl类 实现类的内容是根据xml生成的 当然也用到了反射 业务：某天你看到一位女生，一见钟情，心里发誓要她做你女朋友，但是你想这样直接上去可能会唐突了。于是你采用迂回政策，先和她室友搞好关系，然后通过她室友给她礼物，然后…… 代码实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879public interface GiveGift &#123; /** * 送花 */ void giveFlowers(); /** * 送巧克力 */ void giveChocolate(); /** * 送书 */ void giveBook();&#125;public class BeautifulGirl &#123; String name; public BeautifulGirl(String name)&#123; this.name = name; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125;public class You implements GiveGift &#123; BeautifulGirl mm ; //美女 public You(BeautifulGirl mm)&#123; this.mm = mm; &#125; public void giveBook() &#123; System.out.println(mm.getName() +&quot;,送你一本书....&quot;); &#125; public void giveChocolate() &#123; System.out.println(mm.getName() + &quot;,送你一盒巧克力....&quot;); &#125; public void giveFlowers() &#123; System.out.println(mm.getName() + &quot;,送你一束花....&quot;); &#125;&#125;/** * 代理对象 */public class HerChum implements GiveGift&#123; /** * 代理的对象 */ You you; public HerChum(BeautifulGirl mm)&#123; you = new You(mm); &#125; public void giveBook() &#123; you.giveBook(); &#125; public void giveChocolate() &#123; you.giveChocolate(); &#125; public void giveFlowers() &#123; you.giveFlowers(); &#125;&#125; 12345678public static void main(String[] args) &#123; BeautifulGirl mm = new BeautifulGirl(&quot;张曼玉&quot;); HerChum chum = new HerChum(mm); chum.giveBook(); chum.giveChocolate(); chum.giveFlowers();&#125; 对于以上 代理类 代替you 给 Girl实现功能角色 Subject：抽象角色，声明了真实对象和代理对象的共同接口； Proxy：代理角色，实现了与真实对象相同的接口，所以在任何时刻都能够代理真实对象，并且代理对象内部包含了真实对象的引用，所以它可以操作真实对象，同时也可以附加其他的操作，相当于对真实对象进行封装。 RealSubject：真实对象，是我们最终要引用的对象。 命令模式 command 开发中使用少命令模式的本质是将请求封装成对象，将发出命令与执行命令的责任分开，命令的发送者和接收者完全解耦，发送者只需知道如何发送命令，不需要关心命令是如何实现的，甚至是否执行成功都不需要理会。命令模式的关键在于引入了抽象命令接口，发送者针对抽象命令接口编程，只有实现了抽象命令接口的具体命令才能与接收者相关联。另外命令可以像强对象一样可以被存储和传递，所以可支持撤销的操作 在开发中，我们可能需要向某些对象发送一些请求，但我们不知道请求的具体接收者是谁，也不知道被请求的操作是哪个，只知道在系统运行中指定具体的请求接收者即可，打个比方，电视遥控器，我们只需知道按哪个按钮能够打开电视、关闭电视和换台即可，并不需要知道是怎么开电视、关电视和换台的，对于这种情况，我们可以采用命令模式来进行设计。 电视是请求的接受者，遥控器是请求的发送者，遥控器上有一些按钮，不同的按钮对应着不同的操作。在这里遥控器需要执行三个命令：打开电视机、关闭电视机、换台。代码实现： 12345678/** * Command命令接口，为所有的命令声明一个接口。所有的命令都应该实现它 */public interface Command &#123; public void execute();&#125; 1234567891011121314public class Television &#123; public void open()&#123; System.out.println(&quot;打开电视机......&quot;); &#125; public void close()&#123; System.out.println(&quot;关闭电视机......&quot;); &#125; public void changeChannel()&#123; System.out.println(&quot;切换电视频道......&quot;); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435/** * 这里的controller 只需要执行命令 具体内部做了什么 不需要知道 */public class Controller &#123; private Command openTVCommand; private Command closeTVCommand; private Command changeChannelCommand; public Controller(Command openTvCommand,Command closeTvCommand,Command changeChannelCommand)&#123; this.openTVCommand = openTvCommand; this.closeTVCommand = closeTvCommand; this.changeChannelCommand = changeChannelCommand; &#125; /** * 打开电视剧 */ public void open()&#123; openTVCommand.execute(); &#125; /** * 关闭电视机 */ public void close()&#123; closeTVCommand.execute(); &#125; /** * 换频道 */ public void change()&#123; changeChannelCommand.execute(); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435public class ChangeChannelCommand implements Command&#123; private Television tv; public ChangeChannelCommand(Television tv)&#123; this.tv = tv; &#125; public void execute() &#123; tv.changeChannel(); &#125;&#125;public class CloseTvCommand implements Command&#123; private Television tv; public CloseTvCommand(Television tv)&#123; this.tv = tv; &#125; public void execute() &#123; tv.close(); &#125;&#125;public class OpenTvCommand implements Command&#123; private Television tv; public OpenTvCommand(Television tv)&#123; this.tv = tv; &#125; public void execute() &#123; tv.open(); &#125;&#125; 1234567891011121314public static void main(String a[]) &#123; Television tv = new Television(); Command openCommand, closeCommand, changeCommand; openCommand = new OpenTvCommand(tv); closeCommand = new CloseTvCommand(tv); changeCommand = new ChangeChannelCommand(tv); Controller control = new Controller(openCommand, closeCommand, changeCommand); control.open(); //打开电视机 control.change(); //换频道 control.close(); //关闭电视机&#125; 对于以上而言 controller只需要发送请求 不需要知道谁处理 如何处理 Receiver：接收者，执行命令的对象，任何类都可能成为一个接收者，只要它能够实现命令要求实现的相应功能。 Command：抽象命令类，声明需要执行的方法。 ConcreteCommand：具体命令类，通常会持有接收者，并调用接收者的功能完成命令要执行的操作。 Invoker：调用者，通常会持有命令对象，可以持有多个命令对象，是客户端真正触发命令并要求命令执行相应操作的地方，就是相当于使用命令对象的入口。 Client：客户类，创建具体的命令对象，并且设置命令对象的接收者。注意这里不是指常规意义上的客户端，把这个 Client 称为装配者会合适，主要用于组装命令对象和接收者。 解释器模式 interpreter解释器模式，就是定义语言的文法，并建立一个解释器来解释该语言中的句子，通过构建解释器，解决某一频繁发生的特定类型问题实例。这里我们将语言理解成使用规定格式和语法的代码 适用场景：（1）有一个简单的语法规则，比如一个sql语句，如果我们需要根据sql语句进行rm转换，就可以使用解释器模式来对语句进行解释。（2）一些重复发生的问题，比如加减乘除四则运算，但是公式每次都不同，有时是a+b-cd，有时是ab+c-d，等等，公式千变万化，但是都是由加减乘除四个非终结符来连接的，这时我们就可以使用解释器模式。 例如我们经常利用正则表达式检测某些字符串是否符合我们规定的格式。这里正则表达式就是解释器模式的应用，解释器为正则表达式定义了一个文法，如何表示一个特定的正则表达式，以及如何解释这个正则表达式。 语法分析器 通过定义一系列的定义 根据解析器来解释它意思","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[]},{"title":"Nginx","slug":"Nginx ","date":"2023-07-31T09:31:12.711Z","updated":"2023-07-31T09:31:36.083Z","comments":true,"path":"2023/07/31/Nginx /","link":"","permalink":"http://example.com/2023/07/31/Nginx%20/","excerpt":"","text":"NginxNginx简介背景介绍Nginx（“engine x”）一个具有高性能的【HTTP】和【反向代理】的【WEB服务器】，同时也是一个【POP3&#x2F;SMTP&#x2F;IMAP代理服务器】，是由伊戈尔·赛索耶夫(俄罗斯人)使用C语言编写的，Nginx的第一个版本是2004年10月4号发布的0.1.0版本。另外值得一提的是伊戈尔·赛索耶夫将Nginx的源码进行了开源，这也为Nginx的发展提供了良好的保障。 名词解释 WEB服务器： WEB服务器也叫网页服务器，英文名叫Web Server，主要功能是为用户提供网上信息浏览服务。 HTTP:(协议) HTTP是超文本传输协议的缩写，是用于从WEB服务器传输超文本到本地浏览器的传输协议，也是互联网上应用最为广泛的一种网络协议。HTTP是一个客户端和服务器端请求和应答的标准，客户端是终端用户，服务端是网站，通过使用Web浏览器、网络爬虫或者其他工具，客户端发起一个到服务器上指定端口的HTTP请求。 POP3&#x2F;SMTP&#x2F;IMAP： POP3(Post Offic Protocol 3)邮局协议的第三个版本，SMTP(Simple Mail Transfer Protocol)简单邮件传输协议，IMAP(Internet Mail Access Protocol)交互式邮件存取协议，通过上述名词的解释，我们可以了解到Nginx也可以作为电子邮件代理服务器。 反向代理 正向代理：替客户端向服务器发送请求假设 客户端不能向服务器发送请求（例如 跨域问题） 我们需要找一个代理服务器发送就行了客户端-&gt; 代理服务器 -&gt; 目标服务器上图就是正向代理 所谓的魔法（翻墙） 就是用的这种模式 正向代理代理的是客户端请求 让客户端认为这才是真正的服务器 反向代理反向代理 是为了缓解服务器的压力 将请求部署在多台服务器上 这个时候前端发起请求就要根据不同的url去访问 可以通过访问代理服务器（前端发送同一个地址） 服务器去转发到不同的后端处理反向代理代理的是服务器端 常见服务器对比在介绍这一节内容之前，我们先来认识一家公司叫Netcraft。Netcraft公司于1994年底在英国成立，多年来一直致力于互联网市场以及在线安全方面的咨询服务，其中在国际上最具影响力的当属其针对网站服务器、SSL市场所做的客观严谨的分析研究，公司官网每月公布的调研数据（Web Server Survey）已成为当今人们了解全球网站数量以及服务器市场分额情况的主要参考依据，时常被诸如华尔街杂志，英国BBC，Slashdot等媒体报道或引用。我们先来看一组数据，我们先打开Nginx的官方网站 http://nginx.org/,找到Netcraft公司公布的数据，对当前主流服务器产品进行介绍。 几种常见的服务器： IIS全称(Internet Information Services)即互联网信息服务，是由微软公司提供的基于windows系统的互联网基本服务。windows作为服务器在稳定性与其他一些性能上都不如类UNIX操作系统，因此在需要高性能Web服务器的场合下，IIS可能就会被&quot;冷落&quot;. TomcatTomcat是一个运行Servlet和JSP的Web应用软件，Tomcat技术先进、性能稳定而且开放源代码，因此深受Java爱好者的喜爱并得到了部分软件开发商的认可，成为目前比较流行的Web应用服务器。但是Tomcat天生是一个重量级的Web服务器，对静态文件和高并发的处理比较弱。 ApacheApache的发展时期很长，同时也有过一段辉煌的业绩。从上图可以看出大概在2014年以前都是市场份额第一的服务器。Apache有很多优点，如稳定、开源、跨平台等。但是它出现的时间太久了，在它兴起的年代，互联网的产业规模远远不如今天，所以它被设计成一个重量级的、不支持高并发的Web服务器。在Apache服务器上，如果有数以万计的并发HTTP请求同时访问，就会导致服务器上消耗大量能存，操作系统内核对成百上千的Apache进程做进程间切换也会消耗大量的CUP资源，并导致HTTP请求的平均响应速度降低，这些都决定了Apache不可能成为高性能的Web服务器。这也促使了Lighttpd和Nginx的出现。 LighttpdLighttpd是德国的一个开源的Web服务器软件，它和Nginx一样，都是轻量级、高性能的Web服务器，欧美的业界开发者比较钟爱Lighttpd,而国内的公司更多的青睐Nginx，同时网上Nginx的资源要更丰富些。 其他的服务器Google Servers，Weblogic, Webshpere(IBM)…经过各个服务器的对比，种种迹象都表明，Nginx将以性能为王。这也是我们为什么选择Nginx的理由。 Nginx的优点(1)速度更快、并发更高单次请求或者高并发请求的环境下，Nginx都会比其他Web服务器响应的速度更快。一方面在正常情况下，单次请求会得到更快的响应，另一方面，在高峰期(如有数以万计的并发请求)，Nginx比其他Web服务器更快的响应请求。Nginx之所以有这么高的并发处理能力和这么好的性能原因在于Nginx采用了多进程和I&#x2F;O多路复用(epoll)的底层实现。 (2)配置简单，扩展性强Nginx的设计极具扩展性，它本身就是由很多模块组成，这些模块的使用可以通过配置文件的配置来添加。这些模块有官方提供的也有第三方提供的模块，如果需要完全可以开发服务自己业务特性的定制模块。 (3)高可靠性Nginx采用的是多进程模式运行，其中有一个master主进程和N多个worker进程，worker进程的数量我们可以手动设置，每个worker进程之间都是相互独立提供服务，并且master主进程可以在某一个worker进程出错时，快速去”拉起”新的worker进程提供服务。 (4)热部署现在互联网项目都要求以7*24小时进行服务的提供，针对于这一要求，Nginx也提供了热部署功能，即可以在Nginx不停止的情况下，对Nginx进行文件升级、更新配置和更换日志文件等功能。 (5)成本低、BSD许可证BSD是一个开源的许可证，世界上的开源许可证有很多，现在比较流行的有六种分别是GPL、BSD、MIT、Mozilla、Apache、LGPL。这六种的区别是什么，我们可以通过下面一张图来解释下：Nginx本身是开源的，我们不仅可以免费的将Nginx应用在商业领域，而且还可以在项目中直接修改Nginx的源码来定制自己的特殊要求。这些点也都是Nginx为什么能吸引无数开发者继续为Nginx来贡献自己的智慧和青春。OpenRestry [Nginx+Lua] Tengine[淘宝] Nginx的功能特性及常用功能Nginx提供的基本功能服务从大体上归纳为”基本HTTP服务“、“高级HTTP服务”和”邮件服务”等三大类。 基本HTTP服务Nginx可以提供基本HTTP服务，可以作为HTTP代理服务器和反向代理服务器，支持通过缓存加速访问，可以完成简单的负载均衡和容错，支持包过滤功能，支持SSL等。 处理静态文件、处理索引文件以及支持自动索引； 提供反向代理服务器，并可以使用缓存加上反向代理，同时完成负载均衡和容错； 提供对FastCGI、memcached等服务的缓存机制，，同时完成负载均衡和容错； 使用Nginx的模块化特性提供过滤器功能。Nginx基本过滤器包括gzip压缩、ranges支持、chunked响应、XSLT、SSI以及图像缩放等。其中针对包含多个SSI的页面，经由FastCGI或反向代理，SSI过滤器可以并行处理。 支持HTTP下的安全套接层安全协议SSL. 支持基于加权和依赖的优先权的HTTP&#x2F;2 高级HTTP服务 支持基于名字和IP的虚拟主机设置 支持HTTP&#x2F;1.0中的KEEP-Alive模式和管线(PipeLined)模型连接 自定义访问日志格式、带缓存的日志写操作以及快速日志轮转。 提供3xx~5xx错误代码重定向功能 支持重写（Rewrite)模块扩展 支持重新加载配置以及在线升级时无需中断正在处理的请求 支持网络监控 支持FLV和MP4流媒体传输 邮件服务Nginx提供邮件代理服务也是其基本开发需求之一，主要包含以下特性： 支持IMPA&#x2F;POP3代理服务功能 支持内部SMTP代理服务功能 Nginx常用的功能模块12345678910静态资源部署Rewrite地址重写 正则表达式反向代理负载均衡 轮询、加权轮询、ip_hash、url_hash、fairWeb缓存环境部署 高可用的环境用户认证模块... Nginx的核心组成 1234nginx二进制可执行文件 nginx.conf配置文件error.log错误的日志记录access.log访问日志记录 Nginx环境准备Nginx版本介绍Nginx的官方网站为: http://nginx.orgNginx的官方下载网站为http://nginx.org/en/download.html，当然你也可以之间在首页选中右边的download进入版本下载网页。在下载页面我们会看到如下内容：下载linux的 获取Nginx源码http://nginx.org/download/打开上述网站，就可以查看到Nginx的所有版本，选中自己需要的版本进行下载。下载我们可以直接在windows上下载然后上传到服务器，也可以直接从服务器上下载，这个时候就需要准备一台服务器。 准备服务器系统环境准备 12345VMware WorkStationCentos7MobaXterm xsheel,SecureCRT网络 (1)确认centos的内核准备一个内核为2.6及以上版本的操作系统，因为linux2.6及以上内核才支持epoll,而Nginx需要解决高并发压力问题是需要用到epoll，所以我们需要有这样的版本要求。我们可以使用uname -a命令来查询linux的内核版本。(2)确保centos能联网三种模式(3)确认关闭防火墙这一项的要求仅针对于那些对linux系统的防火墙设置规则不太清楚的，建议大家把防火墙都关闭掉，因为我们此次课程主要的内容是对Nginx的学习，把防火墙关闭掉，可以省掉后续Nginx学习过程中遇到的诸多问题。关闭的方式有如下两种： 123systemctl stop firewalld 关闭运行的防火墙，系统重新启动后，防火墙将重新打开systemctl disable firewalld 永久关闭防火墙，，系统重新启动后，防火墙依然关闭systemctl status firewalld 查看防火墙状态 （4）确认停用selinuxselinux(security-enhanced linux),美国安全局对于强制访问控制的实现，在linux2.6内核以后的版本中，selinux已经成功内核中的一部分。可以说selinux是linux史上最杰出的新安全子系统之一。虽然有了selinux，我们的系统会更安全，但是对于我们的学习Nginx的历程中，会多很多设置，所以这块建议大家将selinux进行关闭sestatus命令查看状态如果查看不是disabled状态，我们可以通过修改配置文件来进行设置,修改SELINUX&#x3D;disabled，然后重启下系统即可生效。vim &#x2F;etc&#x2F;selinux&#x2F;config Nginx安装方式介绍Nginx的安装方式有两种分别是: 1234通过Nginx源码 通过Nginx源码简单安装 (1) 通过Nginx源码复杂安装 (3)通过yum安装 (2) 如果通过Nginx源码安装需要提前准备的内容： GCC编译器Nginx是使用C语言编写的程序，因此想要运行Nginx就需要安装一个编译工具。GCC就是一个开源的编译器集合，用于处理各种各样的语言，其中就包含了C语言。 (想要运行java需要有jdk)使用命令yum install -y gcc来安装安装成功后，可以通过gcc –version来查看gcc是否安装成功 PCRENginx在编译过程中需要使用到PCRE库（perl Compatible Regular Expressoin 兼容正则表达式库)，因为在Nginx的Rewrite模块和http核心模块都会使用到PCRE正则表达式语法。可以使用命令yum install -y pcre pcre-devel来进行安装安装成功后，可以通过rpm -qa pcre pcre-devel来查看是否安装成功 zlibzlib库提供了开发人员的压缩算法，在Nginx的各个模块中需要使用gzip压缩，所以我们也需要提前安装其库及源代码zlib和zlib-devel可以使用命令yum install -y zlib zlib-devel来进行安装安装成功后，可以通过rpm -qa zlib zlib-devel来查看是否安装成功 OpenSSLOpenSSL是一个开放源代码的软件库包，应用程序可以使用这个包进行安全通信，并且避免被窃听。SSL:Secure Sockets Layer安全套接协议的缩写，可以在Internet上提供秘密性传输，其目标是保证两个应用间通信的保密性和可靠性。在Nginx中，如果服务器需要提供安全网页时就需要用到OpenSSL库，所以我们需要对OpenSSL的库文件及它的开发安装包进行一个安装。可以使用命令yum install -y openssl openssl-devel来进行安装安装成功后，可以通过rpm -qa openssl openssl-devel来查看是否安装成功上述命令，一个个来的话比较麻烦，我们也可以通过一条命令来进行安装yum install -y gcc pcre pcre-devel zlib zlib-devel openssl openssl-devel进行全部安装。 方案一：Nginx的源码简单安装(1)进入官网查找需要下载版本的链接地址，然后使用wget命令进行下载wget http://nginx.org/download/nginx-1.16.1.tar.gz(2)建议大家将下载的资源进行包管理 12mkdir -p nginx/coremv nginx-1.16.1.tar.gz nginx/core (3)解压缩tar -xzf nginx-1.16.1.tar.gz(4)进入资源文件中，发现configure.&#x2F;configure(5)编译make(6)安装make install 方案二：yum安装使用源码进行简单安装，我们会发现安装的过程比较繁琐，需要提前准备GCC编译器、PCRE兼容正则表达式库、zlib压缩库、OpenSSL安全通信的软件库包，然后才能进行Nginx的安装。（1）安装yum-utilssudo yum install -y yum-utils（2）添加yum源文件vim /etc/yum.repos.d/nginx.repo 123456789101112131415[nginx-stable]name=nginx stable repobaseurl=http://nginx.org/packages/centos/$releasever/$basearch/gpgcheck=1enabled=1gpgkey=https://nginx.org/keys/nginx_signing.keymodule_hotfixes=true[nginx-mainline]name=nginx mainline repobaseurl=http://nginx.org/packages/mainline/centos/$releasever/$basearch/gpgcheck=1enabled=0gpgkey=https://nginx.org/keys/nginx_signing.keymodule_hotfixes=true （3）查看是否安装成功yum list | grep nginx（4）使用yum进行安装yum install -y nginx（5）查看nginx的安装位置whereis nginx（6）启动测试进入 cd/usr/sbin 执行 ./nginx启动nginx 关于yum安装以后 一些文件的位置https://www.cnblogs.com/zouhong/p/14790750.html日志文件(error.log) 在&#x2F;var&#x2F;log&#x2F;nginx&#x2F; 源码简单安装和yum安装的差异：这里先介绍一个命令: .&#x2F;nginx -V,通过该命令可以查看到所安装Nginx的版本及相关配置信息。简单安装 configure arguments这一栏是空yum安装 configure arguments这一栏有很多的配置 解压Nginx目录执行tar -zxvf nginx-1.16.1.tar.gz对下载的资源进行解压缩，进入压缩后的目录，可以看到如下结构内容解释：auto:存放的是编译相关的脚本CHANGES:版本变更记录CHANGES.ru:俄罗斯文的版本变更记录conf:nginx默认的配置文件configure:nginx软件的自动脚本程序,是一个比较重要的文件，作用如下： （1）检测环境及根据环境检测结果生成C代码 （2）生成编译代码需要的Makefile文件contrib:存放的是几个特殊的脚本文件，其中README中对脚本有着详细的说明html:存放的是Nginx自带的两个html页面，访问Nginx的首页和错误页面LICENSE:许可证的相关描述文件man:nginx的man手册README:Nginx的阅读指南src:Nginx的源代码 方案三:Nginx的源码复杂安装这种方式和简单的安装配置不同的地方在第一步，通过.&#x2F;configure来对编译参数进行设置，需要我们手动来指定。那么都有哪些参数可以进行设置，接下来我们进行一个详细的说明。PATH:是和路径相关的配置信息with:是启动模块，默认是关闭的without:是关闭模块，默认是开启的我们先来认识一些简单的路径配置已经通过这些配置来完成一个简单的编译：–prefix&#x3D;PATH指向Nginx的安装目录，默认值为&#x2F;usr&#x2F;local&#x2F;nginx–sbin-path&#x3D;PATH指向(执行)程序文件(nginx)的路径,默认值为&#x2F;sbin&#x2F;nginx–modules-path&#x3D;PATH 指向Nginx动态模块安装目录，默认值为&#x2F;modules**–conf-path&#x3D;PATH **这里需要注意指向配置文件(nginx.conf)的路径,默认值为&#x2F;conf&#x2F;nginx.conf –error-log-path&#x3D;PATH指向错误日志文件的路径,默认值为&#x2F;logs&#x2F;error.log–http-log-path&#x3D;PATH指向访问日志文件的路径,默认值为&#x2F;logs&#x2F;access.log–pid-path&#x3D;PATH指向Nginx启动后进行ID的文件路径，默认值为&#x2F;logs&#x2F;nginx.pid–lock-path&#x3D;PATH指向Nginx锁文件的存放路径,默认值为&#x2F;logs&#x2F;nginx.lock要想使用可以通过如下命令 12345678./configure --prefix=/usr/local/nginx \\--sbin-path=/usr/local/nginx/sbin/nginx \\--modules-path=/usr/local/nginx/modules \\--conf-path=/usr/local/nginx/conf/nginx.conf \\--error-log-path=/usr/local/nginx/logs/error.log \\--http-log-path=/usr/local/nginx/logs/access.log \\--pid-path=/usr/local/nginx/logs/nginx.pid \\--lock-path=/usr/local/nginx/logs/nginx.lock 在使用上述命令之前，需要将之前服务器已经安装的nginx进行卸载，卸载的步骤分为三步骤：步骤一：需要将nginx的进程关闭./nginx -s stop步骤二:将安装的nginx进行删除rm -rf /usr/local/nginx步骤三:将安装包之前编译的环境清除掉make clean Nginx目录结构分析在使用Nginx之前，我们先对安装好的Nginx目录文件进行一个分析，在这块给大家介绍一个工具tree，通过tree我们可以很方面的去查看centos系统上的文件目录结构，当然，如果想使用tree工具，就得先通过yum install -y tree来进行安装，安装成功后，可以通过执行tree &#x2F;usr&#x2F;local&#x2F;nginx(tree后面跟的是Nginx的安装目录,我们用的是yum 目录在 etc&#x2F;nginx)，获取的结果如下： conf:nginx所有配置文件目录CGI(Common Gateway Interface)通用网关【接口】，主要解决的问题是从客户端发送一个请求和数据，服务端获取到请求和数据后可以调用调用CGI【程序】处理及相应结果给客户端的一种标准规范。 fastcgi.conf:fastcgi相关配置文件 fastcgi.conf.default:fastcgi.conf的备份文件 fastcgi_params:fastcgi的参数文件 fastcgi_params.default:fastcgi的参数备份文件 scgi_params:scgi的参数文件 scgi_params.default：scgi的参数备份文件uwsgi_params:uwsgi的参数文件 uwsgi_params.default:uwsgi的参数备份文件 mime.types:记录的是HTTP协议中的Content-Type的值和文件后缀名的对应关系 mime.types.default:mime.types的备份文件 nginx.conf:这个是Nginx的核心配置文件，这个文件非常重要，也是我们即将要学习的重点 nginx.conf.default:nginx.conf的备份文件 koi-utf、koi-win、win-utf这三个文件都是与编码转换映射相关的配置文件，用来将一种编码转换成另一种编码html:存放nginx自带的两个静态的html页面 50x.html:访问失败后的失败页面 index.html:成功访问的默认首页logs:记录入门的文件，当nginx服务器启动后，这里面会有 access.log error.log 和nginx.pid三个文件出现。sbin:是存放执行程序文件nginx nginx是用来控制Nginx的启动和停止等相关的命令。 Nginx服务器启停命令Nginx安装完成后，接下来我们要学习的是如何启动、重启和停止Nginx的服务。对于Nginx的启停在linux系统中也有很多种方式，我们本次课程介绍两种方式： Nginx服务的信号控制 Nginx的命令行控制 方式一:Nginx服务的信号控制12345Nginx中的master和worker进程?Nginx的工作方式?如何获取进程的PID?信号有哪些?如何通过信号控制Nginx的启停等相关操作? 前面在提到Nginx的高性能，其实也和它的架构模式有关。Nginx默认采用的是多进程的方式来工作的，当将Nginx启动后，我们通过ps -ef | grep nginx命令可以查看到如下内容：从上图中可以看到,Nginx后台进程中包含一个master进程和多个worker进程，master进程主要用来管理worker进程，包含接收外界的信息，并将接收到的信号发送给各个worker进程，监控worker进程的状态，当worker进程出现异常退出后，会自动重新启动新的worker进程。而worker进程则是专门用来处理用户请求的，各个worker进程之间是平等的并且相互独立，处理请求的机会也是一样的。nginx的进程模型，我们可以通过下图来说明下：worker 用来接收用户请求 master进程用来管理worker进程 管理员只需要发送给master进程 他会分发到worker进程 （1）要想操作Nginx的master进程，就需要获取到master进程的进程号ID。获取方式简单介绍两个，方式一：通过ps -ef | grep nginx；方式二：在讲解nginx的.&#x2F;configure的配置参数的时候，有一个参数是–pid-path&#x3D;PATH默认是&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;logs&#x2F;nginx.pid,所以可以通过查看该文件来获取nginx的master进程ID.（2）信号 信号 作用 TERM&#x2F;INT 立即关闭整个服务 QUIT “优雅”地关闭整个服务 HUP 重读配置文件并使用服务对新配置项生效 USR1 重新打开日志文件，可以用来进行日志切割 USR2 平滑升级到最新版的nginx WINCH 所有子进程不在接收处理新连接，相当于给work进程发送QUIT指令 调用命令为kill -signal PIDsignal:即为信号；PID即为获取到的master线程ID 发送TERM&#x2F;INT信号给master进程，会将Nginx服务立即关闭。 12kill -TERM PID / kill -TERM `cat /usr/local/nginx/logs/nginx.pid`kill -INT PID / kill -INT `cat /usr/local/nginx/logs/nginx.pid` 发送QUIT信号给master进程，master进程会控制所有的work进程不再接收新的请求，等所有请求处理完后，在把进程都关闭掉。 kill -QUIT PID / kill -TERM cat &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;logs&#x2F;nginx.pid&#96;&#96; 发送HUP信号给master进程，master进程会把控制旧的work进程不再接收新的请求，等处理完请求后将旧的work进程关闭掉，然后根据nginx的配置文件重新启动新的work进程 kill -HUP PID / kill -TERM cat &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;logs&#x2F;nginx.pid&#96;&#96; 发送USR1信号给master进程，告诉Nginx重新开启日志文件 kill -USR1 PID / kill -TERM cat &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;logs&#x2F;nginx.pid&#96;&#96; 发送USR2信号给master进程，告诉master进程要平滑升级，这个时候，会重新开启对应的master进程和work进程，整个系统中将会有两个master进程，并且新的master进程的PID会被记录在&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;logs&#x2F;nginx.pid而之前的旧的master进程PID会被记录在&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;logs&#x2F;nginx.pid.oldbin文件中，接着再次发送QUIT信号给旧的master进程，让其处理完请求后再进行关闭 kill -USR2 PID / kill -USR2 cat &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;logs&#x2F;nginx.pid `kill -QUIT PID / kill -QUIT `cat /usr/local/nginx/logs/nginx.pid.oldbin 发送WINCH信号给master进程,让master进程控制不让所有的work进程在接收新的请求了，请求处理完后关闭work进程。注意master进程不会被关闭掉 kill -WINCH PID /kill -WINCHcat &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;logs&#x2F;nginx.pid&#96;&#96; 方式二:Nginx的命令行控制此方式是通过Nginx安装目录下的sbin下的可执行文件nginx来进行Nginx状态的控制，我们可以通过nginx -h来查看都有哪些参数可以用： 12345678910111213Options:-?,-h : this help-v : show version and exit-V : show version and configure options then exit-t : test configuration and exit-T : test configuration, dump it and exit-q : suppress non-error messages during configuration testing-s signal : send signal to a master process: stop, quit, reopen, reload-p prefix : set prefix path (default: /etc/nginx/)-e filename : set error log file (default: /var/log/nginx/error.log)-c filename : set configuration file (default: /etc/nginx/nginx.conf)-g directives : set global directives out of configuration file -?和-h:显示帮助信息-v:打印版本号信息并退出-V:打印版本号信息和配置信息并退出-t:测试nginx的配置文件语法是否正确并退出-T:测试nginx的配置文件语法是否正确并列出用到的配置文件信息然后退出-q:在配置测试期间禁止显示非错误消息-s:signal信号，后面可以跟 ： stop[快速关闭，类似于TERM&#x2F;INT信号的作用] quit[优雅的关闭，类似于QUIT信号的作用] reopen[重新打开日志文件类似于USR1信号的作用] reload[类似于HUP信号的作用]-p:prefix，指定Nginx的prefix路径，(默认为: &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;)-c:filename,指定Nginx的配置文件路径,(默认为: conf&#x2F;nginx.conf)-g:用来补充Nginx配置文件，向Nginx服务指定启动时应用全局的配置 经常-tc 联合使用 测试指定文件是否正确 Nginx服务器版本升级和新增模块如果想对Nginx的版本进行更新，或者要应用一些新的模块，最简单的做法就是停止当前的Nginx服务，然后开启新的Nginx服务。但是这样会导致在一段时间内，用户是无法访问服务器。为了解决这个问题，我们就需要用到Nginx服务器提供的平滑升级功能。这个也是Nginx的一大特点，使用这种方式，就可以使Nginx在7*24小时不间断的提供服务了。接下来我们分析下需求：需求：Nginx的版本最开始使用的是Nginx-1.14.2,由于服务升级，需要将Nginx的版本升级到Nginx-1.16.1,要求Nginx不能中断提供服务。为了应对上述的需求，这里我们给大家提供两种解决方案:方案一:使用Nginx服务信号完成Nginx的升级方案二:使用Nginx安装目录的make命令完成升级 环境准备（1）先准备两个版本的Nginx分别是 1.14.2和1.16.1（2）使用Nginx源码安装的方式将1.14.2版本安装成功并正确访问 123进入安装目录./configuremake &amp;&amp; make install （3）将Nginx1.16.1进行参数配置和编译，不需要进行安装。 123进入安装目录./configuremake 方案一:使用Nginx服务信号进行升级第一步:将1.14.2版本的sbin目录下的nginx进行备份 12cd /usr/local/nginx/sbinmv nginx nginxold 第二步:将Nginx1.16.1安装目录编译后的objs目录下的nginx文件，拷贝到原来&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin目录下 12cd ~/nginx/core/nginx-1.16.1/objscp nginx /usr/local/nginx/sbin 第三步:发送信号USR2给Nginx的1.14.2版本对应的master进程第四步:发送信号QUIT给Nginx的1.14.2版本对应的master进程kill -QUIT more /usr/local/logs/nginx.pid.oldbin 方案二:使用Nginx安装目录的make命令完成升级第一步:将1.14.2版本的sbin目录下的nginx进行备份 12cd /usr/local/nginx/sbinmv nginx nginxold 第二步:将Nginx1.16.1安装目录编译后的objs目录下的nginx文件，拷贝到原来&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin目录下 12cd ~/nginx/core/nginx-1.16.1/objscp nginx /usr/local/nginx/sbin 第三步:进入到安装目录，执行make upgrade第四步:查看是否更新成功.&#x2F;nginx -v在整个过程中，其实Nginx是一直对外提供服务的。并且当Nginx的服务器启动成功后，我们是可以通过浏览器进行直接访问的，同时我们可以通过更改html目录下的页面来修改我们在页面上所看到的内容，那么问题来了，为什么我们要修改html目录下的文件，能不能多添加一些页面是Nginx的功能更加丰富，还有前面聊到Nginx的前端功能又是如何来实现的，这就需要我们对Nginx的核心配置文件进行一个详细的学习。 Nginx核心配置文件结构***从前面的内容学习中，我们知道Nginx的核心配置文件默认是放在/etc/nginx/nginx.conf，这一节，我们就来学习下nginx.conf的内容和基本配置方法。读取Nginx自带的Nginx配置文件，我们将其中的注释部分【学习一个技术点就是在Nginx的配置文件中可以使用#来注释】删除掉后，就剩下下面内容: 1234567891011121314151617181920212223242526worker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; # 十分重要 代理 缓存都是在这里设置 include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server &#123; # 服务 listen 80; server_name localhost; location / &#123; root html; index index.html index.htm; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125;&#125; 123456789101112131415161718指令名 指令值; #全局块，主要设置Nginx服务器整体运行的配置指令#events块,主要设置,Nginx服务器与用户的网络连接,这一部分对Nginx服务器的性能影响较大events &#123; 指令名 指令值;&#125;#http块，是Nginx服务器配置中的重要部分，代理、缓存、日志记录、第三方模块配置... http &#123; 指令名 指令值; server &#123; #server块，是Nginx配置和虚拟主机相关的内容 指令名 指令值; location / &#123; #location块，基于Nginx服务器接收请求字符串与location后面的值进行匹配，对特定请求进行处理 指令名 指令值; &#125; &#125; ...&#125; 简单小结下:nginx.conf配置文件中默认有三大块：全局块、events块、http块http块中可以配置多个server块，每个server块又可以配置多个location块。 全局块user指令（1）user:用于配置运行Nginx服务器的worker进程的用户和用户组。 语法 user user [group] 默认值 nobody 位置 全局块 该属性也可以在编译的时候指定，语法如下.&#x2F;configure –user&#x3D;user –group&#x3D;group,如果两个地方都进行了设置，最终生效的是配置文件中的配置。该指令的使用步骤:(1)设置一个用户信息”www”user www;(2) 创建一个用户useradd www(3)修改user属性user www(4)创建&#x2F;root&#x2F;html&#x2F;index.html页面，添加如下内容 1234567891011121314151617181920212223242526&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;title&gt;Welcome to nginx!&lt;/title&gt; &lt;style&gt; body &#123; width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; &#125; &lt;/style&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Welcome to nginx!&lt;/h1&gt; &lt;p&gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.&lt;/p&gt; &lt;p&gt;For online documentation and support please refer to &lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt; Commercial support is available at &lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;I am WWW&lt;/em&gt;&lt;/p&gt; &lt;/body&gt;&lt;/html&gt; (5)修改nginx.conf 1234location / &#123; root /root/html; index index.html index.htm;&#125; (5)测试启动访问页面会报403拒绝访问的错误(6)分析原因因为当前用户没有访问&#x2F;root&#x2F;html目录的权限(7)将文件创建到 &#x2F;home&#x2F;www&#x2F;html&#x2F;index.html,修改配置 1234location / &#123; root /home/www/html; index index.html index.htm;&#125; (8)再次测试启动访问能正常访问。综上所述，使用user指令可以指定启动运行工作进程的用户及用户组，这样对于系统的权限访问控制的更加精细，也更加安全。 work process指令master_process:用来指定是否开启工作进程。 语法 master_process on&#124;off; 默认值 master_process on; 位置 全局块 worker_processes:用于配置Nginx生成工作进程的数量，这个是Nginx服务器实现并发处理服务的关键所在。理论上来说workder process的值越大，可以支持的并发处理量也越多，但事实上这个值的设定是需要受到来自服务器自身的限制，建议将该值和服务器CPU的内核数保存一致。 语法 worker_processes num&#x2F;auto; 默认值 1 位置 全局块 如果将worker_processes设置成2，则会看到如下内容: 关于以上的命令 一般默认即可 需要在修改 其他指令daemon：设定Nginx是否以守护进程的方式启动。守护式进程是linux后台执行的一种服务进程，特点是独立于控制终端，不会随着终端关闭而停止。 语法 daemon on&#124;off; 默认值 daemon on; 位置 全局块 pid:用来配置Nginx当前master进程的进程号ID存储的文件路径。 语法 pid file; 默认值 默认为:&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;logs&#x2F;nginx.pid 位置 全局块 该属性可以通过.&#x2F;configure –pid-path&#x3D;PATH来指定error_log:用来配置Nginx的错误日志存放路径 语法 error_log file [日志级别]; 默认值 error_log logs&#x2F;error.log error; 位置 全局块、http、server、location 该属性可以通过.&#x2F;configure –error-log-path&#x3D;PATH来指定其中日志级别的值有：debug|info|notice|warn|error|crit|alert|emerg，翻译过来为试|信息|通知|警告|错误|临界|警报|紧急，这块建议大家设置的时候不要设置成info以下的等级，因为会带来大量的磁盘I&#x2F;O消耗，影响Nginx的性能。（5）include:用来引入其他配置文件，使Nginx的配置更加灵活 语法 include file; 默认值 无 位置 any events块（1）accept_mutex:用来设置Nginx网络连接序列化 语法 accept_mutex on&#124;off; 默认值 accept_mutex on; 位置 events 这个配置主要可以用来解决常说的”惊群”（所谓惊群 一个请求来了 所有worker进程 进行抢夺获取 只会有一个成功）问题。大致意思是在某一个时刻，客户端发来一个请求连接，Nginx后台是以多进程的工作模式，也就是说有多个worker进程会被同时唤醒，但是最终只会有一个进程可以获取到连接，如果每次唤醒的进程数目太多，就会影响Nginx的整体性能。如果将上述值设置为on(开启状态)，将会对多个Nginx进程接收连接进行序列号，一个个来唤醒接收，就防止了多个进程对连接的争抢。(这也不一定是绝对好 还是要看业务来进行处理 一个个慢一点) （2）multi_accept:用来设置是否允许同时接收多个网络连接 语法 multi_accept on&#124;off; 默认值 multi_accept off; 位置 events 如果multi_accept被禁止了，nginx一个工作进程只能同时接受一个新的连接。否则，一个工作进程可以同时接受所有的新连接（建议设置为on 打开它）（3）worker_connections：用来配置单个worker进程最大的连接数 语法 worker_connections number; 默认值 worker_commections 512; 位置 events 这里的连接数不仅仅包括和前端用户建立的连接数，而是包括所有可能的连接数。另外，number值不能大于操作系统支持打开的最大文件句柄数量。（4）use:用来设置Nginx服务器选择哪种事件驱动来处理网络消息。 语法 use method; 默认值 根据操作系统定 位置 events 注意：此处所选择事件处理模型是Nginx优化部分的一个重要内容，method的可选值有select&#x2F;poll&#x2F;epoll&#x2F;kqueue等，之前在准备centos环境的时候，我们强调过要使用linux内核在2.6以上，就是为了能使用epoll函数来优化Nginx。另外这些值的选择，我们也可以在编译的时候使用–with-select_module、–without-select_module、–with-poll_module、–without-poll_module来设置是否需要将对应的事件驱动模块编译到Nginx的内核。 events指令配置实例打开Nginx的配置文件 nginx.conf,添加如下配置 123456events&#123; accept_mutex on; multi_accept on; worker_commections 1024; use epoll;&#125; 启动测试 12./nginx -t./nginx -s reload http块 *********定义MIME-Type我们都知道浏览器中可以显示的内容有HTML、XML、GIF等种类繁多的文件、媒体等资源，浏览器为了区分这些资源，就需要使用MIME Type。所以说MIME Type是网络资源的媒体类型。Nginx作为web服务器，也需要能够识别前端请求的资源类型。在Nginx的配置文件中，默认有两行配置 12include mime.types;default_type application/octet-stream; # 这个是说二进制流的方式来处理 （1）default_type:用来配置Nginx响应前端请求默认的MIME类型。 语法 default_type mime-type; 默认值 default_type text&#x2F;plain； 位置 http、server、location 在default_type之前还有一句include mime.types,include之前我们已经介绍过，相当于把mime.types文件中MIMT类型与相关类型文件的文件后缀名的对应关系加入到当前的配置文件中。举例来说明：有些时候请求某些接口的时候需要返回指定的文本字符串或者json字符串，如果逻辑非常简单或者干脆是固定的字符串，那么可以使用nginx快速实现，这样就不用编写程序响应请求了，可以减少服务器资源占用并且响应性能非常快。如何实现: 123456789location /get_text &#123; # 访问这个网址的时候 会按照这个配置 #这里也可以设置成text/plain default_type text/html; return 200 &quot;This is nginx&#x27;s text&quot;;&#125;location /get_json&#123; default_type application/json; return 200 &#x27;&#123;&quot;name&quot;:&quot;TOM&quot;,&quot;age&quot;:18&#125;&#x27;;&#125; 自定义服务日志Nginx中日志的类型分access.log、error.log。access.log:用来记录用户所有的访问请求。error.log:记录nginx本身运行时的错误信息，不会记录用户的访问请求。Nginx服务器支持对服务日志的格式、大小、输出等进行设置，需要使用到两个指令，分别是access_log和log_format指令。（1）access_log:用来设置用户访问日志的相关属性。 语法 access_log path[format[buffer&#x3D;size]] 默认值 access_log logs&#x2F;access.log combined; 位置 http, server, location （2）log_format:用来指定日志的输出格式。 语法 log_format name [escape&#x3D;default&#124;json&#124;none] string….; 默认值 log_format combined “…”; 位置 http 其他配置指令（1）sendfile:用来设置Nginx服务器是否使用sendfile()传输文件，该属性可以大大提高Nginx处理静态资源的性能 语法 sendfile on&#124;off； 默认值 sendfile off; 位置 http、server、location （2）keepalive_timeout:用来设置长连接的超时时间。》为什么要使用keepalive？ 12我们都知道HTTP是一种无状态协议，客户端向服务端发送一个TCP请求，服务端响应完毕后断开连接。如何客户端向服务端发送多个请求，每个请求都需要重新创建一次连接，效率相对来说比较多，使用keepalive模式，可以告诉服务器端在处理完一个请求后保持这个TCP连接的打开状态，若接收到来自这个客户端的其他请求，服务端就会利用这个未被关闭的连接，而不需要重新创建一个新连接，提升效率，但是这个连接也不能一直保持，这样的话，连接如果过多，也会是服务端的性能下降，这个时候就需要我们进行设置其的超时时间。 语法 keepalive_timeout time; 默认值 keepalive_timeout 75s; 位置 http、server、location （3）keepalive_requests:用来设置一个keep-alive连接使用的次数。 语法 keepalive_requests number; 默认值 keepalive_requests 100; 位置 http、server、location server块和location块 这里只是简单的使用 具体需要等到后面server块和location块都是我们要重点讲解和学习的内容，因为我们后面会对Nginx的功能进行详细讲解，所以这块内容就放到静态资源部署的地方给大家详细说明。本节我们主要来认识下Nginx默认给的nginx.conf中的相关内容，以及server块与location块在使用的时候需要注意的一些内容。 12345678910111213server &#123; listen 80; # 监听的是80端口 server_name localhost; location / &#123; # 当访问的资源是/ 进入这里 root html; index index.html index.htm; &#125; error_page 500 502 503 504 404 /50x.html; location = /50x.html &#123; root html; &#125;&#125; Nginx进阶篇Nginx服务器基础配置实例前面我们已经对Nginx服务器默认配置文件的结构和涉及的基本指令做了详细的阐述。通过这些指令的合理配置，我们就可以让一台Nginx服务器正常工作，并且提供基本的web服务器功能。接下来我们将通过一个比较完整和最简单的基础配置实例，来巩固下前面所学习的指令及其配置。需求如下: 1234567891011121314（1）有如下访问： http://192.168.200.133:8081/server1/location1 访问的是：index_sr1_location1.html http://192.168.200.133:8081/server1/location2 访问的是：index_sr1_location2.html http://192.168.200.133:8082/server2/location1 访问的是：index_sr2_location1.html http://192.168.200.133:8082/server2/location2 访问的是：index_sr2_location2.html（2）如果访问的资源不存在， 返回自定义的404页面（3）将/server1和/server2的配置使用不同的配置文件分割 将文件放到/home/www/conf.d目录下，然后使用include进行合并（4）为/server1和/server2各自创建一个访问日志文件 准备相关文件，目录如下： 配置的内容如下:(在nginx.conf中配置) 123456789101112131415161718192021222324252627282930313233343536373839404142##全局块 begin###配置允许运行Nginx工作进程的用户和用户组user www;#配置运行Nginx进程生成的worker进程数worker_processes 2;#配置Nginx服务器运行对错误日志存放的路径error_log logs/error.log;#配置Nginx服务器允许时记录Nginx的master进程的PID文件路径和名称pid logs/nginx.pid;#配置Nginx服务是否以守护进程方法启动#daemon on;##全局块 end####events块 begin##events&#123; #设置Nginx网络连接序列化 accept_mutex on; #设置Nginx的worker进程是否可以同时接收多个请求 multi_accept on; #设置Nginx的worker进程最大的连接数 worker_connections 1024; #设置Nginx使用的事件驱动模型 use epoll;&#125;##events块 end####http块 start##http&#123; #定义MIME-Type include mime.types; default_type application/octet-stream; #配置允许使用sendfile方式运输 sendfile on; #配置连接超时时间 keepalive_timeout 65; #配置请求处理日志格式 log_format server1 &#x27;===&gt;server1 access log&#x27;; log_format server2 &#x27;===&gt;server2 access log&#x27;; ##server块 开始 导入这个目录下的所有conf 也就是以下的server块 http中有server块来指定请求配置## include /home/www/conf.d/*.conf; ##server块 结束##&#125; ##http块 end## server1.conf 123456789101112131415161718192021222324server&#123; #配置监听端口和主机名称 listen 8081; server_name localhost; #配置请求处理日志存放路径 access_log /home/www/myweb/server1/logs/access.log server1; #配置错误页面 error_page 404 /404.html; #配置处理/server1/location1请求的location location /server1/location1&#123; root /home/www/myweb; index index_sr1_location1.html; &#125; #配置处理/server1/location2请求的location location /server1/location2&#123; root /home/www/myweb; index index_sr1_location2.html; &#125; #配置错误页面转向 location = /404.html &#123; root /home/www/myweb; index 404.html; &#125;&#125; server2.conf 123456789101112131415161718192021222324server&#123; #配置监听端口和主机名称 listen 8082; server_name localhost; #配置请求处理日志存放路径 access_log /home/www/myweb/server2/logs/access.log server2; #配置错误页面,对404.html做了定向配置 error_page 404 /404.html; #配置处理/server1/location1请求的location location /server2/location1&#123; root /home/www/myweb; index index_sr2_location1.html; &#125; #配置处理/server2/location2请求的location location /server2/location2&#123; root /home/www/myweb; index index_sr2_location2.html; &#125; #配置错误页面转向 location = /404.html &#123; root /home/www/myweb; index 404.html; &#125;&#125; Nginx服务操作的问题经过前面的操作，我们会发现，如果想要启动、关闭或重新加载nginx配置文件，都需要先进入到nginx的安装目录的sbin目录，然后使用nginx的二级制可执行文件来操作，相对来说操作比较繁琐，这块该如何优化？另外如果我们想把Nginx设置成随着服务器启动就自动完成启动操作，又该如何来实现?这就需要用到接下来我们要讲解的两个知识点： 12Nginx配置成系统服务Nginx命令配置到系统环境 Nginx配置成系统服务把Nginx应用服务设置成为系统服务，方便对Nginx服务的启动和停止等相关操作，具体实现步骤:(1) 在&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system目录下添加nginx.service,内容如下:vim &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;nginx.service 12345678910111213141516[Unit]Description=nginx web serviceDocumentation=http://nginx.org/en/docs/After=network.target[Service]Type=forkingPIDFile=/usr/local/nginx/logs/nginx.pid # 这里及以下都需要改成自己的ExecStartPre=/usr/local/nginx/sbin/nginx -t -c /usr/local/nginx/conf/nginx.confExecStart=/usr/local/nginx/sbin/nginxExecReload=/usr/local/nginx/sbin/nginx -s reloadExecStop=/usr/local/nginx/sbin/nginx -s stopPrivateTmp=true[Install]WantedBy=default.target (2)添加完成后如果权限有问题需要进行权限设置chmod 755 &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;nginx.service(3)使用系统命令来操作Nginx服务 123456启动: systemctl start nginx停止: systemctl stop nginx重启: systemctl restart nginx重新加载配置文件: systemctl reload nginx查看nginx状态: systemctl status nginx开机启动: systemctl enable nginx Nginx命令配置到系统环境前面我们介绍过Nginx安装目录下的二级制可执行文件nginx的很多命令，要想使用这些命令前提是需要进入sbin目录下才能使用，很不方便，如何去优化，我们可以将该二进制可执行文件加入到系统的环境变量，这样的话在任何目录都可以使用nginx对应的相关命令。具体实现步骤如下:演示可删除 123/usr/local/nginx/sbin/nginx -Vcd /usr/local/nginx/sbin nginx -V如何优化？？？ (1)修改&#x2F;etc&#x2F;profile文件 123vim /etc/profile在最后一行添加export PATH=$PATH:/usr/sbin (2)使之立即生效source &#x2F;etc&#x2F;profile(3)执行nginx命令nginx -V Nginx静态资源部署 ***Nginx静态资源概述上网去搜索访问资源对于我们来说并不陌生，通过浏览器发送一个HTTP请求实现从客户端发送请求到服务器端获取所需要内容后并把内容回显展示在页面的一个过程。这个时候，我们所请 求的内容就分为两种类型，一类是静态资源、一类是动态资源。静态资源即指在服务器端真实存在并且能直接拿来展示的一些文件，比如常见的html页面、css文件、js文件、图 片、视频等资源；动态资源即指在服务器端真实存在但是要想获取需要经过一定的业务逻辑处理，根据不同的条件展示在页面不同这 一部分内容，比如说报表数据展示、根据当前登录用户展示相关具体数据等资源；Nginx处理静态资源的内容，我们需要考虑下面这几个问题： 12345（1）静态资源的配置指令（2）静态资源的配置优化（3）静态资源的压缩配置指令（4）静态资源的缓存处理（5）静态资源的访问控制，包括跨域问题和防盗链问题 Nginx静态资源的配置指令listen指令listen:用来配置监听端口。 语法 listen address[:port] [default_server]…;listen port [default_server]…; 默认值 listen *:80 &#124; *:8000 位置 server listen的设置比较灵活，我们通过几个例子来把常用的设置方式熟悉下： 1234listen 127.0.0.1:8000; // listen localhost:8000 监听指定的IP和端口listen 127.0.0.1; 监听指定IP的所有端口listen 8000; 监听指定端口上的连接listen *:8000; 监听指定端口上的连接 default_server属性是标识符，用来将此虚拟主机设置成默认主机。所谓的默认主机指的是如果没有匹配到对应的address:port，则会默认执行的。如果不指定默认使用的是第一个server。 1234567891011121314server&#123; listen 8080; server_name 127.0.0.1; location /&#123; root html; index index.html; &#125;&#125;server&#123; listen 8080 default_server; server_name localhost; default_type text/plain; return 444 &#x27;This is a error request&#x27;;&#125; server_name指令server_name：用来设置虚拟主机服务名称。 可以直接写域名127.0.0.1 、 localhost 、域名[www.baidu.com | www.jd.com] 语法 server_name name …;name可以提供多个中间用空格分隔 默认值 server_name “”; 位置 server 关于server_name的配置方式有三种，分别是： 123精确匹配通配符匹配正则表达式匹配 配置方式一：精确匹配如： 12345server &#123; listen 80; server_name www.itcast.cn www.itheima.cn; ...&#125; 补充小知识点:hosts是一个没有扩展名的系统文件，可以用记事本等工具打开，其作用就是将一些常用的网址域名与其对应的IP地址建立一个关联“数据库”，当用户在浏览器中输入一个需要登录的网址时，系统会首先自动从hosts文件中寻找对应的IP地址，一旦找到，系统会立即打开对应网页，如果没有找到，则系统会再将网址提交DNS域名解析服务器进行IP地址的解析。windows:C:\\Windows\\System32\\drivers\\etccentos：&#x2F;etc&#x2F;hosts因为域名是要收取一定的费用，所以我们可以使用修改hosts文件来制作一些虚拟域名来使用。需要修改 &#x2F;etc&#x2F;hosts文件来添加 123vim /etc/hosts127.0.0.1 www.itcast.cn127.0.0.1 www.itheima.cn 配置方式二:使用通配符配置server_name中支持通配符”*”,但需要注意的是通配符不能出现在域名的中间，只能出现在首段或尾段，如： 123456server &#123; listen 80; server_name *.itcast.cn www.itheima.*; # www.itcast.cn abc.itcast.cn www.itheima.cn www.itheima.com ...&#125; 下面的配置就会报错 12345server &#123; listen 80; server_name www.*.cn www.itheima.c* ...&#125; 配置三:使用正则表达式配置server_name中可以使用正则表达式，并且使用~作为正则表达式字符串的开始标记。常见的正则表达式 代码 说明 ^ 匹配搜索字符串开始位置 $ 匹配搜索字符串结束位置 . 匹配除换行符\\n之外的任何单个字符 \\ 转义字符，将下一个字符标记为特殊字符 [xyz] 字符集，与任意一个指定字符匹配 [a-z] 字符范围，匹配指定范围内的任何字符 \\w 与以下任意字符匹配 A-Z a-z 0-9 和下划线,等效于[A-Za-z0-9_] \\d 数字字符匹配，等效于[0-9] {n} 正好匹配n次 {n,} 至少匹配n次 {n,m} 匹配至少n次至多m次 * 零次或多次，等效于{0,} + 一次或多次，等效于{1,} ? 零次或一次，等效于{0,1} 配置如下： 1234567server&#123; listen 80; server_name ~^www\\.(\\w+)\\.com$; default_type text/plain; return 200 $1 $2 ..;&#125;注意 ~后面不能加空格，括号可以取值 匹配执行顺序由于server_name指令支持通配符和正则表达式，因此在包含多个虚拟主机的配置文件中，可能会出现一个名称被多个虚拟主机的server_name匹配成功，当遇到这种情况，当前的请求交给谁来处理呢？ 12345678910111213141516171819202122232425262728293031323334server&#123; listen 80; server_name ~^www\\.\\w+\\.com$; default_type text/plain; return 200 &#x27;regex_success&#x27;;&#125;server&#123; listen 80; server_name www.itheima.*; default_type text/plain; return 200 &#x27;wildcard_after_success&#x27;;&#125;server&#123; listen 80; server_name *.itheima.com; default_type text/plain; return 200 &#x27;wildcard_before_success&#x27;;&#125;server&#123; listen 80; server_name www.itheima.com; default_type text/plain; return 200 &#x27;exact_success&#x27;;&#125;server&#123; listen 80 default_server; server_name _; default_type text/plain; return 444 &#x27;default_server not found server&#x27;;&#125; 结论： 12345exact_successwildcard_before_successwildcard_after_successregex_successdefault_server not found server!! 123456789No1:准确匹配server_nameNo2:通配符在开始时匹配server_name成功No3:通配符在结束时匹配server_name成功No4:正则表达式匹配server_name成功No5:被默认的default_server处理，如果没有指定默认找第一个server location指令1234567891011server&#123; listen 80; server_name localhost; location / &#123; &#125; location /abc&#123; &#125; ...&#125; location:用来设置请求的URI 语法 location [ &#x3D; &#124; ~ &#124; ~ &#124; ^~ &#124;@ ] uri{…}* 默认值 — 位置 server,location uri变量是待匹配的请求字符串，可以不包含正则表达式，也可以包含正则表达式，那么nginx服务器在搜索匹配location的时候，是先使用不包含正则表达式进行匹配，找到一个匹配度最高的一个，然后在通过包含正则表达式的进行匹配，如果能匹配到直接访问，匹配不到，就使用刚才匹配度最高的那个location来处理请求。属性介绍:不带符号，要求必须以指定模式开始 12345678910111213server &#123; listen 80; server_name 127.0.0.1; location /abc&#123; default_type text/plain; return 200 &quot;access success&quot;; &#125;&#125;以下访问都是正确的http://192.168.200.133/abchttp://192.168.200.133/abc?p1=TOMhttp://192.168.200.133/abc/http://192.168.200.133/abcdef &#x3D; : 用于不包含正则表达式的uri前，必须与指定的模式精确匹配 1234567891011121314server &#123; listen 80; server_name 127.0.0.1; location =/abc&#123; default_type text/plain; return 200 &quot;access success&quot;; &#125;&#125;可以匹配到http://192.168.200.133/abchttp://192.168.200.133/abc?p1=TOM匹配不到http://192.168.200.133/abc/http://192.168.200.133/abcdef ~ ： 用于表示当前uri中包含了正则表达式，并且区分大小写~*: 用于表示当前uri中包含了正则表达式，并且不区分大小写换句话说，如果uri包含了正则表达式，需要用上述两个符合来标识 12345678910111213141516server &#123; listen 80; server_name 127.0.0.1; location ~^/abc\\w$&#123; default_type text/plain; return 200 &quot;access success&quot;; &#125;&#125;server &#123; listen 80; server_name 127.0.0.1; location ~*^/abc\\w$&#123; default_type text/plain; return 200 &quot;access success&quot;; &#125;&#125; ^~: 用于不包含正则表达式的uri前，功能和不加符号的一致，唯一不同的是，如果模式匹配，那么就停止搜索其他模式了。 12345678server &#123; listen 80; server_name 127.0.0.1; location ^~/abc&#123; default_type text/plain; return 200 &quot;access success&quot;; &#125;&#125; 设置请求资源的目录root &#x2F; aliasroot：设置请求的根目录 语法 root path; 默认值 root html; 位置 http、server、location path为Nginx服务器接收到请求以后查找资源的根目录路径。alias：用来更改location的URI 语法 alias path; 默认值 — 位置 location path为修改后的根路径。以上两个指令都可以来指定访问资源的路径，那么这两者之间的区别是什么?举例说明：（1）在&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;html目录下创建一个 images目录,并在目录下放入一张图片mv.png图片 123location /images &#123; root /usr/local/nginx/html;&#125; 访问图片的路径为:http://192.168.200.133/images/mv.png（2）如果把root改为alias 123location /images &#123; alias /usr/local/nginx/html;&#125; 再次访问上述地址，页面会出现404的错误，查看错误日志会发现是因为地址不对，所以验证了： 1234root的处理结果是: root路径+location路径/usr/local/nginx/html/images/mv.pngalias的处理结果是:使用alias路径替换location路径/usr/local/nginx/html/images 需要在alias后面路径改为 123location /images &#123; alias /usr/local/nginx/html/images;&#125; （3）如果location路径是以&#x2F;结尾,则alias也必须是以&#x2F;结尾，root没有要求将上述配置修改为 123location /images/ &#123; alias /usr/local/nginx/html/images;&#125; 访问就会出问题，查看错误日志还是路径不对，所以需要把alias后面加上 &#x2F;小结： 1234root的处理结果是: root路径+location路径alias的处理结果是:使用alias路径替换location路径alias是一个目录别名的定义，root则是最上层目录的含义。如果location路径是以/结尾,则alias也必须是以/结尾，root没有要求 index指令index:设置网站的默认首页 语法 index file …; 默认值 index index.html; 位置 http、server、location index后面可以跟多个设置，如果访问的时候没有指定具体访问的资源，则会依次进行查找，找到第一个为止。举例说明： 12345location / &#123; root /usr/local/nginx/html; index index.html index.htm;&#125;访问该location的时候，可以通过 http://ip:port/，地址后面如果不添加任何内容，则默认依次访问index.html和index.htm，找到第一个来进行返回 error_page指令error_page:设置网站的错误页面 语法 error_page code … [&#x3D;[response]] uri; 默认值 — 位置 http、server、location…… 当出现对应的响应code后，如何来处理。举例说明：（1）可以指定具体跳转的地址 123server &#123; error_page 404 http://www.itcast.cn;&#125; （2）可以指定重定向地址 1234567server&#123; error_page 404 /50x.html; error_page 500 502 503 504 /50x.html; location =/50x.html&#123; root html; &#125;&#125; （3）使用location的@符合完成错误信息展示 1234567server&#123; error_page 404 @jump_to_error; location @jump_to_error &#123; default_type text/plain; return 404 &#x27;Not Found Page...&#x27;; &#125;&#125; 可选项&#x3D;[response]的作用是用来将相应代码更改为另外一个 1234567server&#123; error_page 404 =200 /50x.html; location =/50x.html&#123; root html; &#125;&#125;这样的话，当返回404找不到对应的资源的时候，在浏览器上可以看到，最终返回的状态码是200，这块需要注意下，编写error_page后面的内容，404后面需要加空格，200前面不能加空格 静态资源优化配置语法Nginx对静态资源如何进行优化配置。这里从三个属性配置进行优化： 123sendfile on;tcp_nopush on;tcp_nodeplay on; （1）sendﬁle，用来开启高效的文件传输模式。 语法 sendﬁle on &#124;oﬀ; 默认值 sendﬁle oﬀ; 位置 http、server、location… 请求静态资源的过程：客户端通过网络接口向服务端发送请求，操作系统将这些客户端的请求传递给服务器端应用程序，服务器端应用程序会处理这些请求，请求处理完成以后，操作系统还需要将处理得到的结果通过网络适配器传递回去。如： 12345678910server &#123; listen 80; server_name localhost； location / &#123; root html; index index.html; &#125;&#125;在html目录下有一个welcome.html页面，访问地址http://192.168.200.133/welcome.html 不使用sendfile的时候 需要进行多次拷贝 因为从磁盘 到缓冲区 在经由网卡发送 需要多次的拷贝使用了sendfile的时候 他是一个函数 指定从哪里获取 交给谁 少了很多的copy （2）tcp_nopush：该指令必须在sendfile打开的状态下才会生效，主要是用来提升网络包的传输’效率’ 语法 tcp_nopush on&#124;off; 默认值 tcp_nopush oﬀ; 位置 http、server、location （3）tcp_nodelay：该指令必须在keep-alive连接开启的情况下才生效，来提高网络包传输的’实时性’ 语法 tcp_nodelay on&#124;off; 默认值 tcp_nodelay on; 位置 http、server、location 经过刚才的分析，”tcp_nopush”和”tcp_nodelay“看起来是”互斥的”，那么为什么要将这两个值都打开，这个大家需要知道的是在linux2.5.9以后的版本中两者是可以兼容的，三个指令都开启的好处是，sendfile可以开启高效的文件传输模式，tcp_nopush开启可以确保在发送到客户端之前数据包已经充分“填满”， 这大大减少了网络开销，并加快了文件发送的速度。 然后，当它到达最后一个可能因为没有“填满”而暂停的数据包时，Nginx会忽略tcp_nopush参数， 然后，tcp_nodelay强制套接字发送数据。由此可知，TCP_NOPUSH可以与TCP_NODELAY一起设置，它比单独配置TCP_NODELAY具有更强的性能。所以我们可以使用如下配置来优化Nginx静态资源的处理 123sendfile on;tcp_nopush on;tcp_nodelay on; Nginx静态资源压缩实战经过上述内容的优化，我们再次思考一个问题，假如在满足上述优化的前提下，我们传送一个1M的数据和一个10M的数据那个效率高?，答案显而易见，传输内容小，速度就会快。那么问题又来了，同样的内容，如果把大小降下来，我们脑袋里面要蹦出一个词就是”压缩”，接下来，我们来学习Nginx的静态资源压缩模块。在Nginx的配置文件中可以通过配置gzip来对静态资源进行压缩，相关的指令可以配置在http块、server块和location块中，Nginx可以通过 123ngx_http_gzip_module模块ngx_http_gzip_static_module模块ngx_http_gunzip_module模块 对这些指令进行解析和处理。接下来我们从以下内容进行学习 1234（1）Gzip各模块支持的配置指令（2）Gzip压缩功能的配置（3）Gzip和sendfile的冲突解决（4）浏览器不支持Gzip的解决方案 Gzip模块配置指令接下来所学习的指令都来自ngx_http_gzip_module模块，该模块会在nginx安装的时候内置到nginx的安装环境中，也就是说我们可以直接使用这些指令。 gzip指令：该指令用于开启或者关闭gzip功能 语法 gzip on&#124;off; 默认值 gzip off; 位置 http、server、location… 注意只有该指令为打开状态，下面的指令才有效果 123http&#123; gzip on;&#125; gzip_types指令：该指令可以根据响应页的MIME类型选择性地开启Gzip压缩功能 语法 gzip_types mime-type …; 默认值 gzip_types text&#x2F;html; 位置 http、server、location 所选择的值可以从mime.types文件中进行查找，也可以使用”*”代表所有。 123http&#123; gzip_types application/javascript; # 这个类型的才会压缩 可指定位*&#125; gzip_comp_level指令：该指令用于设置Gzip压缩程度，级别从1-9,1表示要是程度最低，要是效率最高，9刚好相反，压缩程度最高，但是效率最低最费时间。 语法 gzip_comp_level level; 默认值 gzip_comp_level 1; 位置 http、server、location 123http&#123; gzip_comp_level 6; # &#125; gzip_vary指令：该指令用于设置使用Gzip进行压缩发送是否携带“Vary:Accept-Encoding”头域的响应头部。主要是告诉接收方，所发送的数据经过了Gzip压缩处理 语法 gzip_vary on&#124;off; 默认值 gzip_vary off; 位置 http、server、location gzip_buffers指令：该指令用于处理请求压缩的缓冲区数量和大小。 语法 gzip_buffers number size; 默认值 gzip_buffers 32 4k&#124;16 8k; 位置 http、server、location 其中number:指定Nginx服务器向系统申请缓存空间个数，size指的是每个缓存空间的大小。主要实现的是申请number个每个大小为size的内存空间。这个值的设定一般会和服务器的操作系统有关，所以建议此项不设置，使用默认值即可。gzip_buffers 4 16K; #缓存空间大小 gzip_disable指令：针对不同种类客户端发起的请求，可以选择性地开启和关闭Gzip功能。 语法 gzip_disable regex …; 默认值 — 位置 http、server、location regex:根据客户端的浏览器标志(user-agent)来设置，支持使用正则表达式。指定的浏览器标志不使用Gzip.该指令一般是用来排除一些明显不支持Gzip的浏览器。gzip_disable “MSIE [1-6].“; gzip_http_version指令：针对不同的HTTP协议版本，可以选择性地开启和关闭Gzip功能。 语法 gzip_http_version 1.0&#124;1.1; 默认值 gzip_http_version 1.1; 位置 http、server、location 该指令是指定使用Gzip的HTTP最低版本，该指令一般采用默认值即可。 gzip_min_length指令：该指令针对传输数据的大小，可以选择性地开启和关闭Gzip功能 语法 gzip_min_length length; 默认值 gzip_min_length 20; 位置 http、server、location 12nignx计量大小的单位：bytes[字节] / kb[千字节] / M[兆]例如: 1024 / 10k|K / 10m|M Gzip压缩功能对大数据的压缩效果明显，但是如果要压缩的数据比较小的化，可能出现越压缩数据量越大的情况，因此我们需要根据响应内容的大小来决定是否使用Gzip功能，响应页面的大小可以通过头信息中的Content-Length来获取。但是如何使用了Chunk编码动态压缩，该指令将被忽略。建议设置为1K或以上。 gzip_proxied指令：该指令设置是否对服务端返回的结果进行Gzip压缩。 语法 gzip_proxied off&#124;expired&#124;no-cache&#124;no-store&#124;private&#124;no_last_modified&#124;no_etag&#124;auth&#124;any; 默认值 gzip_proxied off; 位置 http、server、location off - 关闭Nginx服务器对后台服务器返回结果的Gzip压缩expired - 启用压缩，如果header头中包含 “Expires” 头信息no-cache - 启用压缩，如果header头中包含 “Cache-Control:no-cache” 头信息no-store - 启用压缩，如果header头中包含 “Cache-Control:no-store” 头信息private - 启用压缩，如果header头中包含 “Cache-Control:private” 头信息no_last_modified - 启用压缩,如果header头中不包含 “Last-Modified” 头信息no_etag - 启用压缩 ,如果header头中不包含 “ETag” 头信息auth - 启用压缩 , 如果header头中包含 “Authorization” 头信息any - 无条件启用压缩 Gzip压缩功能的实例配置123456789gzip on; #开启gzip功能gzip_types *; #压缩源文件类型,根据具体的访问资源类型设定gzip_comp_level 6; #gzip压缩级别gzip_min_length 1024; #进行压缩响应页面的最小长度,content-lengthgzip_buffers 4 16K; #缓存空间大小gzip_http_version 1.1; #指定压缩响应所需要的最低HTTP请求版本gzip_vary on; #往头信息中添加压缩标识gzip_disable &quot;MSIE [1-6]\\.&quot;; #对IE6以下的版本都不进行压缩gzip_proxied off； #nginx作为反向代理压缩服务端返回数据的条件 这些配置在很多地方可能都会用到，所以我们可以将这些内容抽取到一个配置文件中，然后通过include指令把配置文件再次加载到nginx.conf配置文件中，方法使用。nginx_gzip.conf 123456789gzip on;gzip_types *;gzip_comp_level 6;gzip_min_length 1024;gzip_buffers 4 16K;gzip_http_version 1.1;gzip_vary on;gzip_disable &quot;MSIE [1-6]\\.&quot;;gzip_proxied off; nginx.confinclude nginx_gzip.conf Gzip和sendfile共存问题前面在讲解sendfile的时候，提到过，开启sendfile以后，在读取磁盘上的静态资源文件的时候，可以减少拷贝的次数，可以不经过用户进程将静态文件通过网络设备发送出去，但是Gzip要想对资源压缩，是需要经过用户进程进行操作的。所以如何解决两个设置的共存问题。可以使用ngx_http_gzip_static_module模块的gzip_static指令来解决。 gzip_static指令gzip_static: 检查与访问资源同名的.gz文件时，response中以gzip相关的header返回.gz文件的内容。 语法 gzip_static on &#124; off &#124; always; 默认值 gzip_static off; 位置 http、server、location 添加上述命令后，会报一个错误，unknown directive “gzip_static”主要的原因是Nginx默认是没有添加ngx_http_gzip_static_module模块。如何来添加? 添加模块到Nginx的实现步骤(1)查询当前Nginx的配置参数nginx -V(2)将nginx安装目录下sbin目录中的nginx二进制文件进行更名 12cd /usr/local/nginx/sbinmv nginx nginxold (3) 进入Nginx的安装目录cd &#x2F;root&#x2F;nginx&#x2F;core&#x2F;nginx-1.16.1(4)执行make clean清空之前编译的内容make clean(5)使用configure来配置参数.&#x2F;configure –with-http_gzip_static_module(6)使用make命令进行编译make(7) 将objs目录下的nginx二进制执行文件移动到nginx安装目录下的sbin目录中mv objs&#x2F;nginx &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin(8)执行更新命令make upgrade gzip_static测试使用静态资源的缓存处理什么是缓存缓存（cache），原始意义是指访问速度比一般随机存取存储器（RAM）快的一种高速存储器，通常它不像系统主存那样使用DRAM技术，而使用昂贵但较快速的SRAM技术。缓存的设置是所有现代计算机系统发挥高性能的重要因素之一。 什么是web缓存Web缓存是指一个Web资源（如html页面，图片，js，数据等）存在于Web服务器和客户端（浏览器）之间的副本。缓存会根据进来的请求保存输出内容的副本；当下一个请求来到的时候，如果是相同的URL，缓存会根据缓存机制决定是直接使用副本响应访问请求，还是向源服务器再次发送请求。比较常见的就是浏览器会缓存访问过网站的网页，当再次访问这个URL地址的时候，如果网页没有更新，就不会再次下载网页，而是直接使用本地缓存的网页。只有当网站明确标识资源已经更新，浏览器才会再次下载网页 web缓存的种类1234客户端缓存 浏览器缓存服务端缓存 Nginx / Redis / Memcached等 浏览器缓存是为了节约网络的资源加速浏览，浏览器在用户磁盘上对最近请求过的文档进行存储，当访问者再次请求这个页面时，浏览器就可以从本地磁盘显示文档，这样就可以加速页面的阅览. 为什么要用浏览器缓存1234成本最低的一种缓存实现减少网络带宽消耗降低服务器压力减少网络延迟，加快页面打开速度 浏览器缓存的执行流程HTTP协议中和页面缓存相关的字段，我们先来认识下： header 说明 Expires 缓存过期的日期和时间 Cache-Control 设置和缓存相关的配置信息 Last-Modified 请求资源最后修改时间 ETag 请求变量的实体标签的当前值，比如文件的MD5值 （1）用户首次通过浏览器发送请求到服务端获取数据，客户端是没有对应的缓存，所以需要发送request请求来获取数据；（2）服务端接收到请求后，获取服务端的数据及服务端缓存的允许后，返回200的成功状态码并且在响应头上附上对应资源以及缓存信息；（3）当用户再次访问相同资源的时候，客户端会在浏览器的缓存目录中查找是否存在响应的缓存文件（4）如果没有找到对应的缓存文件，则走(2)步（5）如果有缓存文件，接下来对缓存文件是否过期进行判断，过期的判断标准是(Expires),（6）如果没有过期，则直接从本地缓存中返回数据进行展示（7）如果Expires过期，接下来需要判断缓存文件是否发生过变化（8）判断的标准有两个，一个是ETag(Entity Tag),一个是Last-Modified（9）判断结果是未发生变化，则服务端返回304，直接从缓存文件中获取数据（10）如果判断是发生了变化，重新从服务端获取数据，并根据缓存协商(服务端所设置的是否需要进行缓存数据的设置)来进行数据缓存。 浏览器缓存相关指令Nginx需要进行缓存相关设置，就需要用到如下的指令 expires指令expires:该指令用来控制页面缓存的作用。可以通过该指令控制HTTP应答中的“Expires”和”Cache-Control” 语法 expires [modified] timeexpires epoch&#124;max&#124;off; 默认值 expires off; 位置 http、server、location time:可以整数也可以是负数，指定过期时间，如果是负数，Cache-Control则为no-cache,如果为整数或0，则Cache-Control的值为max-age&#x3D;time;epoch: 指定Expires的值为’1 January,1970,00:00:01 GMT’(1970-01-01 00:00:00)，Cache-Control的值no-cachemax:指定Expires的值为’31 December2037 23:59:59GMT’ (2037-12-31 23:59:59) ，Cache-Control的值为10年off:默认不缓存。 add_header指令add_header指令是用来添加指定的响应头和响应值。 语法 add_header name value [always]; 默认值 — 位置 http、server、location… Cache-Control作为响应头信息，可以设置如下值：缓存响应指令： 123456789Cache-control: must-revalidateCache-control: no-cacheCache-control: no-storeCache-control: no-transformCache-control: publicCache-control: privateCache-control: proxy-revalidateCache-Control: max-age=&lt;seconds&gt;Cache-control: s-maxage=&lt;seconds&gt; 指令 说明 must-revalidate 可缓存但必须再向源服务器进行确认 no-cache 缓存前必须确认其有效性 no-store 不缓存请求或响应的任何内容 no-transform 代理不可更改媒体类型 public 可向任意方提供响应的缓存 private 仅向特定用户返回响应 proxy-revalidate 要求中间缓存服务器对缓存的响应有效性再进行确认 max-age&#x3D;&lt;秒&gt; 响应最大Age值 s-maxage&#x3D;&lt;秒&gt; 公共缓存服务器响应的最大Age值 max-age&#x3D;[秒]： Nginx的跨域问题解决这块内容，我们主要从以下方面进行解决： 123什么情况下会出现跨域问题?实例演示跨域问题具体的解决方案是什么? 同源策略浏览器的同源策略：是一种约定，是浏览器最核心也是最基本的安全功能，如果浏览器少了同源策略，则浏览器的正常功能可能都会受到影响。同源: 协议、域名(IP)、端口相同即为同源 1234567891011121314151617181920212223http://192.168.200.131/user/1https://192.168.200.131/user/1不http://192.168.200.131/user/1http://192.168.200.132/user/1不http://192.168.200.131/user/1http://192.168.200.131:8080/user/1不http://www.nginx.com/user/1http://www.nginx.org/user/1不http://192.168.200.131/user/1http://192.168.200.131:8080/user/1不http://www.nginx.org:80/user/1http://www.nginx.org/user/1满足 跨域问题简单描述下:有两台服务器分别为A,B,如果从服务器A的页面发送异步请求到服务器B获取数据，如果服务器A和服务器B不满足同源策略，则就会出现跨域问题。 解决方案使用add_header指令，该指令可以用来添加一些头信息 语法 add_header name value… 默认值 — 位置 http、server、location 此处用来解决跨域问题，需要添加两个头信息，一个是Access-Control-Allow-Origin,Access-Control-Allow-MethodsAccess-Control-Allow-Origin: 直译过来是允许跨域访问的源地址信息，可以配置多个(多个用逗号分隔)，也可以使用*代表所有源Access-Control-Allow-Methods:直译过来是允许跨域访问的请求方式，值可以为 GET POST PUT DELETE…,可以全部设置，也可以根据需要设置，多个用逗号分隔具体配置方式 123456location /getUser&#123; add_header Access-Control-Allow-Origin *; add_header Access-Control-Allow-Methods GET,POST,PUT,DELETE; default_type application/json; return 200 &#x27;&#123;&quot;id&quot;:1,&quot;name&quot;:&quot;TOM&quot;,&quot;age&quot;:18&#125;&#x27;;&#125; 跨域最好还是后端服务器处理 静态资源防盗链 保证自己的资源不被别人随意使用什么是资源盗链资源盗链指的是此内容不在自己服务器上，而是通过技术手段，绕过别人的限制将别人的内容放到自己页面上最终展示给用户。以此来盗取大网站的空间和流量。简而言之就是用别人的东西成就自己的网站。效果演示京东:https://img14.360buyimg.com/n7/jfs/t1/101062/37/2153/254169/5dcbd410E6d10ba22/4ddbd212be225fcd.jpg百度:https://pics7.baidu.com/feed/cf1b9d16fdfaaf516f7e2011a7cda1e8f11f7a1a.jpeg?token=551979a23a0995e5e5279b8fa1a48b34&s=BD385394D2E963072FD48543030030BB我们自己准备一个页面，在页面上引入这两个图片查看效果从上面的效果，可以看出来，下面的图片地址添加了防止盗链的功能，京东这边我们可以直接使用其图片。 Nginx防盗链的实现原理：了解防盗链的原理之前，我们得先学习一个HTTP的头信息Referer,当浏览器向web服务器发送请求的时候，一般都会带上Referer,来告诉浏览器该网页是从哪个页面链接过来的。后台服务器可以根据获取到的这个Referer信息来判断是否为自己信任的网站地址，如果是则放行继续访问，如果不是则可以返回403(服务端拒绝访问)的状态信息。在本地模拟上述的服务器效果：Nginx防盗链的具体实现:valid_referers:nginx会通就过查看referer自动和valid_referers后面的内容进行匹配，如果匹配到了就将$invalid_referer变量置0，如果没有匹配到，则将$invalid_referer变量置为1，匹配的过程中不区分大小写。 语法 valid_referers none&#124;blocked&#124;server_names&#124;string… 默认值 — 位置 server、location none: 如果Header中的Referer为空，允许访问blocked:在Header中的Referer不为空，但是该值被防火墙或代理进行伪装过，如不带”http:&#x2F;&#x2F;“ 、”https:&#x2F;&#x2F;“等协议头的资源允许访问。server_names:指定具体的域名或者IPstring: 可以支持正则表达式和*的字符串。如果是正则表达式，需要以~开头表示，例如 12345678location ~*\\.(png|jpg|gif)&#123; valid_referers none blocked www.baidu.com 192.168.200.222 *.example.com example.* www.example.org ~\\.google\\.; if ($invalid_referer)&#123; return 403; &#125; root /usr/local/nginx/html;&#125; 遇到的问题:图片有很多，该如何批量进行防盗链？ 针对目录进行防盗链配置如下： 12345678location /images &#123; valid_referers none blocked www.baidu.com 192.168.200.222 *.example.com example.* www.example.org ~\\.google\\.; if ($invalid_referer)&#123; return 403; &#125; root /usr/local/nginx/html;&#125; 这样我们可以对一个目录下的所有资源进行翻到了操作。遇到的问题：Referer的限制比较粗，比如随意加一个Referer，上面的方式是无法进行限制的。那么这个问题改如何解决？此处我们需要用到Nginx的第三方模块ngx_http_accesskey_module，第三方模块如何实现盗链，如果在Nginx中使用第三方模块的功能，这些我们在后面的Nginx的模块篇再进行详细的讲解。 Rewrite功能配置Rewrite是Nginx服务器提供的一个重要基本功能，是Web服务器产品中几乎必备的功能。主要的作用是用来实现URL的重写。 重写url 可以实现重定向注意:Nginx服务器的Rewrite功能的实现依赖于PCRE的支持，因此在编译安装Nginx服务器之前，需要安装PCRE库。Nginx使用的是ngx_http_rewrite_module模块来解析和处理Rewrite功能的相关配置。 “地址重写”与”地址转发”重写和转发的区别: 12345地址重写浏览器地址会发生变化而地址转发则不变一次地址重写会产生两次请求而一次地址转发只会产生一次请求地址重写到的页面必须是一个完整的路径而地址转发则不需要地址重写因为是两次请求所以request范围内属性不能传递给新页面而地址转发因为是一次请求所以可以传递值地址转发速度快于地址重写 Rewrite规则set指令该指令用来设置一个新的变量。 语法 set $variable value; 默认值 — 位置 server、location、if variable:变量的名称，该变量名称要用”$”作为变量的第一个字符，且不能与Nginx服务器预设的全局变量同名。value:变量的值，可以是字符串、其他变量或者变量的组合等。 Rewrite常用全局变量 变量 说明 $args 变量中存放了请求URL中的请求指令。比如http://192.168.200.133:8080?arg1=value1&args2=value2 中的”arg1&#x3D;value1&amp;arg2&#x3D;value2”，功能和$query_string一样 $http_user_agent 变量存储的是用户访问服务的代理信息(如果通过浏览器访问，记录的是浏览器的相关版本信息) $host 变量存储的是访问服务器的server_name值 $document_uri 变量存储的是当前访问地址的URI。比如http://192.168.200.133/server?id=10&name=zhangsan 中的”&#x2F;server”，功能和$uri一样 $document_root 变量存储的是当前请求对应location的root值，如果未设置，默认指向Nginx自带html目录所在位置 $content_length 变量存储的是请求头中的Content-Length的值 $content_type 变量存储的是请求头中的Content-Type的值 $http_cookie 变量存储的是客户端的cookie信息，可以通过add_header Set-Cookie ‘cookieName&#x3D;cookieValue’来添加cookie数据 $limit_rate 变量中存储的是Nginx服务器对网络连接速率的限制，也就是Nginx配置中对limit_rate指令设置的值，默认是0，不限制。 $remote_addr 变量中存储的是客户端的IP地址 $remote_port 变量中存储了客户端与服务端建立连接的端口号 $remote_user 变量中存储了客户端的用户名，需要有认证模块才能获取 $scheme 变量中存储了访问协议 $server_addr 变量中存储了服务端的地址 $server_name 变量中存储了客户端请求到达的服务器的名称 $server_port 变量中存储了客户端请求到达服务器的端口号 $server_protocol 变量中存储了客户端请求协议的版本，比如”HTTP&#x2F;1.1” $request_body_file 变量中存储了发给后端服务器的本地文件资源的名称 $request_method 变量中存储了客户端的请求方式，比如”GET”,”POST”等 $request_filename 变量中存储了当前请求的资源文件的路径名 $request_uri 变量中存储了当前请求的URI，并且携带请求参数，比如http://192.168.200.133/server?id=10&name=zhangsan 中的”&#x2F;server?id&#x3D;10&amp;name&#x3D;zhangsan” if指令该指令用来支持条件判断，并根据条件判断结果选择不同的Nginx配置。 语法 if (condition){…} 默认值 — 位置 server、location condition为判定条件，可以支持以下写法： 变量名。如果变量名对应的值为空或者是0，if都判断为false,其他条件为true。 123if ($param)&#123; &#125; 使用”&#x3D;”和”!&#x3D;”比较变量和字符串是否相等，满足条件为true，不满足为false 123if ($request_method = POST)&#123; return 405;&#125; 注意：此处和Java不太一样的地方是字符串不需要添加引号。 使用正则表达式对变量进行匹配，匹配成功返回true，否则返回false。变量与正则表达式之间使用”“,”“,”!“,”!“来连接。”“代表匹配正则表达式过程中区分大小写，”“代表匹配正则表达式过程中不区分大小写”!“和”!“刚好和上面取相反值，如果匹配上返回false,匹配不上返回true 123if ($http_user_agent ~ MSIE)&#123; #$http_user_agent的值中是否包含MSIE字符串，如果包含返回true&#125; 注意：正则表达式字符串一般不需要加引号，但是如果字符串中包含”}”或者是”;”等字符时，就需要把引号加上。 判断请求的文件是否存在使用”-f”和”!-f”,当使用”-f”时，如果请求的文件存在返回true，不存在返回false。当使用”!f”时，如果请求文件不存在，但该文件所在目录存在返回true,文件和目录都不存在返回false,如果文件存在返回false 123456if (-f $request_filename)&#123; #判断请求的文件是否存在&#125;if (!-f $request_filename)&#123; #判断请求的文件是否不存在&#125; 判断请求的目录是否存在使用”-d”和”!-d”,当使用”-d”时，如果请求的目录存在，if返回true，如果目录不存在则返回false当使用”!-d”时，如果请求的目录不存在但该目录的上级目录存在则返回true，该目录和它上级目录都不存在则返回false,如果请求目录存在也返回false. 判断请求的目录或者文件是否存在使用”-e”和”!-e”当使用”-e”,如果请求的目录或者文件存在时，if返回true,否则返回false.当使用”!-e”,如果请求的文件和文件所在路径上的目录都不存在返回true,否则返回false 判断请求的文件是否可执行使用”-x”和”!-x”当使用”-x”,如果请求的文件可执行，if返回true,否则返回false当使用”!-x”,如果请求文件不可执行，返回true,否则返回false break指令该指令用于中断当前相同作用域中的其他Nginx配置。与该指令处于同一作用域的Nginx配置中，位于它前面的指令配置生效，位于后面的指令配置无效。 语法 break; 默认值 — 位置 server、location、if 例子: 1234567location /&#123; if ($param)&#123; set $id $1; break; limit_rate 10k; &#125;&#125; return指令该指令用于完成对请求的处理，直接向客户端返回响应状态代码。在return后的所有Nginx配置都是无效的。 语法 return code [text];return code URL;return URL; 默认值 — 位置 server、location、if code:为返回给客户端的HTTP状态代理。可以返回的状态代码为0~999的任意HTTP状态代理text:为返回给客户端的响应体内容，支持变量的使用URL:为返回给客户端的URL地址 rewrite指令该指令通过正则表达式的使用来改变URI。可以同时存在一个或者多个指令，按照顺序依次对URL进行匹配和处理。URL和URI的区别： 12URI:统一资源标识符URL:统一资源定位符 语法 rewrite regex replacement [flag]; 默认值 — 位置 server、location、if regex:用来匹配URI的正则表达式replacement:匹配成功后，用于替换URI中被截取内容的字符串。如果该字符串是以”http:&#x2F;&#x2F;“或者”https:&#x2F;&#x2F;“开头的，则不会继续向下对URI进行其他处理，而是直接返回重写后的URI给客户端。flag:用来设置rewrite对URI的处理行为，可选值有如下： last: break redirect permanent rewrite_log指令该指令配置是否开启URL重写日志的输出功能。 语法 rewrite_log on&#124;off; 默认值 rewrite_log off; 位置 http、server、location、if 开启后，URL重写的相关日志将以notice级别输出到error_log指令配置的日志文件汇总。 Rewrite的案例域名跳转 问题分析 先来看一个效果，如果我们想访问京东网站，大家都知道我们可以输入www.jd.com,但是同样的我们也可以输入www.360buy.com同样也都能访问到京东网站。这个其实是因为京东刚开始的时候域名就是[www.360buy.com](https://www.360buy.com)，后面由于各种原因把自己的域名换成了[www.jd.com](https://www.jd.com), 虽然说域名变量，但是对于以前只记住了www.360buy.com的用户来说，我们如何把这部分用户也迁移到我们新域名的访问上来，针对于这个问题，我们就可以使用Nginx中Rewrite的域名跳转来解决。 环境准备 准备两个域名 www.360buy.com | www.jd.com vim &#x2F;etc&#x2F;hosts 12192.168.200.133 www.360buy.com192.168.200.133 www.jd.com 在&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;html&#x2F;hm目录下创建一个访问页面 123456&lt;html&gt; &lt;title&gt;&lt;/title&gt; &lt;body&gt; &lt;h1&gt;欢迎来到我们的网站&lt;/h1&gt; &lt;/body&gt;&lt;/html&gt; 通过Nginx实现当访问www.访问到系统的首页 12345678server &#123; listen 80; server_name www.hm.com; location /&#123; root /usr/local/nginx/html/hm; index index.html; &#125;&#125; 》通过Rewrite完成将www.360buy.com的请求跳转到www.jd.com 12345server &#123; listen 80; server_name www.360buy.com; rewrite ^/ http://www.jd.com permanent;&#125; 问题描述:如何在域名跳转的过程中携带请求的URI？修改配置信息 12345server &#123; listen 80; server_name www.itheima.com; rewrite ^(.*) http://www.hm.com$1 permanent;&#125; 问题描述:我们除了上述说的www.jd.com 、www.360buy.com其实还有我们也可以通过www.jingdong.com来访问，那么如何通过Rewrite来实现多个域名的跳转?添加域名 12vim /etc/hosts192.168.200.133 www.jingdong.com 修改配置信息 12345server&#123; listen 80; server_name www.360buy.com www.jingdong.com; rewrite ^(.*) http://www.jd.com$1 permanent;&#125; 域名镜像上述案例中，将www.360buy.com 和 www.jingdong.com都能跳转到www.jd.com，那么www.jd.com我们就可以把它起名叫主域名，其他两个就是我们所说的镜像域名，当然如果我们不想把整个网站做镜像，只想为其中某一个子目录下的资源做镜像，我们可以在location块中配置rewrite功能，这样在好处是 一个域名挂了 剩下的还可以顶替他 高可用性 比如: 12345678910server &#123; listen 80; server_name rewrite.myweb.com; location ^~ /source1&#123; rewrite ^/resource1(.*) http://rewrite.myweb.com/web$1 last; &#125; location ^~ /source2&#123; rewrite ^/resource2(.*) http://rewrite.myweb.com/web$1 last; &#125;&#125; 独立域名一个完整的项目包含多个模块，比如购物网站有商品商品搜索模块、商品详情模块已经购物车模块等，那么我们如何为每一个模块设置独立的域名。需求： 123http://search.hm.com 访问商品搜索模块http://item.hm.com 访问商品详情模块http://cart.hm.com 访问商品购物车模块 123456789101112131415server&#123; listen 80; server_name search.hm.com; rewrite ^(.*) http://www.hm.com/bbs$1 last;&#125;server&#123; listen 81; server_name item.hm.com; rewrite ^(.*) http://www.hm.com/item$1 last;&#125;server&#123; listen 82; server_name cart.hm.com; rewrite ^(.*) http://www.hm.com/cart$1 last;&#125; 目录自动添加”&#x2F;“问题描述通过一个例子来演示下问题: 12345678server &#123; listen 80; server_name localhost; location / &#123; root html; index index.html; &#125;&#125; 要想访问上述资源，很简单，只需要通过http://192.168.200.133直接就能访问，地址后面不需要加&#x2F;,但是如果将上述的配置修改为如下内容: 12345678server &#123; listen 80; server_name localhost; location /hm &#123; root html; index index.html; &#125;&#125; 这个时候，要想访问上述资源，按照上述的访问方式，我们可以通过http://192.168.200.133/hm/来访问,但是如果地址后面不加斜杠，页面就会出问题。如果不加斜杠，Nginx服务器内部会自动做一个301的重定向，重定向的地址会有一个指令叫server_name_in_redirect on|off;来决定重定向的地址： 1234如果该指令为on 重定向的地址为: http://server_name/目录名/;如果该指令为off 重定向的地址为: http://原URL中的域名/目录名/; 所以就拿刚才的地址来说，http://192.168.200.133/hm如果不加斜杠，那么按照上述规则，如果指令server_name_in_redirect为on，则301重定向地址变为 http://localhost/hm/,如果为off，则301重定向地址变为http://192.168.200.133/ht/。后面这个是正常的，前面地址就有问题。注意server_name_in_redirect指令在Nginx的0.8.48版本之前默认都是on，之后改成了off,所以现在我们这个版本不需要考虑这个问题，但是如果是0.8.48以前的版本并且server_name_in_redirect设置为on，我们如何通过rewrite来解决这个问题？解决方案我们可以使用rewrite功能为末尾没有斜杠的URL自动添加一个斜杠 12345678910server &#123; listen 80; server_name localhost; server_name_in_redirect on; location /hm &#123; if (-d $request_filename)&#123; rewrite ^/(.*)([^/])$ http://$host/$1$2/ permanent; &#125; &#125;&#125; 合并目录搜索引擎优化(SEO)是一种利用搜索引擎的搜索规则来提供目的网站的有关搜索引擎内排名的方式。我们在创建自己的站点时，可以通过很多中方式来有效的提供搜索引擎优化的程度。其中有一项就包含URL的目录层级一般不要超过三层，否则的话不利于搜索引擎的搜索也给客户端的输入带来了负担，但是将所有的文件放在一个目录下又会导致文件资源管理混乱并且访问文件的速度也会随着文件增多而慢下来，这两个问题是相互矛盾的，那么使用rewrite如何解决上述问题?举例，网站中有一个资源文件的访问路径时 &#x2F;server&#x2F;11&#x2F;22&#x2F;33&#x2F;44&#x2F;20.html,也就是说20.html存在于第5级目录下，如果想要访问该资源文件，客户端的URL地址就要写成 http://www.web.name/server/11/22/33/44/20.html, 1234567server &#123; listen 80; server_name www.web.name; location /server&#123; root html; &#125;&#125; 但是这个是非常不利于SEO搜索引擎优化的，同时客户端也不好记.使用rewrite我们可以进行如下配置: 1234567server &#123; listen 80; server_name www.web.name; location /server&#123; rewrite ^/server-([0-9]+)-([0-9]+)-([0-9]+)-([0-9]+)\\.html$ /server/$1/$2/$3/$4/$5.html last; &#125;&#125; 这样的花，客户端只需要输入http://www.web.name/server-11-22-33-44-20.html就可以访问到20.html页面了。这里也充分利用了rewrite指令支持正则表达式的特性。 防盗链防盗链之前我们已经介绍过了相关的知识，在rewrite中的防盗链和之前将的原理其实都是一样的，只不过通过rewrite可以将防盗链的功能进行完善下，当出现防盗链的情况，我们可以使用rewrite将请求转发到自定义的一张图片和页面，给用户比较好的提示信息。下面我们就通过根据文件类型实现防盗链的一个配置实例: 12345678910server&#123; listen 80; server_name www.web.com; locatin ~* ^.+\\.(gif|jpg|png|swf|flv|rar|zip)$&#123; valid_referers none blocked server_names *.web.com; if ($invalid_referer)&#123; rewrite ^/ http://www.web.com/images/forbidden.png; &#125; &#125;&#125; 根据目录实现防盗链配置： 1234567891011server&#123; listen 80; server_name www.web.com; location /file/&#123; root /server/file/; valid_referers none blocked server_names *.web.com; if ($invalid_referer)&#123; rewrite ^/ http://www.web.com/images/forbidden.png; &#125; &#125;&#125; Nginx反向代理Nginx反向代理概述关于正向代理和反向代理，我们在前面的章节已经通过一张图给大家详细的介绍过了，简而言之就是正向代理代理的对象是客户端，反向代理代理的是服务端，这是两者之间最大的区别。Nginx即可以实现正向代理，也可以实现反向代理。我们先来通？过一个小案例演示下Nginx正向代理的简单应用。先提需求：(1)服务端的设置： 123456789101112http &#123; log_format main &#x27;client send request=&gt;clientIp=$remote_addr serverIp=&gt;$host&#x27;; server&#123; listen 80; server_name localhost; access_log logs/access.log main; location &#123; root html; index index.html index.htm; &#125; &#125;&#125; (2)使用客户端访问服务端，打开日志查看结果 (3)代理服务器设置： 12345678server &#123; listen 82; resolver 8.8.8.8; location /&#123; proxy_pass http://$host$request_uri; &#125; &#125; (4)查看代理服务器的IP(192.168.200.146)和Nginx配置监听的端口(82)(5)在客户端配置代理服务器(6)设置完成后，再次通过浏览器访问服务端 通过对比，上下两次的日志记录，会发现虽然我们是客户端访问服务端，但是如何使用了代理，那么服务端能看到的只是代理发送过去的请求，这样的化，就使用Nginx实现了正向代理的设置。但是Nginx正向代理，在实际的应用中不是特别多，所以我们简单了解下，接下来我们继续学习Nginx的反向代理，这是Nginx比较重要的一个功能。 Nginx反向代理的配置语法 ***首先我们需要明确 当有多个服务器 nginx代理这些服务器 请求发给nginx 他在平摊给不同的服务器 这涉及到了负载均衡 反向代理经常和负载均衡联用 Nginx反向代理模块的指令是由ngx_http_proxy_module模块进行解析，该模块在安装Nginx的时候已经自己加装到Nginx中了，接下来我们把反向代理中的常用指令一一介绍下： 123proxy_passproxy_set_headerproxy_redirect proxy_pass该指令用来设置被代理服务器地址，可以是主机名称、IP地址加端口号形式。 语法 proxy_pass URL; 默认值 — 位置 location URL:为要设置的被代理服务器地址，包含传输协议(http,https://)、主机名称或IP地址加端口号、URI等要素。举例： 123456proxy_pass http://www.baidu.com;location /server&#123;&#125;proxy_pass http://192.168.200.146; http://192.168.200.146/server/index.htmlproxy_pass http://192.168.200.146/; http://192.168.200.146/index.html 大家在编写proxy_pass的时候，后面的值要不要加”&#x2F;“?接下来通过例子来说明刚才我们提到的问题： 1234567891011121314151617181920server &#123; listen 80; server_name localhost; location /&#123; #proxy_pass http://192.168.200.146; proxy_pass http://192.168.200.146/; &#125;&#125;当客户端访问 http://localhost/index.html,效果是一样的server&#123; listen 80; server_name localhost; location /server&#123; #proxy_pass http://192.168.200.146; proxy_pass http://192.168.200.146/; &#125;&#125;当客户端访问 http://localhost/server/index.html这个时候，第一个proxy_pass就变成了http://localhost/server/index.html第二个proxy_pass就变成了http://localhost/index.html效果就不一样了。 proxy_set_header**该指令可以更改Nginx服务器接收到的客户端请求的请求头信息，然后将新的请求头发送给代理的服务器 **说白了 可以对原本的请求头信息 进行操作 语法 proxy_set_header field value; 默认值 proxy_set_header Host $proxy_host; proxy_set_header Connection close; 位置 http、server、location 需要注意的是，如果想要看到结果，必须在被代理的服务器上来获取添加的头信息。被代理服务器： [192.168.200.146] 123456server &#123; listen 8080; server_name localhost; default_type text/plain; return 200 $http_username;&#125; 代理服务器: [192.168.200.133] 12345678server &#123; listen 8080; server_name localhost; location /server &#123; proxy_pass http://192.168.200.146:8080/; proxy_set_header username TOM; &#125; &#125; 访问测试 proxy_redirect该指令是用来重置头信息中的”Location”和”Refresh”的值。 | 语法 | proxy_redirect redirect replacement;proxy_redirect default; proxy_redirect off; || — | — || 默认值 | proxy_redirect default; || 位置 | http、server、location | 为什么要用该指令? 服务端[192.168.200.146] 1234567server &#123; listen 8081; server_name localhost; if (!-f $request_filename)&#123; return 302 http://192.168.200.146; &#125;&#125; 代理服务端[192.168.200.133] 12345678server &#123; listen 8081; server_name localhost; location / &#123; proxy_pass http://192.168.200.146:8081/; proxy_redirect http://192.168.200.146 http://192.168.200.133; &#125;&#125; 该指令的几组选项 proxy_redirect redirect replacement; 12redirect:目标,Location的值replacement:要替换的值 proxy_redirect default; 123default;将location块的uri变量作为replacement,将proxy_pass变量作为redirect进行替换 proxy_redirect off; 1关闭proxy_redirect的功能 Nginx反向代理实战***代理服务器端 服务器1,2,3存在两种情况 12第一种情况: 三台服务器的内容不一样。第二种情况: 三台服务器的内容是一样。 如果服务器1、服务器2和服务器3的内容不一样，那我们可以根据用户请求来分发到不同的服务器。 12345678910111213141516171819202122232425262728293031323334353637代理服务器server &#123; listen 8082; server_name localhost; location /server1 &#123; proxy_pass http://192.168.200.146:9001/; &#125; location /server2 &#123; proxy_pass http://192.168.200.146:9002/; &#125; location /server3 &#123; proxy_pass http://192.168.200.146:9003/; &#125;&#125;服务端server1server &#123; listen 9001; server_name localhost; default_type text/html; return 200 &#x27;&lt;h1&gt;192.168.200.146:9001&lt;/h1&gt;&#x27;&#125;server2server &#123; listen 9002; server_name localhost; default_type text/html; return 200 &#x27;&lt;h1&gt;192.168.200.146:9002&lt;/h1&gt;&#x27;&#125;server3server &#123; listen 9003; server_name localhost; default_type text/html; return 200 &#x27;&lt;h1&gt;192.168.200.146:9003&lt;/h1&gt;&#x27;&#125; 如果服务器1、服务器2和服务器3的内容是一样的，该如何处理? 这就需要用到负载均衡了 服务器分摊这些请求 Nginx的安全控制关于web服务器的安全是比较大的一个话题，里面所涉及的内容很多，Nginx反向代理是如何来提升web服务器的安全呢？ 1安全隔离 什么是安全隔离?通过代理分开了客户端到应用程序服务器端的连接，实现了安全措施。在反向代理之前设置防火墙，仅留一个入口供代理服务器访问。 一模一样 就是在所有服务器的访问前 让请求先去nginx在到具体的服务器 如何使用SSL对流量进行加密 https翻译成大家能熟悉的说法就是将我们常用的http请求转变成https请求，那么这两个之间的区别简单的来说两个都是HTTP协议，只不过https是身披SSL外壳的http.HTTPS是一种通过计算机网络进行安全通信的传输协议。它经由HTTP进行通信，利用SSL&#x2F;TLS建立全通信，加密数据包，确保数据的安全性。SSL(Secure Sockets Layer)安全套接层TLS(Transport Layer Security)传输层安全上述这两个是为网络通信提供安全及数据完整性的一种安全协议，TLS和SSL在传输层和应用层对网络连接进行加密。总结来说为什么要使用https: 1http协议是明文传输数据，存在安全问题，而https是加密传输，相当于http+ssl，并且可以防止流量劫持。 Nginx要想使用SSL，需要满足一个条件即需要添加一个模块--with-http_ssl_module,而该模块在编译的过程中又需要OpenSSL的支持，这个我们之前已经准备好了。 流量劫持（DNS劫持）请求发送了 被中途截取了 把这个请求发送给他指定的web服务器 然后返回这个web服务器的结果 这样安全隐患很大nginx添加SSL的支持（1）完成 --with-http_ssl_module模块的增量添加 123456》将原有/usr/local/nginx/sbin/nginx进行备份》拷贝nginx之前的配置信息》在nginx的安装源码进行配置指定对应模块 ./configure --with-http_ssl_module》通过make模板进行编译》将objs下面的nginx移动到/usr/local/nginx/sbin下》在源码目录下执行 make upgrade进行升级，这个可以实现不停机添加新模块的功能 Nginx的SSL相关指令因为刚才我们介绍过该模块的指令都是通过ngx_http_ssl_module模块来解析的。 ssl:该指令用来在指定的服务器开启HTTPS,可以使用 listen 443 ssl,后面这种方式更通用些。 语法 ssl on &#124; off; 默认值 ssl off; 位置 http、server 123 server&#123; listen 443 ssl;&#125; ssl_certificate:为当前这个虚拟主机指定一个带有PEM格式证书的证书。 语法 ssl_certificate file; 默认值 — 位置 http、server ssl_certificate_key:该指令用来指定PEM secret key文件的路径 语法 ssl_ceritificate_key file; 默认值 — 位置 http、server ssl_session_cache:该指令用来配置用于SSL会话的缓存 语法 ssl_sesion_cache off&#124;none&#124;[builtin[:size]] [shared:name:size] 默认值 ssl_session_cache none; 位置 http、server off:禁用会话缓存，客户端不得重复使用会话none:禁止使用会话缓存，客户端可以重复使用，但是并没有在缓存中存储会话参数builtin:内置OpenSSL缓存，仅在一个工作进程中使用。shared:所有工作进程之间共享缓存，缓存的相关信息用name和size来指定 ssl_session_timeout：开启SSL会话功能后，设置客户端能够反复使用储存在缓存中的会话参数时间。 语法 ssl_session_timeout time; 默认值 ssl_session_timeout 5m; 位置 http、server ssl_ciphers:指出允许的密码，密码指定为OpenSSL支持的格式 语法 ssl_ciphers ciphers; 默认值 ssl_ciphers HIGH:!aNULL:!MD5; 位置 http、server 可以使用openssl ciphers查看openssl支持的格式。 ssl_prefer_server_ciphers：该指令指定是否服务器密码优先客户端密码 语法 ssl_perfer_server_ciphers on&#124;off; 默认值 ssl_perfer_server_ciphers off; 位置 http、server 生成证书方式一：使用阿里云&#x2F;腾讯云等第三方服务进行购买。 当然是使用这种方式二:使用openssl生成证书先要确认当前系统是否有安装openssl 1openssl version 安装下面的命令进行生成 1234567mkdir /root/certcd /root/certopenssl genrsa -des3 -out server.key 1024openssl req -new -key server.key -out server.csrcp server.key server.key.orgopenssl rsa -in server.key.org -out server.keyopenssl x509 -req -days 365 -in server.csr -signkey server.key -out server.crt 开启SSL实例123456789101112131415161718server &#123; listen 443 ssl; server_name localhost; ssl_certificate server.cert; ssl_certificate_key server.key; ssl_session_cache shared:SSL:1m; ssl_session_timeout 5m; ssl_ciphers HIGH:!aNULL:!MD5; ssl_prefer_server_ciphers on; location / &#123; root html; index index.html index.htm; &#125;&#125; （4）验证 反向代理系统调优反向代理值Buffer和CacheBuffer翻译过来是”缓冲”，Cache翻译过来是”缓存”。 总结下： 12345相同点:两种方式都是用来提供IO吞吐效率，都是用来提升Nginx代理的性能。不同点:缓冲主要用来解决不同设备之间数据传递速度不一致导致的性能低的问题，缓冲中的数据一旦此次操作完成后，就可以删除。缓存主要是备份，将被代理服务器的数据缓存一份到代理服务器，这样的话，客户端再次获取相同数据的时候，就只需要从代理服务器上获取，效率较高，缓存中的数据可以重复使用，只有满足特定条件才会删除. （1）Proxy Buffer相关指令 proxy_buffering :该指令用来开启或者关闭代理服务器的缓冲区； 语法 proxy_buffering on&#124;off; 默认值 proxy_buffering on; 位置 http、server、location proxy_buffers:该指令用来指定单个连接从代理服务器读取响应的缓存区的个数和大小。 语法 proxy_buffers number size; 默认值 proxy_buffers 8 4k &#124; 8K;(与系统平台有关) 位置 http、server、location number:缓冲区的个数size:每个缓冲区的大小，缓冲区的总大小就是number*size proxy_buffer_size:该指令用来设置从被代理服务器获取的第一部分响应数据的大小。保持与proxy_buffers中的size一致即可，当然也可以更小。 语法 proxy_buffer_size size; 默认值 proxy_buffer_size 4k &#124; 8k;(与系统平台有关) 位置 http、server、location proxy_busy_buffers_size：该指令用来限制同时处于BUSY状态的缓冲总大小。 语法 proxy_busy_buffers_size size; 默认值 proxy_busy_buffers_size 8k&#124;16K; 位置 http、server、location proxy_temp_path:当缓冲区存满后，仍未被Nginx服务器完全接受，响应数据就会被临时存放在磁盘文件上，该指令设置文件路径 语法 proxy_temp_path path; 默认值 proxy_temp_path proxy_temp; 位置 http、server、location 注意path最多设置三层。 proxy_temp_file_write_size：该指令用来设置磁盘上缓冲文件的大小。 语法 proxy_temp_file_write_size size; 默认值 proxy_temp_file_write_size 8K&#124;16K; 位置 http、server、location 通用网站的配置 1234proxy_buffering on;proxy_buffer_size 4 32k;proxy_busy_buffers_size 64k;proxy_temp_file_write_size 64k; 根据项目的具体内容进行相应的调节。 Nginx负载均衡负载均衡概述早期的网站流量和业务功能都比较简单，单台服务器足以满足基本的需求，但是随着互联网的发展，业务流量越来越大并且业务逻辑也跟着越来越复杂，单台服务器的性能及单点故障问题就凸显出来了，因此需要多台服务器进行性能的水平扩展及避免单点故障出现。那么如何将不同用户的请求流量分发到不同的服务器上呢？ 负载均衡的原理及处理流程系统的扩展可以分为纵向扩展和横向扩展。纵向扩展是从单机的角度出发，通过增加系统的硬件处理能力来提升服务器的处理能力横向扩展是通过添加机器来满足大型网站服务的处理能力。 一般来说都是横向扩展以下是 横向扩展 这里面涉及到两个重要的角色分别是”应用集群”和”负载均衡器”。应用集群：将同一应用部署到多台机器上，组成处理集群，接收负载均衡设备分发的请求，进行处理并返回响应的数据。负载均衡器:将用户访问的请求根据对应的负载均衡算法，分发到集群中的一台服务器进行处理。 负载均衡的作用1、解决服务器的高并发压力，提高应用程序的处理性能。2、提供故障转移，实现高可用。3、通过添加或减少服务器数量，增强网站的可扩展性。4、在负载均衡器上进行过滤，可以提高系统的安全性。 负载均衡常用的处理方式方式一:用户手动选择这种方式比较原始，只要实现的方式就是在网站主页上面提供不同线路、不同服务器链接方式，让用户来选择自己访问的具体服务器，来实现负载均衡。 方式二:DNS轮询方式DNS 1域名系统（服务）协议（DNS）是一种分布式网络目录服务，主要用于域名与 IP 地址的相互转换。 大多域名注册商都支持对同一个主机名添加多条A记录（一个域名可以绑定多个ip地址），这就是DNS轮询，DNS服务器将解析请求按照A记录的顺序，随机分配到不同的IP上，这样就能完成简单的负载均衡。DNS轮询的成本非常低，在一些不重要的服务器，被经常使用。根据请求域名 解析出具体ip 然后轮询分配 验证: 1ping www.nginx521.cn 清空本地的dns缓存 1ipconfig/flushdns 我们发现使用DNS来实现轮询，不需要投入过多的成本，虽然DNS轮询成本低廉，但是DNS负载均衡存在明显的缺点。1.可靠性低假设一个域名DNS轮询多台服务器，如果其中的一台服务器发生故障，那么所有的访问该服务器的请求将不会有所回应，即使你将该服务器的IP从DNS中去掉，但是由于各大宽带接入商将众多的DNS存放在缓存中，以节省访问时间，导致DNS不会实时更新。所以DNS轮流上一定程度上解决了负载均衡问题，但是却存在可靠性不高的缺点。2.负载均衡不均衡DNS负载均衡采用的是简单的轮询负载算法，不能区分服务器的差异，不能反映服务器的当前运行状态，不能做到为性能好的服务器多分配请求，另外本地计算机也会缓存已经解析的域名到IP地址的映射，这也会导致使用该DNS服务器的用户在一定时间内访问的是同一台Web服务器，从而引发Web服务器减的负载不均衡。负载不均衡则会导致某几台服务器负荷很低，而另外几台服务器负荷确很高，处理请求的速度慢，配置高的服务器分配到的请求少，而配置低的服务器分配到的请求多。 方式三:四&#x2F;七层负载均衡 常用介绍四&#x2F;七层负载均衡之前，我们先了解一个概念，OSI(open system interconnection),叫开放式系统互联模型，这个是由国际标准化组织ISO指定的一个不基于具体机型、操作系统或公司的网络体系结构。该模型将网络通信的工作分为七层。 计网应用层：为应用程序提供网络服务。表示层：对数据进行格式化、编码、加密、压缩等操作。会话层：建立、维护、管理会话连接。传输层：建立、维护、管理端到端的连接，常见的有TCP&#x2F;UDP。网络层：IP寻址和路由选择数据链路层：控制网络层与物理层之间的通信。物理层：比特流传输。所谓四层负载均衡指的是OSI七层模型中的传输层，主要是基于IP+PORT的负载均衡 123实现四层负载均衡的方式：硬件：F5 BIG-IP、Radware等软件：LVS、Nginx、Hayproxy等 所谓的七层负载均衡指的是在应用层，主要是基于虚拟的URL或主机IP的负载均衡 12实现七层负载均衡的方式：软件：Nginx、Hayproxy等 四层和七层负载均衡的区别 12四层负载均衡数据包是在底层就进行了分发，而七层负载均衡数据包则在最顶端进行分发，所以四层负载均衡的效率比七层负载均衡的要高。四层负载均衡不识别域名，而七层负载均衡识别域名。 处理四层和七层负载以为其实还有二层、三层负载均衡，二层是在数据链路层基于mac地址来实现负载均衡，三层是在网络层一般采用虚拟IP地址的方式实现负载均衡。实际环境采用的模式 1四层负载(LVS)+七层负载(Nginx) 实际 我全都要 Nginx七层负载均衡 **************Nginx要实现七层负载均衡需要用到proxy_pass代理模块配置。Nginx默认安装支持这个模块，我们不需要再做任何处理。Nginx的负载均衡是在Nginx的反向代理基础上把用户的请求根据指定的算法分发到一组【upstream虚拟服务池】。 Nginx七层负载均衡的指令upstream指令该指令是用来定义一组服务器，它们可以是监听不同端口的服务器，并且也可以是同时监听TCP和Unix socket的服务器。服务器可以指定不同的权重，默认为1。 语法 upstream name {…} 默认值 — 位置 http server指令该指令用来指定后端服务器的名称和一些参数，可以使用域名、IP、端口或者unix socket 语法 server name [paramerters] 默认值 — 位置 upstream Nginx七层负载均衡的实现流程服务端设置 配置三台端口来模拟 123456789101112131415161718192021222324server &#123; listen 9001; server_name localhost; default_type text/html; location /&#123; return 200 &#x27;&lt;h1&gt;192.168.200.146:9001&lt;/h1&gt;&#x27;; &#125;&#125;server &#123; listen 9002; server_name localhost; default_type text/html; location /&#123; return 200 &#x27;&lt;h1&gt;192.168.200.146:9002&lt;/h1&gt;&#x27;; &#125;&#125;server &#123; listen 9003; server_name localhost; default_type text/html; location /&#123; return 200 &#x27;&lt;h1&gt;192.168.200.146:9003&lt;/h1&gt;&#x27;; &#125;&#125; 负载均衡器设置 123456789101112upstream backend&#123; # 监听这里的所有 监听组 server 192.168.200.146:9091; server 192.168.200.146:9092; server 192.168.200.146:9093;&#125;server &#123; listen 8083; server_name localhost; location /&#123; proxy_pass http://backend; &#125;&#125; 负载均衡状态代理服务器在负责均衡调度中的状态有以下几个： 状态 概述 down 当前的server暂时不参与负载均衡 backup 预留的备份服务器 max_fails 允许请求失败的次数 fail_timeout 经过max_fails失败后, 服务暂停时间 max_conns 限制最大的接收连接数 downdown:将该服务器标记为永久不可用，那么该代理服务器将不参与负载均衡。 123456789101112upstream backend&#123; server 192.168.200.146:9001 down; server 192.168.200.146:9002 server 192.168.200.146:9003;&#125;server &#123; listen 8083; server_name localhost; location /&#123; proxy_pass http://backend; &#125;&#125; 该状态一般会对需要停机维护的服务器进行设置。 backupbackup:将该服务器标记为备份服务器，当主服务器不可用时，将用来传递请求。 123456789101112upstream backend&#123; server 192.168.200.146:9001 down; server 192.168.200.146:9002 backup; server 192.168.200.146:9003;&#125;server &#123; listen 8083; server_name localhost; location /&#123; proxy_pass http://backend; &#125;&#125; 此时需要将9094端口的访问禁止掉来模拟下唯一能对外提供访问的服务宕机以后，backup的备份服务器就要开始对外提供服务，此时为了测试验证，我们需要使用防火墙来进行拦截。介绍一个工具firewall-cmd,该工具是Linux提供的专门用来操作firewall的。查询防火墙中指定的端口是否开放 1firewall-cmd --query-port=9001/tcp 如何开放一个指定的端口 1firewall-cmd --permanent --add-port=9002/tcp 批量添加开发端口 1firewall-cmd --permanent --add-port=9001-9003/tcp 如何移除一个指定的端口 1firewall-cmd --permanent --remove-port=9003/tcp 重新加载 1firewall-cmd --reload 其中 –permanent表示设置为持久 –add-port表示添加指定端口 –remove-port表示移除指定端口 max_connsmax_conns&#x3D;number:用来设置代理服务器同时活动链接的最大数量，默认为0，表示不限制，使用该配置可以根据后端服务器处理请求的并发量来进行设置，防止后端服务器被压垮。 max_fails和fail_timeoutmax_fails&#x3D;number:设置允许请求代理服务器失败的次数，默认为1。fail_timeout&#x3D;time:设置经过max_fails失败后，服务暂停的时间，默认是10秒。 123456789101112upstream backend&#123; server 192.168.200.133:9001 down; server 192.168.200.133:9002 backup; server 192.168.200.133:9003 max_fails=3 fail_timeout=15; # 失败三次以后 服务暂停15秒&#125;server &#123; listen 8083; server_name localhost; location /&#123; proxy_pass http://backend; &#125;&#125; 负载均衡策略 *********介绍完Nginx负载均衡的相关指令后，我们已经能实现将用户的请求分发到不同的服务器上，那么除了采用默认的分配方式以外，我们还能采用什么样的负载算法?Nginx的upstream支持如下六种方式的分配算法，分别是: 算法名称 说明 轮询 默认方式 weight 权重方式 ip_hash 依据ip分配方式 least_conn 依据最少连接方式 url_hash 依据URL分配方式 fair 依据响应时间方式 轮询是upstream模块负载均衡默认的策略。每个请求会按时间顺序逐个分配到不同的后端服务器。轮询不需要额外的配置。 123456789101112upstream backend&#123; server 192.168.200.146:9001 weight=1; server 192.168.200.146:9002; server 192.168.200.146:9003;&#125;server &#123; listen 8083; server_name localhost; location /&#123; proxy_pass http://backend; &#125;&#125; weight加权[加权轮询]weight&#x3D;number:用来设置服务器的权重，默认为1，权重数据越大，被分配到请求的几率越大；该权重值，主要是针对实际工作环境中不同的后端服务器硬件配置进行调整的，所有此策略比较适合服务器的硬件配置差别比较大的情况。 123456789101112upstream backend&#123; server 192.168.200.146:9001 weight=10; server 192.168.200.146:9002 weight=5; server 192.168.200.146:9003 weight=3;&#125;server &#123; listen 8083; server_name localhost; location /&#123; proxy_pass http://backend; &#125;&#125; ip_hash当对后端的多台动态应用服务器做负载均衡时，ip_hash指令能够将某个客户端IP的请求通过哈希算法定位到同一台后端服务器上。这样，当来自某一个IP的用户在后端Web服务器A上登录后，在访问该站点的其他URL，能保证其访问的还是后端web服务器A。 语法 ip_hash; 默认值 — 位置 upstream 12345678910111213upstream backend&#123; ip_hash; server 192.168.200.146:9001; server 192.168.200.146:9002; server 192.168.200.146:9003;&#125;server &#123; listen 8083; server_name localhost; location /&#123; proxy_pass http://backend; &#125;&#125; 需要额外多说一点的是使用ip_hash指令无法保证后端服务器的负载均衡，可能导致有些后端服务器接收到的请求多，有些后端服务器接收的请求少，而且设置后端服务器权重等方法将不起作用。 least_conn最少连接，把请求转发给连接数较少的后端服务器。轮询算法是把请求平均的转发给各个后端，使它们的负载大致相同；但是，有些请求占用的时间很长，会导致其所在的后端负载较高。这种情况下，least_conn这种方式就可以达到更好的负载均衡效果。 12345678910111213upstream backend&#123; least_conn; server 192.168.200.146:9001; server 192.168.200.146:9002; server 192.168.200.146:9003;&#125;server &#123; listen 8083; server_name localhost; location /&#123; proxy_pass http://backend; &#125;&#125; 此负载均衡策略适合请求处理时间长短不一造成服务器过载的情况。 url_hash按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，要配合缓存命中来使用。同一个资源多次请求，可能会到达不同的服务器上，导致不必要的多次下载，缓存命中率不高，以及一些资源时间的浪费。而使用url_hash，可以使得同一个url（也就是同一个资源请求）会到达同一台服务器，一旦缓存住了资源，再此收到请求，就可以从缓存中读取。 12345678910111213upstream backend&#123; hash &amp;request_uri; # &amp;request_uri 内置变量 获取的就是uri地址 server 192.168.200.146:9001; server 192.168.200.146:9002; server 192.168.200.146:9003;&#125;server &#123; listen 8083; server_name localhost; location /&#123; proxy_pass http://backend; &#125;&#125; 访问如下地址： 123http://192.168.200.133:8083/ahttp://192.168.200.133:8083/bhttp://192.168.200.133:8083/c fair–公平公正fair采用的不是内建负载均衡使用的轮换的均衡算法，而是可以根据页面大小、加载时间长短智能的进行负载均衡。那么如何使用第三方模块的fair负载均衡策略。 12345678910111213upstream backend&#123; fair; server 192.168.200.146:9001; server 192.168.200.146:9002; server 192.168.200.146:9003;&#125;server &#123; listen 8083; server_name localhost; location /&#123; proxy_pass http://backend; &#125;&#125; 但是如何直接使用会报错，因为fair属于第三方模块实现的负载均衡。需要添加nginx-upstream-fair,如何添加对应的模块: 下载nginx-upstream-fair模块 12下载地址为: https://github.com/gnosek/nginx-upstream-fair 将下载的文件上传到服务器并进行解压缩 1unzip nginx-upstream-fair-master.zip 重命名资源 1mv nginx-upstream-fair-master fair 使用.&#x2F;configure命令将资源添加到Nginx模块中 1./configure --add-module=/root/fair 编译 1make 编译可能会出现如下错误，ngx_http_upstream_srv_conf_t结构中缺少default_port 解决方案:在Nginx的源码中 src&#x2F;http&#x2F;ngx_http_upstream.h,找到ngx_http_upstream_srv_conf_s，在模块中添加添加default_port属性 1in_port_t default_port 然后再进行make. 更新Nginx 6.1 将sbin目录下的nginx进行备份 1mv /usr/local/nginx/sbin/nginx /usr/local/nginx/sbin/nginxold 6.2 将安装目录下的objs中的nginx拷贝到sbin目录 12cd objscp nginx /usr/local/nginx/sbin 6.3 更新Nginx 12cd ../make upgrade 编译测试使用Nginx 上面介绍了Nginx常用的负载均衡的策略，有人说是5种，是把轮询和加权轮询归为一种，也有人说是6种。那么在咱们以后的开发中到底使用哪种，这个需要根据实际项目的应用场景来决定的。 负载均衡案例案例一：对所有请求实现一般轮询规则的负载均衡123456789101112upstream backend&#123; server 192.168.200.146:9001; server 192.168.200.146:9002; server 192.168.200.146:9003;&#125;server &#123; listen 8083; server_name localhost; location /&#123; proxy_pass http://backend; &#125;&#125; 案例二：对所有请求实现加权轮询规则的负载均衡123456789101112upstream backend&#123; server 192.168.200.146:9001 weight=7; server 192.168.200.146:9002 weight=5; server 192.168.200.146:9003 weight=3;&#125;server &#123; listen 8083; server_name localhost; location /&#123; proxy_pass http://backend; &#125;&#125; 案例三：对特定资源实现负载均衡123456789101112131415161718upstream videobackend&#123; server 192.168.200.146:9001; server 192.168.200.146:9002;&#125;upstream filebackend&#123; server 192.168.200.146:9003; server 192.168.200.146:9004;&#125;server &#123; listen 8084; server_name localhost; location /video/ &#123; proxy_pass http://videobackend; &#125; location /file/ &#123; proxy_pass http://filebackend; &#125;&#125; 案例四：对不同域名实现负载均衡12345678910111213141516171819202122upstream itcastbackend&#123; server 192.168.200.146:9001; server 192.168.200.146:9002;&#125;upstream itheimabackend&#123; server 192.168.200.146:9003; server 192.168.200.146:9004;&#125;server &#123; listen 8085; server_name www.itcast.cn; location / &#123; proxy_pass http://itcastbackend; &#125;&#125;server &#123; listen 8086; server_name www.itheima.cn; location / &#123; proxy_pass http://itheimabackend; &#125;&#125; 案例五：实现带有URL重写的负载均衡123456789101112131415upstream backend&#123; server 192.168.200.146:9001; server 192.168.200.146:9002; server 192.168.200.146:9003;&#125;server &#123; listen 80; server_name localhost; location /file/ &#123; rewrite ^(/file/.*) /server/$1 last; &#125; location / &#123; proxy_pass http://backend; &#125;&#125; Nginx四层负载均衡Nginx在1.9之后，增加了一个stream模块，用来实现四层协议的转发、代理、负载均衡等。stream模块的用法跟http的用法类似，允许我们配置一组TCP或者UDP等协议的监听，然后通过proxy_pass来转发我们的请求，通过upstream添加多个后端服务，实现负载均衡。 四层协议负载均衡的实现，一般都会用到LVS、HAProxy、F5等，要么很贵要么配置很麻烦，而Nginx的配置相对来说更简单，更能快速完成工作。 添加stream模块的支持Nginx默认是没有编译这个模块的，需要使用到stream模块，那么需要在编译的时候加上--with-stream。完成添加--with-stream的实现步骤: 123456》将原有/usr/local/nginx/sbin/nginx进行备份》拷贝nginx之前的配置信息》在nginx的安装源码进行配置指定对应模块 ./configure --with-stream》通过make模板进行编译》将objs下面的nginx移动到/usr/local/nginx/sbin下》在源码目录下执行 make upgrade进行升级，这个可以实现不停机添加新模块的功能 Nginx四层负载均衡的指令stream指令该指令提供在其中指定流服务器指令的配置文件上下文。和http指令同级。 语法 stream { … } 默认值 — 位置 main upstream指令该指令和http的upstream指令是类似的。 四层负载均衡的案例需求分析实现步骤(1)准备Redis服务器,在一条服务器上准备三个Redis，端口分别是6379,63781.上传redis的安装包，redis-4.0.14.tar.gz2.将安装包进行解压缩 1tar -zxf redis-4.0.14.tar.gz 3.进入redis的安装包 1cd redis-4.0.14 4.使用make和install进行编译和安装 1make PREFIX=/usr/local/redis/redis01 install 5.拷贝redis配置文件redis.conf到&#x2F;usr&#x2F;local&#x2F;redis&#x2F;redis01&#x2F;bin目录中 1cp redis.conf /usr/local/redis/redis01/bin 6.修改redis.conf配置文件 12port 6379 #redis的端口daemonize yes #后台启动redis 7.将redis01复制一份为redis02 12cd /usr/local/rediscp -r redis01 redis02 8.将redis02文件文件夹中的redis.conf进行修改 12port 6378 #redis的端口daemonize yes #后台启动redis 9.分别启动，即可获取两个Redis.并查看 1ps -ef | grep redis 使用Nginx将请求分发到不同的Redis服务器上。 (2)准备Tomcat服务器.1.上传tomcat的安装包，apache-tomcat-8.5.56.tar.gz 2.将安装包进行解压缩 1tar -zxf apache-tomcat-8.5.56.tar.gz 3.进入tomcat的bin目录 12cd apache-tomcat-8.5.56/bin./startup nginx.conf配置 1234567891011121314151617stream &#123; upstream redisbackend &#123; server 192.168.200.146:6379; server 192.168.200.146:6378; &#125; upstream tomcatbackend &#123; server 192.168.200.146:8080; &#125; server &#123; listen 81; proxy_pass redisbackend; &#125; server &#123; listen 82; proxy_pass tomcatbackend; &#125;&#125; 访问测试。 Nginx缓存集成缓存的概念缓存就是数据交换的缓冲区(称作:Cache),当用户要获取数据的时候，会先从缓存中去查询获取数据，如果缓存中有就会直接返回给用户，如果缓存中没有，则会发请求从服务器重新查询数据，将数据返回给用户的同时将数据放入缓存，下次用户就会直接从缓存中获取数据。 缓存其实在很多场景中都有用到，比如： 场景 作用 操作系统磁盘缓存 减少磁盘机械操作 数据库缓存 减少文件系统的IO操作 应用程序缓存 减少对数据库的查询 Web服务器缓存 减少对应用服务器请求次数 浏览器缓存 减少与后台的交互次数 缓存的优点 1.减少数据传输，节省网络流量，加快响应速度，提升用户体验； 2.减轻服务器压力； 3.提供服务端的高可用性；缓存的缺点 1.数据的不一致 2.增加成本 本次课程注解讲解的是Nginx,Nginx作为web服务器，Nginx作为Web缓存服务器，它介于客户端和应用服务器之间，当用户通过浏览器访问一个URL时，web缓存服务器会去应用服务器获取要展示给用户的内容，将内容缓存到自己的服务器上，当下一次请求到来时，如果访问的是同一个URL，web缓存服务器就会直接将之前缓存的内容返回给客户端，而不是向应用服务器再次发送请求。web缓存降低了应用服务器、数据库的负载，减少了网络延迟，提高了用户访问的响应速度，增强了用户的体验。 Nginx的web缓存服务Nginx是从0.7.48版开始提供缓存功能。Nginx是基于Proxy Store来实现的，其原理是把URL及相关组合当做Key,在使用MD5算法对Key进行哈希，得到硬盘上对应的哈希目录路径，从而将缓存内容保存在该目录中。它可以支持任意URL连接，同时也支持404&#x2F;301&#x2F;302这样的非200状态码。Nginx即可以支持对指定URL或者状态码设置过期时间，也可以使用purge命令来手动清除指定URL的缓存。 Nginx缓存设置的相关指令Nginx的web缓存服务主要是使用ngx_http_proxy_module模块相关指令集来完成，接下来我们把常用的指令来进行介绍下。 proxy_cache_path该指定用于设置缓存文件的存放路径 | 语法 | proxy_cache_path path [levels&#x3D;number] keys_zone&#x3D;zone_name:zone_size [inactive&#x3D;time][max_size&#x3D;size]; || — | — || 默认值 | — || 位置 | http | path:缓存路径地址,如： 1/usr/local/proxy_cache levels: 指定该缓存空间对应的目录，最多可以设置3层，每层取值为1|2如 : 123456levels=1:2 缓存空间有两层目录，第一次是1个字母，第二次是2个字母举例说明:itheima[key]通过MD5加密以后的值为 43c8233266edce38c2c9af0694e2107dlevels=1:2 最终的存储路径为/usr/local/proxy_cache/d/07levels=2:1:2 最终的存储路径为/usr/local/proxy_cache/7d/0/21levels=2:2:2 最终的存储路径为??/usr/local/proxy_cache/7d/10/e2 keys_zone:用来为这个缓存区设置名称和指定大小，如： 1keys_zone=itcast:200m 缓存区的名称是itcast,大小为200M,1M大概能存储8000个keys inactive:指定缓存的数据多次时间未被访问就将被删除，如： 1inactive=1d 缓存数据在1天内没有被访问就会被删除 max_size:设置最大缓存空间，如果缓存空间存满，默认会覆盖缓存时间最长的资源，如: 1max_size=20g 配置实例: 123http&#123; proxy_cache_path /usr/local/proxy_cache keys_zone=itcast:200m levels=1:2:1 inactive=1d max_size=20g;&#125; proxy_cache该指令用来开启或关闭代理缓存，如果是开启则自定使用哪个缓存区来进行缓存。 语法 proxy_cache zone_name&#124;off; 默认值 proxy_cache off; 位置 http、server、location zone_name：指定使用缓存区的名称 proxy_cache_key该指令用来设置web缓存的key值，Nginx会根据key值MD5哈希存缓存。 语法 proxy_cache_key key; 默认值 proxy_cache_key $scheme$proxy_host$request_uri; 位置 http、server、location proxy_cache_valid该指令用来对不同返回状态码的URL设置不同的缓存时间 语法 proxy_cache_valid [code …] time; 默认值 — 位置 http、server、location 如： 12345proxy_cache_valid 200 302 10m;proxy_cache_valid 404 1m;为200和302的响应URL设置10分钟缓存，为404的响应URL设置1分钟缓存proxy_cache_valid any 1m;对所有响应状态码的URL都设置1分钟缓存 proxy_cache_min_uses该指令用来设置资源被访问多少次后被缓存 语法 proxy_cache_min_uses number; 默认值 proxy_cache_min_uses 1; 位置 http、server、location proxy_cache_methods该指令用户设置缓存哪些HTTP方法 语法 proxy_cache_methods GET&#124;HEAD&#124;POST; 默认值 proxy_cache_methods GET HEAD; 位置 http、server、location 默认缓存HTTP的GET和HEAD方法，不缓存POST方法。 Nginx缓存设置案例需求分析 步骤实现1.环境准备应用服务器的环境准备（1）在192.168.200.146服务器上的tomcat的webapps下面添加一个js目录，并在js目录中添加一个jquery.js文件（2）启动tomcat（3）访问测试 1http://192.168.200.146:8080/js/jquery.js Nginx的环境准备（1）完成Nginx反向代理配置 123456789101112http&#123; upstream backend&#123; server 192.168.200.146:8080; &#125; server &#123; listen 8080; server_name localhost; location / &#123; proxy_pass http://backend/js/; &#125; &#125;&#125; （2）完成Nginx缓存配置 4.添加缓存配置 1234567891011121314151617181920http&#123; proxy_cache_path /usr/local/proxy_cache levels=2:1 keys_zone=itcast:200m inactive=1d max_size=20g; upstream backend&#123; server 192.168.200.146:8080; &#125; server &#123; listen 8080; server_name localhost; location / &#123; proxy_cache itcast; proxy_cache_key itheima; proxy_cache_min_uses 5; proxy_cache_valid 200 5d; proxy_cache_valid 404 30s; proxy_cache_valid any 1m; add_header nginx-cache &quot;$upstream_cache_status&quot;; proxy_pass http://backend/js/; &#125; &#125;&#125; Nginx缓存的清除方式一:删除对应的缓存目录1rm -rf /usr/local/proxy_cache/...... 方式二:使用第三方扩展模块ngx_cache_purge（1）下载ngx_cache_purge模块对应的资源包，并上传到服务器上。 1ngx_cache_purge-2.3.tar.gz （2）对资源文件进行解压缩 1tar -zxf ngx_cache_purge-2.3.tar.gz （3）修改文件夹名称，方便后期配置 1mv ngx_cache_purge-2.3 purge （4）查询Nginx的配置参数 1nginx -V （5）进入Nginx的安装目录，使用.&#x2F;configure进行参数配置 1./configure --add-module=/root/nginx/module/purge （6）使用make进行编译 1make （7）将nginx安装目录的nginx二级制可执行文件备份 1mv /usr/local/nginx/sbin/nginx /usr/local/nginx/sbin/nginxold （8）将编译后的objs中的nginx拷贝到nginx的sbin目录下 1cp objs/nginx /usr/local/nginx/sbin （9）使用make进行升级 1make upgrade （10）在nginx配置文件中进行如下配置 12345server&#123; location ~/purge(/.*) &#123; proxy_cache_purge itcast itheima; &#125;&#125; Nginx设置资源不缓存前面咱们已经完成了Nginx作为web缓存服务器的使用。但是我们得思考一个问题就是不是所有的数据都适合进行缓存。比如说对于一些经常发生变化的数据。如果进行缓存的话，就很容易出现用户访问到的数据不是服务器真实的数据。所以对于这些资源我们在缓存的过程中就需要进行过滤，不进行缓存。Nginx也提供了这块的功能设置，需要使用到如下两个指令 proxy_no_cache该指令是用来定义不将数据进行缓存的条件。 语法 proxy_no_cache string …; 默认值 — 位置 http、server、location 配置实例 1proxy_no_cache $cookie_nocache $arg_nocache $arg_comment; proxy_cache_bypass该指令是用来设置不从缓存中获取数据的条件。 语法 proxy_cache_bypass string …; 默认值 — 位置 http、server、location 配置实例 1proxy_cache_bypass $cookie_nocache $arg_nocache $arg_comment; 上述两个指令都有一个指定的条件，这个条件可以是多个，并且多个条件中至少有一个不为空且不等于”0”,则条件满足成立。上面给的配置实例是从官方网站获取的，里面使用到了三个变量，分别是$cookie_nocache、$arg_nocache、$arg_comment $cookie_nocache、$arg_nocache、$arg_comment这三个参数分别代表的含义是: 1234$cookie_nocache指的是当前请求的cookie中键的名称为nocache对应的值$arg_nocache和$arg_comment指的是当前请求的参数中属性名为nocache和comment对应的属性值 案例演示下: 1234567891011log_format params $cookie_nocache | $arg_nocache | $arg_comment；server&#123; listen 8081; server_name localhost; location /&#123; access_log logs/access_params.log params; add_header Set-Cookie &#x27;nocache=999&#x27;; root html; index index.html; &#125;&#125; 案例实现设置不缓存资源的配置方案 1234567891011server&#123; listen 8080; server_name localhost; location / &#123; if ($request_uri ~ /.*\\.js$)&#123; set $nocache 1; &#125; proxy_no_cache $nocache $cookie_nocache $arg_nocache $arg_comment; proxy_cache_bypass $nocache $cookie_nocache $arg_nocache $arg_comment; &#125;&#125; Nginx高可用解决方案 ***nginx宕机怎么办？ 需要两台以上的Nginx服务器对外提供服务，这样的话就可以解决其中一台宕机了，另外一台还能对外提供服务，但是如果是两台Nginx服务器的话，会有两个IP地址，用户该访问哪台服务器，用户怎么知道哪台是好的，哪台是宕机了的? Keepalived ***使用Keepalived来解决，Keepalived 软件由 C 编写的，最初是专为 LVS 负载均衡软件设计的，Keepalived 软件主要是通过 VRRP 协议实现高可用功能。 VRRP介绍说白了 以上两个路由虚拟出来一个路由 这个虚拟的路由有自己的ip地址 用户访问的就是这虚拟的ip地址 最终还是到真实路由上（真是路由会分为 主节点master 次节点backup） 主节点挂了 次节点补上 并且次节点会竞争谁来补上 （backup就是和master节点有联系 用来接收master的信息 如果某个时间段内没收到这样的信息 master就是G了 就需要补上） 这就是nginx的宕机的解决原理VRRP（Virtual Route Redundancy Protocol）协议，翻译过来为虚拟路由冗余协议。VRRP协议将两台或多台路由器设备虚拟成一个设备，对外提供虚拟路由器IP,而在路由器组内部，如果实际拥有这个对外IP的路由器如果工作正常的话就是MASTER,MASTER实现针对虚拟路由器IP的各种网络功能。其他设备不拥有该虚拟IP，状态为BACKUP,处了接收MASTER的VRRP状态通告信息以外，不执行对外的网络功能。当主机失效时，BACKUP将接管原先MASTER的网络功能。从上面的介绍信息获取到的内容就是VRRP是一种协议，那这个协议是用来干什么的？1.选择协议VRRP可以把一个虚拟路由器的责任动态分配到局域网上的 VRRP 路由器中的一台。其中的虚拟路由即Virtual路由是由VRRP路由群组创建的一个不真实存在的路由，这个虚拟路由也是有对应的IP地址。而且VRRP路由1和VRRP路由2之间会有竞争选择，通过选择会产生一个Master路由和一个Backup路由。2.路由容错协议Master路由和Backup路由之间会有一个心跳检测，Master会定时告知Backup自己的状态，如果在指定的时间内，Backup没有接收到这个通知内容，Backup就会替代Master成为新的Master。Master路由有一个特权就是虚拟路由和后端服务器都是通过Master进行数据传递交互的，而备份节点则会直接丢弃这些请求和数据，不做处理，只是去监听Master的状态用了Keepalived后，解决方案如下: 用户访问以上的vip虚拟 它在给master 如果master挂了 备份补上 环境搭建环境准备 VIP IP 主机名 主&#x2F;从 192.168.200.133 keepalived1 Master 192.168.200.222 192.168.200.122 keepalived2 Backup keepalived的安装 1234567891011步骤1:从官方网站下载keepalived,官网地址https://keepalived.org/步骤2:将下载的资源上传到服务器 keepalived-2.0.20.tar.gz步骤3:创建keepalived目录，方便管理资源 mkdir keepalived步骤4:将压缩文件进行解压缩，解压缩到指定的目录 tar -zxf keepalived-2.0.20.tar.gz -C keepalived/步骤5:对keepalived进行配置，编译和安装 cd keepalived/keepalived-2.0.20 ./configure --sysconf=/etc --prefix=/usr/local make &amp;&amp; make install 安装完成后，有两个文件需要我们认识下，一个是 &#x2F;etc&#x2F;keepalived&#x2F;keepalived.conf(keepalived的系统配置文件，我们主要操作的就是该文件)，一个是&#x2F;usr&#x2F;local&#x2F;sbin目录下的keepalived,是系统配置脚本，用来启动和关闭keepalived Keepalived配置文件介绍打开keepalived.conf配置文件这里面会分三部，第一部分是global全局配置、第二部分是vrrp相关配置、第三部分是LVS相关配置。本次课程主要是使用keepalived实现高可用部署，没有用到LVS，所以我们重点关注的是前两部分 12345678910111213141516171819202122232425global全局部分：global_defs &#123; #通知邮件，当keepalived发送切换时需要发email给具体的邮箱地址 notification_email &#123; tom@itcast.cn jerry@itcast.cn &#125; #设置发件人的邮箱信息 notification_email_from zhaomin@itcast.cn #指定smpt服务地址 smtp_server 192.168.200.1 #指定smpt服务连接超时时间 smtp_connect_timeout 30 #运行keepalived服务器的一个标识，可以用作发送邮件的主题信息 router_id LVS_DEVEL #默认是不跳过检查。检查收到的VRRP通告中的所有地址可能会比较耗时，设置此命令的意思是，如果通告与接收的上一个通告来自相同的master路由器，则不执行检查(跳过检查) vrrp_skip_check_adv_addr #严格遵守VRRP协议。 vrrp_strict #在一个接口发送的两个免费ARP之间的延迟。可以精确到毫秒级。默认是0 vrrp_garp_interval 0 #在一个网卡上每组na消息之间的延迟时间，默认为0 vrrp_gna_interval 0&#125; 123456789101112131415161718192021VRRP部分，该部分可以包含以下四个子模块1. vrrp_script2. vrrp_sync_group3. garp_group4. vrrp_instance我们会用到第一个和第四个，#设置keepalived实例的相关信息，VI_1为VRRP实例名称vrrp_instance VI_1 &#123; state MASTER #有两个值可选MASTER主 BACKUP备 interface ens33 #vrrp实例绑定的接口，用于发送VRRP包[当前服务器使用的网卡名称] virtual_router_id 51#指定VRRP实例ID，范围是0-255 priority 100 #指定优先级，优先级高的将成为MASTER advert_int 1 #指定发送VRRP通告的间隔，单位是秒 authentication &#123; #vrrp之间通信的认证信息 auth_type PASS #指定认证方式。PASS简单密码认证(推荐) auth_pass 1111 #指定认证使用的密码，最多8位 &#125; virtual_ipaddress &#123; #虚拟IP地址设置虚拟IP地址，供用户访问使用，可设置多个，一行一个 192.168.200.222 &#125;&#125; 配置内容如下:服务器1 1234567891011121314151617181920212223242526272829global_defs &#123; notification_email &#123; tom@itcast.cn jerry@itcast.cn &#125; notification_email_from zhaomin@itcast.cn smtp_server 192.168.200.1 smtp_connect_timeout 30 router_id keepalived1 vrrp_skip_check_adv_addr vrrp_strict vrrp_garp_interval 0 vrrp_gna_interval 0&#125;vrrp_instance VI_1 &#123; state MASTER interface ens33 virtual_router_id 51 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.200.222 &#125;&#125; 服务器2 12345678910111213141516171819202122232425262728293031! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; tom@itcast.cn jerry@itcast.cn &#125; notification_email_from zhaomin@itcast.cn smtp_server 192.168.200.1 smtp_connect_timeout 30 router_id keepalived2 vrrp_skip_check_adv_addr vrrp_strict vrrp_garp_interval 0 vrrp_gna_interval 0&#125;vrrp_instance VI_1 &#123; state BACKUP interface ens33 virtual_router_id 51 priority 90 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.200.222 &#125;&#125; 访问测试 启动keepalived之前，咱们先使用命令 ip a,查看192.168.200.133和192.168.200.122这两台服务器的IP情况。 分别启动两台服务器的keepalived 12cd /usr/local/sbin./keepalived 再次通过 ip a查看ip 当把192.168.200.133服务器上的keepalived关闭后，再次查看ip 通过上述的测试，我们会发现，虚拟IP(VIP)会在MASTER节点上，当MASTER节点上的keepalived出问题以后，因为BACKUP无法收到MASTER发出的VRRP状态通过信息，就会直接升为MASTER。VIP也会”漂移”到新的MASTER。 上面测试和Nginx有什么关系?我们把192.168.200.133服务器的keepalived再次启动下，由于它的优先级高于服务器192.168.200.122的，所有它会再次成为MASTER，VIP也会”漂移”过去，然后我们再次通过浏览器访问:http://192.168.200.222/如果把192.168.200.133服务器的keepalived关闭掉，再次访问相同的地址效果实现了以后， 我们会发现要想让vip进行切换，就必须要把服务器上的keepalived进行关闭，而什么时候关闭keepalived呢?应该是在keepalived所在服务器的nginx出现问题后，把keepalived关闭掉，就可以让VIP执行另外一台服务器，但是现在这所有的操作都是通过手动来完成的，我们如何能让系统自动判断当前服务器的nginx是否正确启动，如果没有，要能让VIP自动进行”漂移”，这个问题该如何解决? keepalived之vrrp_scriptkeepalived只能做到对网络故障和keepalived本身的监控，即当出现网络故障或者keepalived本身出现问题时，进行切换。但是这些还不够，我们还需要监控keepalived所在服务器上的其他业务，比如Nginx,如果Nginx出现异常了，仅仅keepalived保持正常，是无法完成系统的正常工作的，因此需要根据业务进程的运行状态决定是否需要进行主备切换，这个时候，我们可以通过编写脚本对业务进程进行检测监控。实现步骤: 在keepalived配置文件中添加对应的配置像 123456vrrp_script 脚本名称&#123; script &quot;脚本位置&quot; interval 3 #执行时间间隔 weight -20 #动态调整vrrp_instance的优先级&#125; 编写脚本 ck_nginx.sh 123456789#!/bin/bashnum=`ps -C nginx --no-header | wc -l`if [ $num -eq 0 ];then /usr/local/nginx/sbin/nginx sleep 2 if [ `ps -C nginx --no-header | wc -l` -eq 0 ]; then killall keepalived fifi Linux ps命令用于显示当前进程 (process) 的状态。-C(command) :指定命令的所有进程–no-header 排除标题 为脚本文件设置权限 chmod 755 ck_nginx.sh 将脚本添加到 12345678910111213141516171819202122vrrp_script ck_nginx &#123; script &quot;/etc/keepalived/ck_nginx.sh&quot; #执行脚本的位置 interval 2 #执行脚本的周期，秒为单位 weight -20 #权重的计算方式&#125;vrrp_instance VI_1 &#123; state MASTER interface ens33 virtual_router_id 10 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.200.111 &#125; track_script &#123; ck_nginx &#125;&#125; 如果效果没有出来，可以使用 tail -f &#x2F;var&#x2F;log&#x2F;messages查看日志信息，找对应的错误信息。 测试 问题思考:通常如果master服务死掉后backup会变成master，但是当master服务又好了的时候 master此时会抢占VIP，这样就会发生两次切换对业务繁忙的网站来说是不好的。所以我们要在配置文件加入 nopreempt 非抢占，但是这个参数只能用于state 为backup，故我们在用HA的时候最好master 和backup的state都设置成backup 让其通过priority来竞争。 Nginx制作下载站点首先我们先要清楚什么是下载站点?我们先来看一个网站http://nginx.org/download/这个我们刚开始学习Nginx的时候给大家看过这样的网站，该网站主要就是用来提供用户来下载相关资源的网站，就叫做下载网站。如何制作一个下载站点:nginx使用的是模块ngx_http_autoindex_module来实现的，该模块处理以斜杠(“&#x2F;“)结尾的请求，并生成目录列表。nginx编译的时候会自动加载该模块，但是该模块默认是关闭的，我们需要使用下来指令来完成对应的配置（1）autoindex:启用或禁用目录列表输出 语法 autoindex on&#124;off; 默认值 autoindex off; 位置 http、server、location （2）autoindex_exact_size:对应HTLM格式，指定是否在目录列表展示文件的详细大小默认为on，显示出文件的确切大小，单位是bytes。改为off后，显示出文件的大概大小，单位是kB或者MB或者GB 语法 autoindex_exact_size on&#124;off; 默认值 autoindex_exact_size on; 位置 http、server、location （3）autoindex_format：设置目录列表的格式 语法 autoindex_format html&#124;xml&#124;json&#124;jsonp; 默认值 autoindex_format html; 位置 http、server、location 注意:该指令在1.7.9及以后版本中出现（4）autoindex_localtime:对应HTML格式，是否在目录列表上显示时间。默认为off，显示的文件时间为GMT时间。改为on后，显示的文件时间为文件的服务器时间 语法 autoindex_localtime on &#124; off; 默认值 autoindex_localtime off; 位置 http、server、location 配置方式如下: 1234567location /download&#123; root /usr/local; autoindex on; autoindex_exact_size on; autoindex_format html; autoindex_localtime on;&#125; XML&#x2F;JSON格式[一般不用这两种方式] Nginx的用户认证模块对应系统资源的访问，我们往往需要限制谁能访问，谁不能访问。这块就是我们通常所说的认证部分，认证需要做的就是根据用户输入的用户名和密码来判定用户是否为合法用户，如果是则放行访问，如果不是则拒绝访问。Nginx对应用户认证这块是通过ngx_http_auth_basic_module模块来实现的，它允许通过使用”HTTP基本身份验证”协议验证用户名和密码来限制对资源的访问。默认情况下nginx是已经安装了该模块，如果不需要则使用–without-http_auth_basic_module。该模块的指令比较简单，（1）auth_basic:使用“ HTTP基本认证”协议启用用户名和密码的验证 语法 auth_basic string&#124;off; 默认值 auth_basic off; 位置 http,server,location,limit_except 开启后，服务端会返回401，指定的字符串会返回到客户端，给用户以提示信息，但是不同的浏览器对内容的展示不一致。（2）auth_basic_user_file:指定用户名和密码所在文件 语法 auth_basic_user_file file; 默认值 — 位置 http,server,location,limit_except 指定文件路径，该文件中的用户名和密码的设置，密码需要进行加密。可以采用工具自动生成实现步骤:1.nginx.conf添加如下内容 123456789location /download&#123; root /usr/local; autoindex on; autoindex_exact_size on; autoindex_format html; autoindex_localtime on; auth_basic &#x27;please input your auth&#x27;; auth_basic_user_file htpasswd;&#125; 2.我们需要使用htpasswd工具生成yum install -y httpd-tools 1234htpasswd -c /usr/local/nginx/conf/htpasswd username //创建一个新文件记录用户名和密码htpasswd -b /usr/local/nginx/conf/htpasswd username password //在指定文件新增一个用户名和密码htpasswd -D /usr/local/nginx/conf/htpasswd username //从指定文件删除一个用户信息htpasswd -v /usr/local/nginx/conf/htpasswd username //验证用户名和密码是否正确 上述方式虽然能实现用户名和密码的验证，但是大家也看到了，所有的用户名和密码信息都记录在文件里面，如果用户量过大的话，这种方式就显得有点麻烦了，这时候我们就得通过后台业务代码来进行用户权限的校验了。","categories":[{"name":"中间件","slug":"中间件","permalink":"http://example.com/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[]},{"title":"redis","slug":"redis","date":"2023-07-31T09:30:13.676Z","updated":"2023-07-31T09:30:42.565Z","comments":true,"path":"2023/07/31/redis/","link":"","permalink":"http://example.com/2023/07/31/redis/","excerpt":"","text":"基础篇线上运行网站：https://try.redis.io/ sql和nosqlsql(Structured Query Language)：关系型数据库NoSql（Not Only Sql）： 非关系型数据库 Redis是一种键值型的NoSql数据库键值型： Redis存储数据的方式是 key-value的方式NoSql： 相对于传统关系型数据库而言，有很大差异的一种数据库。 对于存储的数据，没有类似Mysql那么严格的约束，比如唯一性，是否可以为null等等，所以我们把这种松散结构的数据库，称之为NoSQL数据库。 **sql和nosql差异 **差异1 结构化和非结构化 sql是一种结构化查询语言 结构化的意思就是： 需要指定一些格式 例如 MySQL建表语句 指定字段的大小 类型 是否是主键 是非为空等等 nosql他是非结构化 不是没有结构而是说 他对于结构的约束很松散 差异2 关系型和非关系型 另外sql叫关系型数据库 关系说明 数据和数据之间是有关联的 例如外键 多对多需要一个关系表 nosql 而他是非关系型数据库 也就是数据之间没有关系 它使用json文档嵌套的方式 例如一个id为1的用户有一个orders属性 这里记录了id下单的所有商品 虽然没有关系 但是缺点就是 多个用户就会有多个重复的商品123456789101112131415161718&#123; id: 1, name: &quot;张三&quot;, orders: [ &#123; id: 1, item: &#123; id: 10, title: &quot;荣耀6&quot;, price: 4999 &#125; &#125;, &#123; id: 2, item: &#123; id: 20, title: &quot;小米11&quot;, price: 3999 &#125; &#125; ]&#125; 差异3 查询语法不同 sql查询 格式固定 语法固定 因此只要是关系型数据库 可以共用相同的语句 例如：mysql Oracle的查询方式差不多 nosql查询 没有固定的格式 每个nosql数据库的查询语法都不太相同 例如 redis mongoDB elasticsearch查询方式不相同 差异4 事务事务必须满足ACID的特性 原子性一致性隔离性持久性ACID，是指数据库系统管理（DBMS）在写入或更新资料的过程中，为保证事务（transaction）是正确可靠的，所必须具备的四个特性：原子性（atomicity，或称不可分割性）、一致性（consistency）、隔离性（isolation，又称独立性）、持久性（durability）。 对于关系型数据库（sql）来说 底层帮我做了ACID特性 它满足ACID特性 nosql 要么没有事务 要么支持一点 BASE 差异5 存储方式 sql一般是存放在磁盘 nosql一般保存在内存 这就说明了查询效率高 扩展性高 Redis简单介绍Redis诞生于2009年全称是Remote Dictionary Server 远程词典服务器，是一个基于内存的键值型NoSQL数据库。 端口号6379在是手机按键上MERZ对应的号码，而MERZ取自意大利歌女Alessia Merz的名字。MERZ长期以来被Redis作者antirez及其朋友当作愚蠢的代名词。后来Redis作者在开发Redis时就选用了这个端口。特征： 键值（key-value）型，value支持多种不同数据结构，功能丰富 单线程，每个命令具备原子性(这里说的是redis7以前 redis7已经支持多线程了) 低延迟，速度快（基于内存主要原因.IO多路复用.良好的编码）。 支持数据持久化（这是因为 数据存在内存的坏处 一断电就没了 因此做持久化 隔一段时间序列化到磁盘） 支持主从集群.分片集群 支持多语言客户端 作者：AntirezRedis的官方网站地址：https://redis.io/ 序列化 反序列化序列化 (Serialization)是将对象的状态信息转换为可以存储或传输的形式的过程。在序列化期间，对象将其当前状态写入到临时或持久性存储区。以后，可以通过从存储区中读取或反序列化对象的状态，重新创建该对象。 序列化主要有两个用途 把对象的字节序列永久保存到硬盘上，通常存放在一个文件中（序列化对象）在网络上传送对象的字节序列（网络传输对象）实际上就是将数据持久化，防止一直存储在内存当中，消耗内存资源。而且序列化后也能更好的便于网络运输何传播 序列化：将java对象转换为字节序列 存储在磁盘上反序列化：把字节序列回复为原先的java对象 serialVersionUID是序列化前后的唯一标识符默认如果没有人为显式定义过serialVersionUID，那编译器会为它自动声明一个！ serialVersionUID序列化ID，可以看成是序列化和反序列化过程中的“暗号”，在反序列化时，JVM会把字节流中的序列号ID和被序列化类中的序列号ID做比对，只有两者一致，才能重新反序列化，否则就会报异常来终止反序列化的过程。 所以，为了serialVersionUID的确定性，写代码时还是建议，凡是implements Serializable的类，都最好人为显式地为它声明一个serialVersionUID明确值！ 安装redisRedis是基于C语言编写的，因此首先需要安装Redis所需要的gcc依赖：yum install -y gcc tcl 下载安装包 上传到虚拟机中解压：tar -xzf redis-6.2.6.tar.gz进入redis目录：cd redis-6.2.6运行编译命令：make &amp;&amp; make install如果没有出错，应该就安装成功了。 启动：前台启动（不推荐）后台启动修改redis.conf文件中的一些配置： 123456# 允许访问的地址，默认是127.0.0.1，会导致只能在本地访问。修改为0.0.0.0则可以在任意IP访问，生产环境不要设置为0.0.0.0bind 0.0.0.0# 守护进程，修改为yes后即可后台运行daemonize yes # 密码，设置后访问Redis必须输入密码requirepass 123321 Redis的其它常见配置： 12345678910# 监听的端口port 6379# 工作目录，默认是当前目录，也就是运行redis-server时的命令，日志.持久化等文件会保存在这个目录dir .# 数据库数量，设置为1，代表只使用1个库，默认有16个库，编号0~15databases 1# 设置redis能够使用的最大内存maxmemory 512mb# 日志文件，默认为空，不记录日志，可以指定日志文件名logfile &quot;redis.log&quot; 启动Redis： 1234# 进入redis安装目录 cd /usr/local/src/redis-6.2.6# 启动redis-server redis.conf 停止服务： 123# 利用redis-cli来执行 shutdown 命令，即可停止 Redis 服务，# 因为之前配置了密码，因此需要通过 -u 来指定密码redis-cli -u 123321 shutdown 打开redis命令行： 如果设置了密码 两种方式 12345678# 第一种redis-cli # 进入命令行auth 123456 # auth 输入密码 就可以了# 第二种redis-cli -a 123456 # 使用redis-cli -a 密码 这种将密码明文了 因此会有警告# 另外可通过ping命令 来看有没有连上redis 如果连上了 就会返回 pong Key的层级结构Redis没有类似MySQL中的Table的概念，我们该如何区分不同类型的key呢？例如，需要存储用户.商品信息到redis，有一个用户id是1，有一个商品id恰好也是1，此时如果使用id作为key，那就会冲突了，该怎么办？我们可以通过给key添加前缀加以区分，不过这个前缀不是随便加的，有一定的规范：Redis的key允许有多个单词形成层级结构，多个单词之间用’:’隔开例如：我们的项目名称叫 heima，有user和product两种不同类型的数据，我们可以这样定义key： user相关的key：heima:user:1 product相关的key：heima:product:1 如果Value是一个Java对象，例如一个User对象，则可以将对象序列化为JSON字符串后存储： KEY VALUE heima:user:1 {“id”:1, “name”: “Jack”, “age”: 21} heima:product:1 {“id”:1, “name”: “小米11”, “price”: 4999} 一旦我们向redis采用这样的方式存储，那么在可视化界面中，redis会以层级结构来进行存储，形成类似于这样的结构，更加方便Redis获取数据 项目名：业务名：类型：id 具体还是看具体项目要求 redis命令贴心小建议：命令不要死记，学会查询就好啦Redis是一个key-value的数据库，key一般是String类型，不过value的类型多种多样：基本常用：String、Hash、List、Set、SortedSet特殊类型：GEO、BitMap、HyperLogRedis为了方便我们学习，将操作不同数据类型的命令也做了分组，在官网（ https://redis.io/commands ）可以查看到不同的命令 Redis 通用命令通用指令是部分数据类型的，都可以使用的指令，常见的有： KEYS：查看符合模板的所有key 不建议在生产设备上使用 他会阻塞所有请求 DEL：删除一个指定的key(可以是多个) del [keys...] EXISTS：判断key是否存在（可以是多个） exists [keys] EXPIRE：给一个key设置有效期，有效期到期时该key会被自动删除 expire key seconds TTL：查看一个KEY的剩余有效期 ttl key select redis库名 来切换 通过help [command] 可以查看一个命令的具体用法 例如：help keys String命令 最简单且非常常用其value是字符串，不过根据字符串的格式不同，又可以分为3类： string：普通字符串 int：整数类型，可以做自增.自减操作 float：浮点类型，可以做自增.自减操作 String的常见命令有： SET：添加或者修改已经存在的一个String类型的键值对 GET：根据key获取String类型的value MSET：批量添加多个String类型的键值对 MGET：根据多个key获取多个String类型的value INCR：让一个整型的key自增1 INCRBY:让一个整型的key自增并指定步长，例如：incrby num 2 让num值自增2 INCRBYFLOAT：让一个浮点类型的数字自增并指定步长 SETNX：添加一个String类型的键值对，前提是这个key不存在，否则不执行 SETEX：添加一个String类型的键值对，并且指定有效期1234567891011127.0.0.1:6379&gt; set name Rose //原来不存在OK127.0.0.1:6379&gt; get name &quot;Rose&quot;127.0.0.1:6379&gt; set name Jack //原来存在，就是修改OK127.0.0.1:6379&gt; get name&quot;Jack&quot; 123456789127.0.0.1:6379&gt; MSET k1 v1 k2 v2 k3 v3OK127.0.0.1:6379&gt; MGET name age k1 k2 k31) &quot;Jack&quot; //之前存在的name2) &quot;10&quot; //之前存在的age3) &quot;v1&quot;4) &quot;v2&quot;5) &quot;v3&quot; 12345678910111213141516171819202122232425262728127.0.0.1:6379&gt; get age &quot;10&quot;127.0.0.1:6379&gt; incr age //增加1(integer) 11 127.0.0.1:6379&gt; get age //获得age&quot;11&quot;127.0.0.1:6379&gt; incrby age 2 //一次增加2(integer) 13 //返回目前的age的值 127.0.0.1:6379&gt; incrby age 2(integer) 15 127.0.0.1:6379&gt; incrby age -1 //也可以增加负数，相当于减(integer) 14 127.0.0.1:6379&gt; incrby age -2 //一次减少2个(integer) 12 127.0.0.1:6379&gt; DECR age //相当于 incr 负数，减少正常用法(integer) 11 127.0.0.1:6379&gt; get age &quot;11&quot;127.0.0.1:6379&gt; DECRBY age 10//相当于 incr 负数，减少正常用法(integer) 1 123456789101112131415161718127.0.0.1:6379&gt; help setnx SETNX key value summary: Set the value of a key, only if the key does not exist 只有当key不存在时候才会成功 否则报错 since: 1.0.0 group: string127.0.0.1:6379&gt; set name Jack //设置名称OK127.0.0.1:6379&gt; setnx name lisi //如果key不存在，则添加成功(integer) 0127.0.0.1:6379&gt; get name //由于name已经存在，所以lisi的操作失败&quot;Jack&quot;127.0.0.1:6379&gt; setnx name2 lisi //name2 不存在，所以操作成功(integer) 1127.0.0.1:6379&gt; get name2 &quot;lisi&quot; 1234567891011127.0.0.1:6379&gt; setex name 10 jack // 设置一个key的value 并且指定过期时间OK127.0.0.1:6379&gt; ttl name(integer) 8127.0.0.1:6379&gt; ttl name(integer) 7127.0.0.1:6379&gt; ttl name(integer) 5 Hash命令Hash类型，也叫散列，其value是一个无序字典，类似于Java中的HashMap结构。String结构是将对象序列化为JSON字符串后存储，当需要修改对象某个字段时很不方便key: redis:user:1 value：&#123;name:&quot;zhangsan&quot;, age:12&#125;而hash是key: redis:user:1 value: [field:name, value:zhangsan]将value字段从原本json 做了提取 将字段抽取出来作为field 这样你想要对value中的具体数据修改就很方便 更灵活Hash类型的常见命令 HSET key field value：添加或者修改hash类型key的field的值 HGET key field：获取一个hash类型key的field的值 HMSET：批量添加多个hash类型key的field的值 HMGET：批量获取多个hash类型key的field的值 HGETALL：获取一个hash类型的key中的所有的field和value HKEYS：获取一个hash类型的key中的所有的field HINCRBY:让一个hash类型key的字段值自增并指定步长 HSETNX：添加一个hash类型的key的field值，前提是这个field不存在，否则不执行 贴心小提示：哈希结构也是我们以后实际开发中常用的命令哟 12345678910127.0.0.1:6379&gt; HSET heima:user:3 name Lucy//大key是 heima:user:3 小key是name，小value是Lucy(integer) 1127.0.0.1:6379&gt; HSET heima:user:3 age 21// 如果操作不存在的数据，则是新增(integer) 1127.0.0.1:6379&gt; HSET heima:user:3 age 17 //如果操作存在的数据，则是修改(integer) 0127.0.0.1:6379&gt; HGET heima:user:3 name &quot;Lucy&quot;127.0.0.1:6379&gt; HGET heima:user:3 age&quot;17&quot; 12345678127.0.0.1:6379&gt; HMSET heima:user:4 name HanMeiMeiOK127.0.0.1:6379&gt; HMSET heima:user:4 name LiLei age 20 sex manOK127.0.0.1:6379&gt; HMGET heima:user:4 name age sex1) &quot;LiLei&quot;2) &quot;20&quot;3) &quot;man&quot; 1234567127.0.0.1:6379&gt; HGETALL heima:user:4 // 获取key的所有field 和value1) &quot;name&quot;2) &quot;LiLei&quot;3) &quot;age&quot;4) &quot;20&quot;5) &quot;sex&quot;6) &quot;man&quot; 12345678127.0.0.1:6379&gt; HKEYS heima:user:4 // 获取所有的key1) &quot;name&quot;2) &quot;age&quot;3) &quot;sex&quot;127.0.0.1:6379&gt; HVALS heima:user:4 // 获取所有hvals1) &quot;LiLei&quot;2) &quot;20&quot;3) &quot;man&quot; 12345678127.0.0.1:6379&gt; HINCRBY heima:user:4 age 2 // 如果是空默认是0(integer) 22127.0.0.1:6379&gt; HVALS heima:user:41) &quot;LiLei&quot;2) &quot;22&quot;3) &quot;man&quot;127.0.0.1:6379&gt; HINCRBY heima:user:4 age -2(integer) 20 12345678910111213141516127.0.0.1:6379&gt; HSETNX heima:user4 sex woman //为key添加一个哈希类型的field 只有不存在才能创建(integer) 1127.0.0.1:6379&gt; HGETALL heima:user:31) &quot;name&quot;2) &quot;Lucy&quot;3) &quot;age&quot;4) &quot;17&quot;127.0.0.1:6379&gt; HSETNX heima:user:3 sex woman(integer) 1127.0.0.1:6379&gt; HGETALL heima:user:31) &quot;name&quot;2) &quot;Lucy&quot;3) &quot;age&quot;4) &quot;17&quot;5) &quot;sex&quot;6) &quot;woman&quot; List命令Redis中的List类型与Java中的LinkedList类似，可以看做是一个双向链表结构。既可以支持正向检索和也可以支持反向检索。特征也与LinkedList类似： 有序 元素可以重复 插入和删除快 查询速度一般 常用来存储一个有序数据，例如：朋友圈点赞列表，评论列表等。 List的常见命令有： LPUSH key element … ：向列表左侧插入一个或多个元素 left push LPOP key：移除并返回列表左侧的第一个元素，没有则返回nil RPUSH key element … ：向列表右侧插入一个或多个元素 right push RPOP key：移除并返回列表右侧的第一个元素 LRANGE key star end：返回一段角标范围内的所有元素 BLPOP和BRPOP：与LPOP和RPOP类似，只不过在没有元素时等待指定时间，而不是直接返回nil 1234127.0.0.1:6379&gt; LPUSH users 1 2 3 (integer) 3127.0.0.1:6379&gt; RPUSH users 4 5 6(integer) 6 1234127.0.0.1:6379&gt; LPOP users&quot;3&quot;127.0.0.1:6379&gt; RPOP users&quot;6&quot; 123127.0.0.1:6379&gt; LRANGE users 1 21) &quot;1&quot;2) &quot;4&quot; Set命令Redis的Set结构与Java中的HashSet类似，可以看做是一个value为null的HashMap。因为也是一个hash表，因此具备与HashSet类似的特征： 无序 元素不可重复 查找快 支持交集.并集.差集等功能 Set类型的常见命令 SADD key member … ：向set中添加一个或多个元素 SREM key member … : 移除set中的指定元素 SCARD key： 返回set中元素的个数 SISMEMBER key member：判断一个元素是否存在于set中 SMEMBERS：获取set中的所有元素 SINTER key1 key2 … ：求key1与key2的交集 SDIFF key1 key2 … ：求key1与key2的差集 SUNION key1 key2 ..：求key1和key2的并集 1234567891011121314151617127.0.0.1:6379&gt; sadd s1 a b c(integer) 3127.0.0.1:6379&gt; smembers s11) &quot;c&quot;2) &quot;b&quot;3) &quot;a&quot;127.0.0.1:6379&gt; srem s1 a(integer) 1 127.0.0.1:6379&gt; SISMEMBER s1 a(integer) 0 127.0.0.1:6379&gt; SISMEMBER s1 b(integer) 1 127.0.0.1:6379&gt; SCARD s1(integer) 2 SortedSet类型Redis的SortedSet是一个可排序的set集合，与Java中的TreeSet有些类似，但底层数据结构却差别很大。SortedSet中的每一个元素都带有一个score属性，可以基于score属性对元素排序，底层的实现是一个跳表（SkipList）加 hash表。SortedSet具备下列特性： 可排序 元素不重复 查询速度快 因为SortedSet的可排序特性，经常被用来实现排行榜这样的功能。SortedSet的常见命令有： ZADD key score member：添加一个或多个元素到sorted set ，如果已经存在则更新其score值 ZREM key member：删除sorted set中的一个指定元素 ZSCORE key member : 获取sorted set中的指定元素的score值 ZRANK key member：获取sorted set 中的指定元素的排名 ZCARD key：获取sorted set中的元素个数 ZCOUNT key min max：统计score值在给定范围内的所有元素的个数 ZINCRBY key increment member：让sorted set中的指定元素自增，步长为指定的increment值 ZRANGE key min max：按照score排序后，获取指定排名范围内的元素 ZRANGEBYSCORE key min max：按照score排序后，获取指定score范围内的元素 ZDIFF.ZINTER.ZUNION：求差集.交集.并集 注意：所有的排名默认都是升序，如果要降序则在命令的Z后面添加REV即可，例如： 升序获取sorted set 中的指定元素的排名：ZRANK key member 降序获取sorted set 中的指定元素的排名：ZREVRANK key memeber1zadd stu 85 zhangsan 79 lisi Redis的Java客户端jedis12345678910111213&lt;!--jedis--&gt;&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;3.7.0&lt;/version&gt;&lt;/dependency&gt;&lt;!--单元测试--&gt;&lt;dependency&gt; &lt;groupId&gt;org.junit.jupiter&lt;/groupId&gt; &lt;artifactId&gt;junit-jupiter&lt;/artifactId&gt; &lt;version&gt;5.7.0&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt; 12345678910111213141516171819202122232425262728293031323334353637383940private Jedis jedis;@BeforeEachvoid setUp() &#123; // 1.建立连接 // jedis = new Jedis(&quot;192.168.150.101&quot;, 6379); jedis = JedisConnectionFactory.getJedis(); // 2.设置密码 jedis.auth(&quot;123321&quot;); // 3.选择库 jedis.select(0);&#125;@Testvoid testString() &#123; // 存入数据 String result = jedis.set(&quot;name&quot;, &quot;虎哥&quot;); System.out.println(&quot;result = &quot; + result); // 获取数据 String name = jedis.get(&quot;name&quot;); System.out.println(&quot;name = &quot; + name);&#125;@Testvoid testHash() &#123; // 插入hash数据 jedis.hset(&quot;user:1&quot;, &quot;name&quot;, &quot;Jack&quot;); jedis.hset(&quot;user:1&quot;, &quot;age&quot;, &quot;21&quot;); // 获取 Map&lt;String, String&gt; map = jedis.hgetAll(&quot;user:1&quot;); System.out.println(map);&#125;@AfterEachvoid tearDown() &#123; if (jedis != null) &#123; jedis.close(); &#125;&#125; Jedis本身是线程不安全的，并且频繁的创建和销毁连接会有性能损耗，因此我们推荐大家使用Jedis连接池代替Jedis的直连方式有关池化思想，并不仅仅是这里会使用，很多地方都有，比如说我们的数据库连接池，比如我们tomcat中的线程池，这些都是池化思想的体现。 1234567891011121314151617181920public class JedisConnectionFacotry &#123; private static final JedisPool jedisPool; static &#123; //配置连接池 JedisPoolConfig poolConfig = new JedisPoolConfig(); poolConfig.setMaxTotal(8); poolConfig.setMaxIdle(8); poolConfig.setMinIdle(0); poolConfig.setMaxWaitMillis(1000); //创建连接池对象 jedisPool = new JedisPool(poolConfig, &quot;192.168.150.101&quot;,6379,1000,&quot;123321&quot;); &#125; public static Jedis getJedis()&#123; return jedisPool.getResource(); &#125;&#125; 代码说明： 1） JedisConnectionFacotry：工厂设计模式是实际开发中非常常用的一种设计模式，我们可以使用工厂，去降低代的耦合，比如Spring中的Bean的创建，就用到了工厂设计模式 2）静态代码块：随着类的加载而加载，确保只能执行一次，我们在加载当前工厂类的时候，就可以执行static的操作完成对 连接池的初始化 3）最后提供返回连接池中连接的方法.123456789101112131415 @BeforeEach void setUp()&#123; //建立连接 /*jedis = new Jedis(&quot;127.0.0.1&quot;,6379);*/ jedis = JedisConnectionFacotry.getJedis(); //选择库 jedis.select(0); &#125;@AfterEach void tearDown() &#123; if (jedis != null) &#123; jedis.close(); &#125; &#125; SpringDataRedis***SpringData是Spring中数据操作的模块，包含对各种数据库的集成，其中对Redis的集成模块就叫做SpringDataRedis，官网地址：https://spring.io/projects/spring-data-redis 提供了对不同Redis客户端的整合（Lettuce和Jedis） 提供了RedisTemplate统一API来操作Redis 支持Redis的发布订阅模型 支持Redis哨兵和Redis集群 支持基于Lettuce的响应式编程 支持基于JDK.JSON.字符串.Spring对象的数据序列化及反序列化 支持基于Redis的JDKCollection实现 SpringDataRedis中提供了RedisTemplate工具类，其中封装了各种对Redis的操作。并且将不同数据类型的操作API封装到了不同的类型中 12345&lt;!--redis依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; 1234567891011spring: redis: host: 192.168.150.101 port: 6379 password: 123456 lettuce: pool: max-active: 8 #最大连接 max-idle: 8 #最大空闲连接 min-idle: 0 #最小空闲连接 max-wait: 100ms #连接等待时间 123456789101112131415@SpringBootTestclass RedisDemoApplicationTests &#123; @Autowired private RedisTemplate&lt;String, Object&gt; redisTemplate; @Test void testString() &#123; // 写入一条String数据 redisTemplate.opsForValue().set(&quot;name&quot;, &quot;虎哥&quot;); // 获取string数据 Object name = redisTemplate.opsForValue().get(&quot;name&quot;); System.out.println(&quot;name = &quot; + name); &#125;&#125; 贴心小提示：SpringDataRedis使用起来非常简单，记住如下几个步骤即可SpringDataRedis的使用步骤： 引入spring-boot-starter-data-redis依赖 在application.yml配置Redis信息 注入RedisTemplate 12redisTemplate.opsForValue().set(&quot;key&quot;,&quot;value&quot;); // redis会将这两个object 由jdk序列化为redis可以处理的数据（将对象转为字节） RedisTemplate可以接收任意Object作为值写入Redis 只不过写入前会把Object序列化为字节形式，默认是采用JDK序列化，得到的结果是这样的&#x2F;da&#x2F;da&#x2F;ad&#x2F;weq&#x2F;wrq&#x2F;wr&#x2F;qe&#x2F;daname缺点： 可读性差 内存占用较大 我们可以自定义RedisTemplate的序列化方式，代码如下： 12345678910111213141516171819202122@Configurationpublic class RedisConfig &#123; @Bean public RedisTemplate&lt;String, Object&gt; redisTemplate(RedisConnectionFactory connectionFactory)&#123; // 创建RedisTemplate对象 RedisTemplate&lt;String, Object&gt; template = new RedisTemplate&lt;&gt;(); // 设置连接工厂 template.setConnectionFactory(connectionFactory); // 创建JSON序列化工具 GenericJackson2JsonRedisSerializer jsonRedisSerializer = new GenericJackson2JsonRedisSerializer(); // 设置Key的序列化 template.setKeySerializer(RedisSerializer.string()); template.setHashKeySerializer(RedisSerializer.string()); // 设置Value的序列化 template.setValueSerializer(jsonRedisSerializer); template.setHashValueSerializer(jsonRedisSerializer); // 返回 return template; &#125;&#125; 以上也有点麻烦需要自己自定义设置 StringRedisTemplate尽管JSON的序列化方式可以满足我们的需求，但依然存在一些问题为了在反序列化时知道对象的类型，JSON序列化器会将类的class类型写入json结果中，存入Redis，会带来额外的内存开销。为了减少内存的消耗，我们可以采用手动序列化的方式，换句话说，就是不借助默认的序列化器，而是我们自己来控制序列化的动作，同时，我们只采用String的序列化器，这样，在存储value时，我们就不需要在内存中就不用多存储数据，从而节约我们的内存空间 这种用法比较普遍，因此SpringDataRedis就提供了RedisTemplate的子类：StringRedisTemplate，它的key和value的序列化方式默认就是String方式。 12345678910111213141516171819202122232425262728293031323334@SpringBootTestclass RedisStringTests &#123; @Autowired private StringRedisTemplate stringRedisTemplate; @Test void testString() &#123; // 写入一条String数据 stringRedisTemplate.opsForValue().set(&quot;verify:phone:13600527634&quot;, &quot;124143&quot;); // 获取string数据 Object name = stringRedisTemplate.opsForValue().get(&quot;name&quot;); System.out.println(&quot;name = &quot; + name); &#125; private static final ObjectMapper mapper = new ObjectMapper(); @Test void testSaveUser() throws JsonProcessingException &#123; // 创建对象 User user = new User(&quot;虎哥&quot;, 21); // 手动序列化 String json = mapper.writeValueAsString(user); // 写入数据 stringRedisTemplate.opsForValue().set(&quot;user:200&quot;, json); // 获取数据 String jsonUser = stringRedisTemplate.opsForValue().get(&quot;user:200&quot;); // 手动反序列化 User user1 = mapper.readValue(jsonUser, User.class); System.out.println(&quot;user1 = &quot; + user1); &#125;&#125; 最后小总结：RedisTemplate的两种序列化实践方案： 方案一： 自定义RedisTemplate 修改RedisTemplate的序列化器为GenericJackson2JsonRedisSerializer 方案二： 使用StringRedisTemplate 写入Redis时，手动把对象序列化为JSON 读取Redis时，手动把读取到的JSON反序列化为对象 操作hash 12345678910111213141516@SpringBootTestclass RedisStringTests &#123; @Autowired private StringRedisTemplate stringRedisTemplate; @Test void testHash() &#123; stringRedisTemplate.opsForHash().put(&quot;user:400&quot;, &quot;name&quot;, &quot;虎哥&quot;); stringRedisTemplate.opsForHash().put(&quot;user:400&quot;, &quot;age&quot;, &quot;21&quot;); Map&lt;Object, Object&gt; entries = stringRedisTemplate.opsForHash().entries(&quot;user:400&quot;); System.out.println(&quot;entries = &quot; + entries); &#125;&#125; 实战篇 ***登录模拟短信验证码的方式登录ThreadLocal https://blog.csdn.net/u010445301/article/details/111322569他的作用主要是用来做数据隔离，每个线程都有自己的ThreadLocalMap实例变量 这个变量对于其他线程来说是隔离的 也就是每个线程独一份ThreadLocal叫做线程变量，意思是ThreadLocal中填充的变量属于当前线程，该变量对其他线程而言是隔离的，也就是说该变量是当前线程独有的变量。ThreadLocal为变量在每个线程中都创建了一个副本，那么每个线程可以访问自己内部的副本变量。 ThreadLoal 变量，线程局部变量，同一个 ThreadLocal 所包含的对象，在不同的 Thread 中有不同的副本。这里有几点需要注意： 因为每个 Thread 内有自己的实例副本，且该副本只能由当前 Thread 使用。这是也是 ThreadLocal 命名的由来。既然每个 Thread 有自己的实例副本，且其它 Thread 不可访问，那就不存在多线程间共享的问题。ThreadLocal 提供了线程本地的实例。它与普通变量的区别在于，每个使用该变量的线程都会初始化一个完全独立的实例副本。ThreadLocal 变量通常被private static修饰。当一个线程结束时，它所使用的所有 ThreadLocal 相对的实例副本都可被回收。ThreadLocal 适用于每个线程需要自己独立的实例且该实例需要在多个方法中被使用，也即变量在线程间隔离而在方法或类间共享的场景ThreadLocal其实是与线程绑定的一个变量。ThreadLocal和Synchonized都用于解决多线程并发访问。 但是ThreadLocal与synchronized有本质的区别： 1、Synchronized用于线程间的数据共享，而ThreadLocal则用于线程间的数据隔离。 2、Synchronized是利用锁的机制，使变量或代码块在某一时该只能被一个线程访问。而ThreadLocal为每一个线程都提供了变量的副本 ，使得每个线程在某一时间访问到的并不是同一个对象，这样就隔离了多个线程对数据的数据共享。 而Synchronized却正好相反，它用于在多个线程间通信时能够获得数据共享。 一句话理解ThreadLocal，threadlocl是作为当前线程中属性ThreadLocalMap集合中的某一个Entry的key值Entry（threadlocl,value），虽然不同的线程之间threadlocal这个key值是一样，但是不同的线程所拥有的ThreadLocalMap是独一无二的，也就是不同的线程间同一个ThreadLocal（key）对应存储的值(value)不一样，从而到达了线程间变量隔离的目的，但是在同一个线程中这个value变量地址是一样的。 基于session实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748@Override public Result sendCode(String phone, HttpSession session) &#123; if (RegexUtils.isPhoneInvalid(phone)) &#123; return Result.fail(&quot;失败&quot;); &#125; // 模拟生成验证码 String code = RandomUtil.randomNumbers(4); // 保存到session session.setAttribute(&quot;code&quot;, code); log.debug(&quot;验证码发送成功: &#123;&#125;&quot;, code); return Result.ok(); &#125; @Override public Result login(LoginFormDTO loginForm, HttpSession session) &#123; String code = (String) session.getAttribute(&quot;code&quot;); if (RegexUtils.isPhoneInvalid(loginForm.getPhone())) &#123; return Result.fail(&quot;失败&quot;); &#125; if (!loginForm.getCode().equals(code)) &#123; return Result.fail(&quot;失败&quot;); &#125; // 如果正确了 查询数据 LambdaQueryWrapper&lt;User&gt; queryWrapper = new LambdaQueryWrapper&lt;&gt;(); queryWrapper.eq(User::getPhone, loginForm.getPhone()); User user = getOne(queryWrapper); if (user == null) &#123; // 创建用户 user = createUserWithPhone(loginForm.getPhone()); &#125; session.setAttribute(&quot;user&quot;, user); return Result.ok(); &#125; @Override public Result me() &#123; // 从线程中获取对象 UserDTO user = UserHolder.getUser(); return Result.ok(user); &#125; private User createUserWithPhone(String phone) &#123; User user = new User(); user.setPhone(phone); user.setNickName(SystemConstants.USER_NICK_NAME_PREFIX + RandomUtil.randomString(10)); boolean save = this.save(user); return save? user : null; &#125; 编写拦截器 并注册 1234567891011121314151617181920212223public class LoginInterceptor implements HandlerInterceptor &#123; @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception &#123; UserHolder.removeUser(); &#125; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; // 获取session 来判断用户是否登录 可以访问业务 HttpSession session = request.getSession(); User user = (User) session.getAttribute(&quot;user&quot;); if (user == null) &#123; response.setStatus(401); return false; &#125; // 将用户保存到ThreadLacal 保证每一个请求都有他自己的独立线程 UserDTO userDTO = new UserDTO(); BeanUtils.copyProperties(user, userDTO); UserHolder.saveUser(userDTO); // 放行 return true; &#125;&#125; 123456789101112/** * 将拦截器记载MVC配置中 使得它生效 * @author luhumu */@Configurationpublic class MvcConfig implements WebMvcConfigurer &#123; @Override public void addInterceptors(InterceptorRegistry registry) &#123; registry.addInterceptor(new LoginInterceptor()) .excludePathPatterns(&quot;/user/code&quot;, &quot;/user/login&quot;, &quot;/shop/**&quot;, &quot;/blog/hot&quot;, &quot;/shop-type/**&quot;, &quot;voucher/**&quot;);// 放行 &#125;&#125; 温馨小贴士：关于threadlocal如果小伙伴们看过threadLocal的源码，你会发现在threadLocal中，无论是他的put方法和他的get方法， 都是先从获得当前用户的线程，然后从线程中取出线程的成员变量map，只要线程不一样，map就不一样，所以可以通过这种方式来做到线程隔离 基于session的问题核心思路分析：每个tomcat中都有一份属于自己的session,假设用户第一次访问第一台tomcat，并且把自己的信息存放到第一台服务器的session中，但是第二次这个用户访问到了第二台tomcat，那么在第二台服务器上，肯定没有第一台服务器存放的session，所以此时 整个登录拦截功能就会出现问题，我们能如何解决这个问题呢？早期的方案是session拷贝，就是说虽然每个tomcat上都有不同的session，但是每当任意一台服务器的session修改时，都会同步给其他的Tomcat服务器的session，这样的话，就可以实现session的共享了但是这种方案具有两个大问题1、每台服务器中都有完整的一份session数据，服务器压力过大。2、session拷贝数据时，可能会出现延迟所以咱们后来采用的方案都是基于redis来完成，我们把session换成redis，redis数据本身就是共享的，就可以避免session共享的问题了 基于redis实现将原本保存在session中的数据 保存到redis中 注意设置过期时间需要注意的是 redis存放的key需要唯一也就是每个用户的key都不一样 如果key是一样的就会出现问题覆盖的问题 这里使用手机号作为key值 建议加一个盐值作为前缀使用String类型 存储 验证码使用hash类型 存储用户信息 1234567891011121314151617181920212223242526272829303132333435363738394041424344@Resource private StringRedisTemplate stringRedisTemplate; @Override public Result sendCode(String phone, HttpSession session) &#123; if (RegexUtils.isPhoneInvalid(phone)) &#123; return Result.fail(&quot;失败&quot;); &#125; // 模拟生成验证码 String code = RandomUtil.randomNumbers(4); // 保存到redis 一定要设置有效期 stringRedisTemplate.opsForValue().set(LOGIN_CODE_KEY + phone, code, 30, TimeUnit.MINUTES); log.debug(&quot;验证码发送成功: &#123;&#125;&quot;, code); return Result.ok(); &#125; @Override public Result login(LoginFormDTO loginForm, HttpSession session) &#123; String code = stringRedisTemplate.opsForValue().get(LOGIN_CODE_KEY + loginForm.getPhone()); if (RegexUtils.isPhoneInvalid(loginForm.getPhone())) &#123; return Result.fail(&quot;失败&quot;); &#125; if (!loginForm.getCode().equals(code)) &#123; return Result.fail(&quot;失败&quot;); &#125; // 如果正确了 查询数据 LambdaQueryWrapper&lt;User&gt; queryWrapper = new LambdaQueryWrapper&lt;&gt;(); queryWrapper.eq(User::getPhone, loginForm.getPhone()); User user = getOne(queryWrapper); if (user == null) &#123; // 创建用户 user = createUserWithPhone(loginForm.getPhone()); &#125; // 保存到redis // 随机生成token String token = UUID.randomUUID().toString(true); UserDTO userDto = BeanUtil.copyProperties(user, UserDTO.class); Map&lt;String, Object&gt; userMap = BeanUtil.beanToMap(userDto, new HashMap&lt;&gt;(), CopyOptions.create().setIgnoreNullValue(true).setFieldValueEditor((name, value) -&gt; value.toString())); // 使用map直接将对象元素转为hash stringRedisTemplate.opsForHash().putAll(LOGIN_USER_KEY + token, userMap); stringRedisTemplate.expire(LOGIN_USER_KEY + token, 30, TimeUnit.MINUTES); return Result.ok(token); &#125; 12345678910111213141516171819202122@Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; // 从请求头中获取token String token = request.getHeader(&quot;authorization&quot;); if (StringUtils.isEmpty(token)) &#123; response.setStatus(401); return false; &#125; Map&lt;Object, Object&gt; userMap = stringRedisTemplate.opsForHash().entries(LOGIN_USER_KEY + token); // 获取session 来判断用户是否登录 可以访问业务 if (userMap.isEmpty()) &#123; response.setStatus(401); return false; &#125; // 将用户保存到ThreadLacal 保证每一个请求都有他自己的独立线程 UserDTO userDTO = BeanUtil.fillBeanWithMap(userMap, new UserDTO(), false); UserHolder.saveUser(userDTO); // 刷新token有效期 stringRedisTemplate.expire(LOGIN_USER_KEY + token, 30, TimeUnit.MINUTES); // 放行 return true; &#125; 123456789@Resourceprivate StringRedisTemplate stringRedisTemplate;@Overridepublic void addInterceptors(InterceptorRegistry registry) &#123;registry.addInterceptor(new LoginInterceptor()).excludePathPatterns(&quot;/user/code&quot;, &quot;/user/login&quot;, &quot;/shop/**&quot;, &quot;/blog/hot&quot;, &quot;/shop-type/**&quot;, &quot;voucher/**&quot;).order(1);// 放行registry.addInterceptor(new RefreshTokenInterceptor(stringRedisTemplate)).addPathPatterns(&quot;/**&quot;).order(0);&#125; 缓存 ***读写效率高缓存(Cache),就是数据交换的缓冲区,俗称的缓存就是缓冲区内的数据,一般从数据库中获取,存储于本地代码(例如: 12345例1:Static final ConcurrentHashMap&lt;K,V&gt; map = new ConcurrentHashMap&lt;&gt;(); 本地用于高并发例2:static final Cache&lt;K,V&gt; USER_CACHE = CacheBuilder.newBuilder().build(); 用于redis等缓存例3:Static final Map&lt;K,V&gt; map = new HashMap(); 本地缓存 由于其被Static修饰,所以随着类的加载而被加载到内存之中,作为本地缓存,由于其又被final修饰,所以其引用(例3:map)和对象(例3:new HashMap())之间的关系是固定的,不能改变,因此不用担心赋值(&#x3D;)导致缓存失效; 为什么要使用缓存一句话:因为速度快,好用缓存数据存储于代码中,而代码运行在内存中,内存的读写性能远高于磁盘,缓存可以大大降低用户访问并发量带来的服务器读写压力实际开发过程中,企业的数据量,少则几十万,多则几千万,这么大数据量,如果没有缓存来作为”避震器”,系统是几乎撑不住的,所以企业会大量运用到缓存技术;但是缓存也会增加代码复杂度和运营的成本例如：数据的一致性成本 代码维护成本 运维成本 如何使用缓存实际开发中,会构筑多级缓存来使系统运行速度进一步提升,例如:本地缓存与redis中的缓存并发使用浏览器缓存：主要是存在于浏览器端的缓存应用层缓存：可以分为tomcat本地缓存，比如之前提到的map，或者是使用redis作为缓存数据库缓存：在数据库中有一片空间是 buffer pool，增改查数据都会先加载到mysql的缓存中CPU缓存：当代计算机最大的问题是 cpu性能提升了，但内存读写速度没有跟上，所以为了适应当下的情况，增加了cpu的L1，L2，L3级的缓存 添加商户缓存流程：用户发起请求 先从redis中查询 如果没有 从数据库中查询 然后将数据写入redis在返回 下次来可以从redis中取了 但是这里有个问题 如果第一个人来访问从数据库中取一定就会很慢 所以需要预热 写一个定时器每天自己执行 从数据库中加载到redis 1234567891011121314151617181920212223@Resource private StringRedisTemplate stringRedisTemplate; /** * 使用redis缓存来提高查询性能 * @param id * @return */ @Override public Result queryShopById(Long id) &#123; String shopJson = stringRedisTemplate.opsForValue().get(CACHE_SHOP_KEY + id); if (!StrUtil.isNotBlank(shopJson)) &#123; Shop shop = JSONUtil.toBean(shopJson, Shop.class); return Result.ok(shop); &#125; // redis中没有 Shop shop = getById(id); if (shop == null) &#123; return Result.fail(&quot;商铺是空&quot;); &#125; stringRedisTemplate.opsForValue().set(CACHE_SHOP_KEY + id, JSONUtil.toJsonStr(shop)); return Result.ok(shop); &#125; 在查询的时候使用redis使得查询效率提高这里还有问题 对于数据没有添加过期时间 会占用内存 如果访问量小的数据一致存在redis中会有内存浪费的问题 缓存更新策略（为了节约内存） ***缓存更新是redis为了节约内存而设计出来的一个东西，主要是因为内存数据宝贵，当我们向redis插入太多数据，此时就可能会导致缓存中的数据过多，所以redis会对部分数据进行更新，或者把他叫为淘汰更合适。内存淘汰：redis自动进行，当redis内存达到咱们设定的max-memery的时候，会自动触发淘汰机制，淘汰掉一些不重要的数据(可以自己设置策略方式) 数据一致性差超时剔除：当我们给redis设置了过期时间ttl之后，redis会将超时的数据进行删除，方便咱们继续使用缓存 数据一致性一般主动更新：我们可以手动调用方法把缓存删掉，通常用于解决缓存和数据库不一致问题 数据一致性高 成本也高 数据库缓存不一致解决方案：由于我们的缓存的数据源来自于数据库,而数据库的数据是会发生变化的,因此,如果当数据库中数据发生变化,而缓存却没有同步,此时就会有一致性问题存在,其后果是:用户使用缓存中的过时数据,就会产生类似多线程数据安全问题,从而影响业务,产品口碑等;怎么解决呢？有如下几种方案Cache Aside Pattern 人工编码方式：缓存调用者在更新完数据库后再去更新缓存，也称之为双写方案Read&#x2F;Write Through Pattern : 由系统本身完成，数据库与缓存的问题交由系统本身去处理Write Behind Caching Pattern ：调用者只操作缓存，其他线程去异步处理数据库，实现最终一致 数据库和缓存不一致采用什么方案***综合考虑使用方案一（更新数据库的同时更新redis），但是方案一调用者如何处理呢？这里有几个问题操作缓存和数据库时有三个问题需要考虑：如果采用第一个方案，那么假设我们每次操作数据库后，都操作缓存，但是中间如果没有人查询，那么这个更新动作实际上只有最后一次生效，中间的更新动作意义并不大，我们可以把缓存删除，等待再次查询时，将缓存中的数据加载出来 删除缓存还是更新缓存？ 更新缓存：每次更新数据库都更新缓存，无效写操作较多 删除缓存：更新数据库时让缓存失效，查询时再更新缓存 ✔ 如何保证缓存与数据库的操作的同时成功或失败？ 单体系统，将缓存与数据库操作放在一个事务 分布式系统，利用TCC等分布式事务方案 应该具体操作缓存还是操作数据库，我们应当是先操作数据库，再删除缓存，原因在于，如果你选择第一种方案，在两个线程并发来访问时，假设线程1先来，他先把缓存删了，此时线程2过来，他查询缓存数据并不存在，此时他写入缓存，当他写入缓存后，线程1再执行更新动作时，实际上写入的就是旧的数据，新的数据被旧数据覆盖了。 先操作缓存还是先操作数据库？ 以下两种都有线程安全问题 但是第二种发生的机率更低 先删除缓存，再操作数据库 先操作数据库，再删除缓存 ✔ 总结： 低一致性需求：使用redis自带的内存淘汰 高一致性需求： 读（查询）操作： redis中有数据直接返回 没有从数据库中查 写入redis 并且设定超时时间 写（更新）操作 先写数据库 再删除缓存 确保数据库和缓存的原子性 实现商铺和缓存与数据库双写一致核心思路如下：修改ShopController中的业务逻辑，满足下面的需求：根据id查询店铺时，如果缓存未命中，则查询数据库，将数据库结果写入缓存，并设置超时时间 1234567891011121314public Result queryShopById(Long id) &#123; String shopJson = stringRedisTemplate.opsForValue().get(CACHE_SHOP_KEY + id); if (!StrUtil.isNotBlank(shopJson)) &#123; Shop shop = JSONUtil.toBean(shopJson, Shop.class); return Result.ok(shop); &#125; // redis中没有 Shop shop = getById(id); if (shop == null) &#123; return Result.fail(&quot;商铺是空&quot;); &#125; stringRedisTemplate.opsForValue().set(CACHE_SHOP_KEY + id, JSONUtil.toJsonStr(shop), CACHE_SHOP_TTL, TimeUnit.MINUTES); return Result.ok(shop); &#125; 根据id修改店铺时，先修改数据库，再删除缓存 1234567891011121314151617181920212223242526/** * 更新操作 保证数据的一致性 最好先更新数据库在删除redis中的数据 * 这样的发生线程安全的几率更低相比先删缓存在删除数据库来说 * 更新数据库时有有请求也会在redis返回数据此时数据库没有更新完 等到数据库完成了 删除redis的时间很快 相对来说安全 * 为了保证更新数据库和删除redis操作的原子性 因此加上事务 * @param shop * @return */ @Override @Transactional public Result updateShop(Shop shop) &#123; Long id = shop.getId(); if (id == null) &#123; return Result.fail(&quot;失败&quot;); &#125; // 先更新数据库 boolean result = updateById(shop); if (!result) &#123; return Result.fail(&quot;失败&quot;); &#125; // 然后删除redis中的数据 stringRedisTemplate.delete(CACHE_SHOP_KEY + id); // 等下一次访问就会重新获取到新的数据 return null; &#125; 缓存穿透 ***缓存穿透 ：缓存穿透是指客户端请求的数据在缓存中和数据库中都不存在，这样缓存永远不会生效，这些请求都会打到数据库。(一个虚假的id发送 redis没有 数据库没有 返回null 这个时候假设攻击者就有可能并发的发起请求 因为redis没有缓存 因此一定会到数据库查询 就有可能搞崩数据库)常见的解决方案有两种： 缓存空对象 （将空对象也缓存到redis 这样下次来就从redis中取） 优点：实现简单，维护方便 缺点： 额外的内存消耗 （这也可以解决 加一个短期的过期时间） 可能造成短期的不一致 （可能获得短期的不一致 假设用户注册了有这个id了 可能过期时间还在 用户访问还是null 也可以解决 就是新增的时候覆盖原本的redis数据） 布隆过滤 （算法） 优点：内存占用较少，没有多余key 缺点： 实现复杂 存在误判可能 缓存空对象思路分析：当我们客户端访问不存在的数据时，先请求redis，但是此时redis中没有数据，此时会访问到数据库，但是数据库中也没有数据，这个数据穿透了缓存，直击数据库，我们都知道数据库能够承载的并发不如redis这么高，如果大量的请求同时过来访问这种不存在的数据，这些请求就都会访问到数据库，简单的解决方案就是哪怕这个数据在数据库中也不存在，我们也把这个数据存入到redis中去，这样，下次用户过来访问这个不存在的数据，那么在redis中也能找到这个数据就不会进入到缓存了布隆过滤：布隆过滤器其实采用的是哈希思想来解决这个问题，通过一个庞大的二进制数组，走哈希思想去判断当前这个要查询的这个数据是否存在，如果布隆过滤器判断存在，则放行，这个请求会去访问redis，哪怕此时redis中的数据过期了，但是数据库中一定存在这个数据，在数据库中查询出来这个数据后，再将其放入到redis中，假设布隆过滤器判断这个数据不存在，则直接返回这种方式优点在于节约内存空间，存在误判，误判原因在于：布隆过滤器走的是哈希思想，只要哈希思想，就可能存在哈希冲突 他的判断并不是很准确 **因此解决缓存穿透使用的是： **缓存空对象 （将空对象也缓存到redis 这样下次来就从redis中取） 编码解决商品查询的缓存穿透问题核心思路如下：在原来的逻辑中，我们如果发现这个数据在mysql中不存在，直接就返回404了，这样是会存在缓存穿透问题的现在的逻辑中：如果这个数据不存在，我们不会返回404 ，还是会把这个数据写入到Redis中，并且将value设置为空，欧当再次发起查询时，我们如果发现命中之后，判断这个value是否是null，如果是null，则是之前写入的数据，证明是缓存穿透数据，如果不是，则直接返回数据。 12345678910111213141516171819202122232425@Override public Result queryShopById(Long id) &#123; // 先从数据库中获取 String shopJson = stringRedisTemplate.opsForValue().get(CACHE_SHOP_KEY + id); // 判断是否有值 if (StrUtil.isNotBlank(shopJson)) &#123; Shop shop = JSONUtil.toBean(shopJson, Shop.class); return Result.ok(shop); &#125; // 没有值的情况下 分为两种 以上isNotBlank()方法 null &quot;&quot; \\t\\n 等等都是false // 我们需要的判断从redis中取到的是不是 &quot;&quot; 如果是 说明这是我们为了防止缓存穿透设置的空值 不需要查询数据库 直接返回 if (shopJson != null) &#123; // 不等于null 就一定是&quot;&quot; return Result.ok(&quot;null&quot;); &#125; // redis中没有命中 查询的是null Shop shop = getById(id); if (shop == null) &#123; // 为了防止缓存穿透 如果数据库也是null 将null也缓存在redis中 必须家过期时间 stringRedisTemplate.opsForValue().set(CACHE_SHOP_KEY + id, &quot;&quot;, 2, TimeUnit.MINUTES); return Result.ok(null); &#125; stringRedisTemplate.opsForValue().set(CACHE_SHOP_KEY + id, JSONUtil.toJsonStr(shop), CACHE_SHOP_TTL, TimeUnit.MINUTES); return Result.ok(shop); &#125; 小总结：缓存穿透产生的原因是什么？ 用户请求的数据在缓存中和数据库中都不存在，不断发起这样的请求，给数据库带来巨大压力 缓存穿透的解决方案有哪些？ 缓存null值 布隆过滤 增强id的复杂度，避免被猜测id规律 (他猜不出来具体id 在查询前就被拦截了) 做好数据的基础格式校验 加强用户权限校验 做好热点参数的限流 缓存雪崩问题及解决思路 ***缓存雪崩是指在同一时段大量的缓存key同时失效或者Redis服务宕机（OMG），导致大量请求到达数据库，带来巨大压力。 解决方案： 给不同的Key的TTL添加随机值 （项目做预热的时候批量的把数据库的值存入redis 这就会使得一瞬间大量key过期 因此加个随机值使得过期时间分散在多个时间端内 减少多个key同时过期的情况） 这是解决多个key同时失效的问题 利用Redis集群提高服务的可用性 （说白了 就是为了避免redis的宕机 使用redis的集群 哨兵机制 主redis宕机了 从redis中找出一个代替） 给缓存业务添加降级限流策略 （如果出现了不可抗力原因 那这个时候没办法了 只能降级 保护数据库做一些限流 防止数据库寄了） 给业务添加多级缓存（多个层面做缓存 例如说 浏览器 代理服务器等等） 缓存击穿 ***缓存击穿 也叫热点key问题 一个被高并发访问（例如一个热点商品）并且缓存重建业务较复杂的key（这个数据可能是从多个数据库中获取合成的数据）突然失效，无数的请求访问会瞬间给数据库带来巨大压力 如何解决： 互斥锁 （既然无数请求进来 那我们加一个锁 只能让一个请求查询数据库 然后将写入缓存 其他请求等一会从新获取 从缓存取） 性能会有所下降 逻辑过期（不设置过期时间 手动写入一个过期字段 查询缓存 如果逻辑时间过期了 获取互斥锁 开启一个新线程去获取数据库的数据 重新写入缓存 在另一个线程去数据库处理的时候 我直接返回旧数据 只是逻辑过期了 但是能用 如果在这期间有一个新的请求进来 发现互斥锁有人占用了 那就直接返回旧数据） 数据会不一致 他们都是用来解决在缓存重构时期的并发问题 加锁逻辑分析：假设线程1在查询缓存之后，本来应该去查询数据库，然后把这个数据重新加载到缓存的，此时只要线程1走完这个逻辑，其他线程就都能从缓存中加载这些数据了，但是假设在线程1没有走完的时候，后续的线程2，线程3，线程4同时过来访问当前这个方法， 那么这些线程都不能从缓存中查询到数据，那么他们就会同一时刻来访问查询缓存，都没查到，接着同一时间去访问数据库，同时的去执行数据库代码，对数据库访问压力过大 没有优劣之分 只是看你更看重什么解决方案一、使用锁来解决：（实现起来非常简单） （保证了一致性 但是可用性下降）因为锁能实现互斥性。假设线程过来，只能一个人一个人的来访问数据库，从而避免对于数据库访问压力过大，但这也会影响查询的性能，因为此时会让查询的性能从并行变成了串行，我们可以采用tryLock方法 + double check来解决这样的问题。假设现在线程1过来访问，他查询缓存没有命中，但是此时他获得到了锁的资源，那么线程1就会一个人去执行逻辑，假设现在线程2过来，线程2在执行过程中，并没有获得到锁，那么线程2就可以进行到休眠，直到线程1把锁释放后，线程2获得到锁，然后再来执行逻辑，此时就能够从缓存中拿到数据了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081public Result queryShopById(Long id) &#123; // 缓存穿透 // Shop shop = queryWithPassThrough(id); // 互斥锁解决缓存击穿 Shop shop = queryWithMutex(id); if (shop == null) &#123; return Result.fail(&quot;null&quot;); &#125; return Result.ok(shop); &#125; /** * 自定义锁 获取互斥锁 使用的是redis的string类型的 setnx 只有这个key值没有的时候才能获取 * * @param key * @return */ public boolean tryLock(String key) &#123; // 为了防止突然的情况（例如说宕机了，请求挂了等等）锁没有释放 又没有处理完 这个时候别人没办法获取到锁 因此堵塞了 // 加上锁的过期时间 // 如果拉屎的人 拉到一般寄了 别人怎么拉 只能无限等待？ // 另外防止执行的时候网络延迟 最好把过期时间放长一点 不然执行到一半 别的锁进来了 拉的一半 门开了 // 或者写一个锁的续期 Boolean lock = stringRedisTemplate.opsForValue().setIfAbsent(key, &quot;1&quot;, 10, TimeUnit.SECONDS); return BooleanUtil.isTrue(lock); &#125; /** * 释放锁 也就是把锁删除 让别的线程可以获取或者说设置 * * @param key */ public void unLock(String key) &#123; stringRedisTemplate.delete(key); &#125; /** * 缓存击穿的方式 * * @param id * @return */ private Shop queryWithMutex(Long id) &#123; // 先从数据库中获取 String shopJson = stringRedisTemplate.opsForValue().get(CACHE_SHOP_KEY + id); // 判断是否有值 if (StrUtil.isNotBlank(shopJson)) &#123; Shop shop = JSONUtil.toBean(shopJson, Shop.class); return shop; &#125; if (shopJson != null) &#123; return null; &#125; // redis中没有 尝试获取锁 Shop shop = null; try &#123; // 没有值的情况下 redis没有值 那这个时候就需要去获取锁 if (!tryLock(LOCK_SHOP_KEY + id)) &#123; // 没有获取到有人占用了 休眠 Thread.sleep(50); //重新再次发送请求 return queryWithMutex(id); &#125; // 获取到了 去数据库查询 并且写入数据库 shop = getById(id); // 模拟延迟 Thread.sleep(200); if (shop == null) &#123; // 为了防止缓存穿透 如果数据库也是null 将null也缓存在redis中 必须家过期时间 stringRedisTemplate.opsForValue().set(CACHE_SHOP_KEY + id, &quot;&quot;, 2, TimeUnit.MINUTES); return null; &#125; stringRedisTemplate.opsForValue().set(CACHE_SHOP_KEY + id, JSONUtil.toJsonStr(shop), CACHE_SHOP_TTL, TimeUnit.MINUTES); &#125; catch (InterruptedException e) &#123; throw new RuntimeException(e); &#125; finally &#123; unLock(LOCK_SHOP_KEY + id); &#125; return shop; &#125; 解决方案二、逻辑过期方案 （保证了可用性 但是不保证一致性）方案分析：我们之所以会出现这个缓存击穿问题，主要原因是在于我们对key设置了过期时间，假设我们不设置过期时间，其实就不会有缓存击穿的问题，但是不设置过期时间，这样数据不就一直占用我们内存了吗，我们可以采用逻辑过期方案。我们把过期时间设置在 redis的value中，注意：这个过期时间并不会直接作用于redis，而是我们后续通过逻辑去处理。假设线程1去查询缓存，然后从value中判断出来当前的数据已经过期了，此时线程1去获得互斥锁，那么其他线程会进行阻塞，获得了锁的线程他会开启一个 线程去进行 以前的重构数据的逻辑，直到新开的线程完成这个逻辑后，才释放锁， 而线程1直接进行返回，假设现在线程3过来访问，由于线程线程2持有着锁，所以线程3无法获得锁，线程3也直接返回数据，只有等到新开的线程2把重建数据构建完后，其他线程才能走返回正确的数据。这种方案巧妙在于，异步的构建缓存，缺点在于在构建完缓存之前，返回的都是脏数据。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192/** * 将数据加入redis并且加上逻辑过期时间 使用逻辑过期方案解决缓存击穿 * @param id * @param expireSeconds */ private void saveShopExpireRedis(Long id, Long expireSeconds) &#123; Shop shop = getById(id); RedisData redisData = new RedisData(); redisData.setExpireTime(LocalDateTime.now().plusSeconds(expireSeconds)); redisData.setData(shop); // 写入redis stringRedisTemplate.opsForValue().set(CACHE_SHOP_KEY + id, JSONUtil.toJsonStr(redisData)); &#125; /** * 自定义锁 获取互斥锁 使用的是redis的string类型的 setnx 只有这个key值没有的时候才能获取 * * @param key * @return */ private boolean tryLock(String key) &#123; // 为了防止突然的情况（例如说宕机了，请求挂了等等）锁没有释放 又没有处理完 这个时候别人没办法获取到锁 因此堵塞了 // 加上锁的过期时间 // 如果拉屎的人 拉到一般寄了 别人怎么拉 只能无限等待？ // 另外防止执行的时候网络延迟 最好把过期时间放长一点 不然执行到一半 别的锁进来了 拉的一半 门开了 // 或者写一个锁的续期 Boolean lock = stringRedisTemplate.opsForValue().setIfAbsent(key, &quot;1&quot;, 10, TimeUnit.SECONDS); return BooleanUtil.isTrue(lock); &#125; /** * 释放锁 也就是把锁删除 让别的线程可以获取或者说设置 * * @param key */ public void unLock(String key) &#123; stringRedisTemplate.delete(key); &#125; private static final ExecutorService EXECUTOR_SERVICE = Executors.newFixedThreadPool(10); /** * 逻辑过期解决缓存击穿 * @param id * @return */ private Shop queryWithLogicalExpire(Long id) &#123; // 先从数据库中获取 String shopJson = stringRedisTemplate.opsForValue().get(CACHE_SHOP_KEY + id); // 判断是否有值 // 理论上讲这个key是不会过期的因此没有指定ttl 只要这个key添加到了缓存里面 一定会永久存在 // 除非活动结束 人工删除 if (StrUtil.isBlank(shopJson)) &#123; // 没有命中 说明没有数据说明数据库也没有 // 这样的热点key 都会做一些预热 在那时候就设置了逻辑过期时间 // 如果redis中没获取到 说明它不属于是热点key return null; &#125; // 命中 判断是否expire值过期 RedisData redisData = JSONUtil.toBean(shopJson, RedisData.class); JSONObject data = (JSONObject) redisData.getData(); Shop shop = JSONUtil.toBean(data, Shop.class); // 现在时间比过期时间大 说明过期了 反之没过期 if (redisData.getExpireTime().isAfter(LocalDateTime.now())) &#123; // 没过期 return shop; &#125; // 过期了 需要重建 // 先获取锁 获取到了进行重构 没有获取到返回假数据 if (tryLock(LOCK_SHOP_KEY + id)) &#123; // 这里需要判断再次判断redis是否有缓存了 // 如果线程b检查完过期时间后，线程a刚好重建完成并释放锁，此时线程b可以拿到锁并再次重建 String shopJson2 = stringRedisTemplate.opsForValue().get(CACHE_SHOP_KEY + id); // 判断是否有值 if (StrUtil.isNotBlank(shopJson2)) &#123; // 没有命中 说明没有数据直接返回 RedisData redisData2 = JSONUtil.toBean(shopJson, RedisData.class); JSONObject data2 = (JSONObject) redisData2.getData(); Shop shop2 = JSONUtil.toBean(data2, Shop.class); return shop2; &#125; // 获取到锁 进行重构 需要开启独立线程 使用线程池 EXECUTOR_SERVICE.submit(() -&gt; &#123; try &#123; saveShopExpireRedis(id, 1800L); &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; finally &#123; unLock(LOCK_SHOP_KEY + id); &#125; &#125;); &#125; return shop; &#125; 封装Redis工具类基于StringRedisTemplate封装一个缓存工具类，满足下列需求： 方法1：将任意Java对象序列化为json并存储在string类型的key中，并且可以设置TTL过期时间 方法2：将任意Java对象序列化为json并存储在string类型的key中，并且可以设置逻辑过期时间，用于处理缓 存击穿问题 方法3：根据指定的key查询缓存，并反序列化为指定类型，利用缓存空值的方式解决缓存穿透问题 方法4：根据指定的key查询缓存，并反序列化为指定类型，需要利用逻辑过期解决缓存击穿问题 将逻辑进行封装 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170@Slf4j@Componentpublic class CacheClient &#123; private final StringRedisTemplate stringRedisTemplate; private static final ExecutorService CACHE_REBUILD_EXECUTOR = Executors.newFixedThreadPool(10); public CacheClient(StringRedisTemplate stringRedisTemplate) &#123; this.stringRedisTemplate = stringRedisTemplate; &#125; // 工具直接序列化为json对象存入 public void set(String key, Object value, Long time, TimeUnit unit) &#123; stringRedisTemplate.opsForValue().set(key, JSONUtil.toJsonStr(value), time, unit); &#125; // 设置逻辑过期时间 public void setWithLogicalExpire(String key, Object value, Long time, TimeUnit unit) &#123; // 设置逻辑过期 RedisData redisData = new RedisData(); redisData.setData(value); redisData.setExpireTime(LocalDateTime.now().plusSeconds(unit.toSeconds(time))); // 写入Redis stringRedisTemplate.opsForValue().set(key, JSONUtil.toJsonStr(redisData)); &#125; // 缓存穿透 public &lt;R,ID&gt; R queryWithPassThrough( String keyPrefix, ID id, Class&lt;R&gt; type, Function&lt;ID, R&gt; dbFallback, Long time, TimeUnit unit)&#123; String key = keyPrefix + id; // 1.从redis查询商铺缓存 String json = stringRedisTemplate.opsForValue().get(key); // 2.判断是否存在 if (StrUtil.isNotBlank(json)) &#123; // 3.存在，直接返回 return JSONUtil.toBean(json, type); &#125; // 判断命中的是否是空值 if (json != null) &#123; // 返回一个错误信息 return null; &#125; // 4.不存在，根据id查询数据库 R r = dbFallback.apply(id); // 5.不存在，返回错误 if (r == null) &#123; // 将空值写入redis stringRedisTemplate.opsForValue().set(key, &quot;&quot;, CACHE_NULL_TTL, TimeUnit.MINUTES); // 返回错误信息 return null; &#125; // 6.存在，写入redis this.set(key, r, time, unit); return r; &#125; /** * * @param keyPrefix * @param id * @param type * @param dbFallback 指定一个函数 因为需要使用mybatisplus去查询 但是工具类不知道使用哪一个来查询 需要调用者自己去指定 * @param time * @param unit * @return * @param &lt;R&gt; * @param &lt;ID&gt; */ // 逻辑过期解决缓存击穿 public &lt;R, ID&gt; R queryWithLogicalExpire( String keyPrefix, ID id, Class&lt;R&gt; type, Function&lt;ID, R&gt; dbFallback, Long time, TimeUnit unit) &#123; String key = keyPrefix + id; // 1.从redis查询商铺缓存 String json = stringRedisTemplate.opsForValue().get(key); // 2.判断是否存在 if (StrUtil.isBlank(json)) &#123; // 3.存在，直接返回 return null; &#125; // 4.命中，需要先把json反序列化为对象 RedisData redisData = JSONUtil.toBean(json, RedisData.class); R r = JSONUtil.toBean((JSONObject) redisData.getData(), type); LocalDateTime expireTime = redisData.getExpireTime(); // 5.判断是否过期 if(expireTime.isAfter(LocalDateTime.now())) &#123; // 5.1.未过期，直接返回店铺信息 return r; &#125; // 5.2.已过期，需要缓存重建 // 6.缓存重建 // 6.1.获取互斥锁 String lockKey = LOCK_SHOP_KEY + id; boolean isLock = tryLock(lockKey); // 6.2.判断是否获取锁成功 if (isLock)&#123; // 6.3.成功，开启独立线程，实现缓存重建 CACHE_REBUILD_EXECUTOR.submit(() -&gt; &#123; try &#123; // 查询数据库 R newR = dbFallback.apply(id); // 重建缓存 this.setWithLogicalExpire(key, newR, time, unit); &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125;finally &#123; // 释放锁 unlock(lockKey); &#125; &#125;); &#125; // 6.4.返回过期的商铺信息 return r; &#125; // 互斥锁解决缓存击穿 public &lt;R, ID&gt; R queryWithMutex( String keyPrefix, ID id, Class&lt;R&gt; type, Function&lt;ID, R&gt; dbFallback, Long time, TimeUnit unit) &#123; String key = keyPrefix + id; // 1.从redis查询商铺缓存 String shopJson = stringRedisTemplate.opsForValue().get(key); // 2.判断是否存在 if (StrUtil.isNotBlank(shopJson)) &#123; // 3.存在，直接返回 return JSONUtil.toBean(shopJson, type); &#125; // 判断命中的是否是空值 if (shopJson != null) &#123; // 返回一个错误信息 return null; &#125; // 4.实现缓存重建 // 4.1.获取互斥锁 String lockKey = LOCK_SHOP_KEY + id; R r = null; try &#123; boolean isLock = tryLock(lockKey); // 4.2.判断是否获取成功 if (!isLock) &#123; // 4.3.获取锁失败，休眠并重试 Thread.sleep(50); return queryWithMutex(keyPrefix, id, type, dbFallback, time, unit); &#125; // 4.4.获取锁成功，根据id查询数据库 r = dbFallback.apply(id); // 5.不存在，返回错误 if (r == null) &#123; // 将空值写入redis stringRedisTemplate.opsForValue().set(key, &quot;&quot;, CACHE_NULL_TTL, TimeUnit.MINUTES); // 返回错误信息 return null; &#125; // 6.存在，写入redis this.set(key, r, time, unit); &#125; catch (InterruptedException e) &#123; throw new RuntimeException(e); &#125;finally &#123; // 7.释放锁 unlock(lockKey); &#125; // 8.返回 return r; &#125; private boolean tryLock(String key) &#123; Boolean flag = stringRedisTemplate.opsForValue().setIfAbsent(key, &quot;1&quot;, 10, TimeUnit.SECONDS); return BooleanUtil.isTrue(flag); &#125; private void unlock(String key) &#123; stringRedisTemplate.delete(key); &#125;&#125; 优惠券秒杀 ***全局ID生成器当用户抢购时，就会生成订单并保存到tb_voucher_order这张表中，而订单表如果使用数据库自增ID就存在一些问题： id的规律性太明显 受单表数据量的限制 场景分析：如果我们的id具有太明显的规则，用户或者说商业对手很容易猜测出来我们的一些敏感信息，比如商城在一天时间内，卖出了多少单，这明显不合适。场景分析二：随着我们商城规模越来越大，mysql的单表的容量不宜超过500W，数据量过大之后，我们要进行拆库拆表，但拆分表了之后，他们从逻辑上讲他们是同一张表，所以他们的id是不能一样的， 于是乎我们需要保证id的唯一性。所以我们需要指定可以自增id 又要保证他自增的不让人看出来规律 保证唯一 安全全局ID生成器，是一种在分布式系统下用来生成全局唯一ID的工具，一般要满足下列特性： 唯一性 高可用 高性能 递增性 安全性 为了增加ID的安全性，我们可以不直接使用Redis自增的数值，而是拼接一些其它信息ID的组成部分：符号位：1bit，永远为0时间戳：31bit，以秒为单位，可以使用69年序列号：32bit，秒内的计数器，支持每秒产生2^32个不同ID 这种用redis生成全局ID生成器的方案 并不是唯一的 当然也可以使用其他工具 UUID （不满足以上五种特性） redis自增 snowflake算法 数据库自增 redis实现生成全局ID生成器： 核心是 时间戳+自增id另外注意 这里使用的是 每天一个key 方便统计订单量 也防止了32位自增id溢出的情况 123456789101112131415161718192021222324252627282930313233@Componentpublic class RedisIdWorker &#123; /** * 开始时间戳 */ private static final long BEGIN_TIMESTAMP = 1640995200L; /** * 序列号的位数 */ private static final int COUNT_BITS = 32; private StringRedisTemplate stringRedisTemplate; public RedisIdWorker(StringRedisTemplate stringRedisTemplate) &#123; this.stringRedisTemplate = stringRedisTemplate; &#125; public long nextId(String keyPrefix) &#123; // 1.生成时间戳 LocalDateTime now = LocalDateTime.now(); long nowSecond = now.toEpochSecond(ZoneOffset.UTC); long timestamp = nowSecond - BEGIN_TIMESTAMP; // 2.生成序列号 // 2.1.获取当前日期，精确到天 String date = now.format(DateTimeFormatter.ofPattern(&quot;yyyy:MM:dd&quot;)); // 2.2.自增长 因为序列号这里是32位（也就是可以有2^32个id） 有可能随着时间会被超过 因此指定时间 每天的就不会超过32位了 long count = stringRedisTemplate.opsForValue().increment(&quot;icr:&quot; + keyPrefix + &quot;:&quot; + date); // 3.拼接并返回 位运算拼接 return timestamp &lt;&lt; COUNT_BITS | count; // timestamp 向左移动32位 使用或|运算 有一个为真就是真 也就是count是说明就填什么 &#125;&#125; 测试类知识小贴士：关于countdownlatchcountdownlatch名为信号枪：主要的作用是同步协调在多线程的等待于唤醒问题我们如果没有CountDownLatch ，那么由于程序是异步的，当异步程序没有执行完时，主线程就已经执行完了，然后我们期望的是分线程全部走完之后，主线程再走，所以我们此时需要使用到CountDownLatchCountDownLatch 中有两个最重要的方法1、countDown2、awaitawait 方法 是阻塞方法，我们担心分线程没有执行完时，main线程就先执行，所以使用await可以让main线程阻塞，那么什么时候main线程不再阻塞呢？当CountDownLatch 内部维护的 变量变为0时，就不再阻塞，直接放行，那么什么时候CountDownLatch 维护的变量变为0 呢，我们只需要调用一次countDown ，内部变量就减少1，我们让分线程和变量绑定， 执行完一个分线程就减少一个变量，当分线程全部走完，CountDownLatch 维护的变量就是0，此时await就不再阻塞，统计出来的时间也就是所有分线程执行完后的时间。 12345678910111213141516171819@Testvoid testIdWorker() throws InterruptedException &#123;CountDownLatch latch = new CountDownLatch(300);Runnable task = () -&gt; &#123; for (int i = 0; i &lt; 100; i++) &#123; long id = redisIdWorker.nextId(&quot;order&quot;); System.out.println(&quot;id = &quot; + id); &#125; latch.countDown();&#125;;long begin = System.currentTimeMillis();for (int i = 0; i &lt; 300; i++) &#123; es.submit(task);&#125;latch.await();long end = System.currentTimeMillis();System.out.println(&quot;time = &quot; + (end - begin));&#125; 添加优惠卷每个店铺都可以发布优惠券，分为平价券和特价券。平价券可以任意购买，而特价券需要秒杀抢购：tb_voucher：优惠券的基本信息，优惠金额、使用规则等tb_seckill_voucher：优惠券的库存、开始抢购时间，结束抢购时间。特价优惠券才需要填写这些信息平价卷由于优惠力度并不是很大，所以是可以任意领取而代金券由于优惠力度大，所以像第二种卷，就得限制数量，从表结构上也能看出，特价卷除了具有优惠卷的基本信息以外，还具有库存，抢购时间，结束时间等等字段特价券是我们需要注意的 他需要考虑多并发抢购有限商品的问题 也就是秒杀**新增普通卷代码： **VoucherController 12345@PostMappingpublic Result addVoucher(@RequestBody Voucher voucher) &#123; voucherService.save(voucher); return Result.ok(voucher.getId());&#125; 新增秒杀卷代码：VoucherController 12345@PostMapping(&quot;seckill&quot;)public Result addSeckillVoucher(@RequestBody Voucher voucher) &#123; voucherService.addSeckillVoucher(voucher); return Result.ok(voucher.getId());&#125; VoucherServiceImpl 123456789101112131415@Override@Transactionalpublic void addSeckillVoucher(Voucher voucher) &#123; // 保存优惠券 save(voucher); // 保存秒杀信息 SeckillVoucher seckillVoucher = new SeckillVoucher(); seckillVoucher.setVoucherId(voucher.getId()); seckillVoucher.setStock(voucher.getStock()); seckillVoucher.setBeginTime(voucher.getBeginTime()); seckillVoucher.setEndTime(voucher.getEndTime()); seckillVoucherService.save(seckillVoucher); // 保存秒杀库存到Redis中 stringRedisTemplate.opsForValue().set(SECKILL_STOCK_KEY + voucher.getId(), voucher.getStock().toString());&#125; 实现秒杀下单 **秒杀下单应该思考的内容：下单时需要判断两点： 秒杀是否开始或结束，如果尚未开始或已经结束则无法下单 库存是否充足，不足则无法下单 （这里要考虑并发问题 需要加锁的问题） 下单核心逻辑分析：当用户开始进行下单，我们应当去查询优惠卷信息，查询到优惠卷信息，判断是否满足秒杀条件比如时间是否充足，如果时间充足，则进一步判断库存是否足够，如果两者都满足，则扣减库存，创建订单，然后返回订单id，如果有一个条件不满足则直接结束。 123456789101112131415161718192021222324252627282930313233343536373839404142434445@Resource private ISeckillVoucherService iSeckillVoucherService; @Resource private RedisIdWorker redisIdWorker; @Override @Transactional public Result seckillVoucher(Long voucherId) &#123; // 获取又优惠券信息 SeckillVoucher voucher = iSeckillVoucherService.getById(voucherId); if (voucher == null) &#123; return Result.fail(&quot;无此优惠券&quot;); &#125; // 有优惠券 判断抢购时间是否开始 if (voucher.getBeginTime().isAfter(LocalDateTime.now())) &#123; // 没有开始 return Result.fail(&quot;失败&quot;); &#125; // 判断秒杀是否已经结束 if (voucher.getEndTime().isBefore(LocalDateTime.now())) &#123; return Result.fail(&quot;失败&quot;); &#125; // 可以抢购 判断库足是否充足 需要从数据库中查 Integer stock = voucher.getStock(); if (stock &lt; 1) &#123; return Result.fail(&quot;失败&quot;); &#125; // 库存减一 boolean success = iSeckillVoucherService.update() .setSql(&quot;stock = stock - 1&quot;) .eq(&quot;voucher_id&quot;, voucherId).update(); if (!success) &#123; return Result.fail(&quot;error&quot;); &#125; // 创建订单 加入到订单表中 VoucherOrder voucherOrder = new VoucherOrder(); // 需要有订单id 这个id需要是全局唯一的 long orderId = redisIdWorker.nextId(&quot;order&quot;); voucherOrder.setId(orderId); // 用户id 代金券id voucherOrder.setVoucherId(voucherId); UserDTO user = UserHolder.getUser(); voucherOrder.setUserId(user.getId()); save(voucherOrder); return Result.ok(orderId); &#125; 以上的问题：假设多线程并发 有可能会在更新数据库 执行库存减一的时候进来用户 假设最后一张优惠券 更新数据库之前 有用户进来 获取了数据库的值 显示还有一张这就会有超卖的问题 库存超卖问题分析 ***有关超卖问题分析：在我们原有代码中是这么写的 123456789101112if (voucher.getStock() &lt; 1) &#123; // 库存不足 return Result.fail(&quot;库存不足！&quot;); &#125; //5，扣减库存 boolean success = seckillVoucherService.update() .setSql(&quot;stock= stock -1&quot;) .eq(&quot;voucher_id&quot;, voucherId).update(); if (!success) &#123; //扣减库存 return Result.fail(&quot;库存不足！&quot;); &#125; 假设线程1过来查询库存，判断出来库存大于1，正准备去扣减库存，但是还没有来得及去扣减，此时线程2过来，线程2也去查询库存，发现这个数量一定也大于1，那么这两个线程都会去扣减库存，最终多个线程相当于一起去扣减库存，此时就会出现库存的超卖问题。超卖问题是典型的多线程安全问题，针对这一问题的常见解决方案就是加锁：而对于加锁，我们通常有两种解决方案： 悲观锁：(认为线程安全问题一定会发生 因此在操作数据前先获取锁 性能不是很好 不适合高并发问题)悲观锁可以实现对于数据的串行化执行，比如syn，和lock都是悲观锁的代表，同时，悲观锁中又可以再细分为公平锁，非公平锁，可重入锁，等等实现直接方法加锁 每次只允许一个线程进入乐观锁：（认为线程安全问题不一定会发生 因此不加锁 他会在数据做更新时再去判断是否有其他线程对数据做了修改 如果没有修改自己再去做修改 有修改说明有发生了安全问题 此时可以重试或异常）乐观锁：会有一个版本号，每次操作数据会对版本号+1，再提交回数据时，会去校验是否比之前的版本大1 ，如果大1 ，则进行操作成功，这套机制的核心逻辑在于，如果在操作过程中，版本号只比原来大1 ，那么就意味着操作过程中没有人对他进行过修改，他的操作就是安全的，如果不大1，则数据被修改过，当然乐观锁还有一些变种的处理方式比如cas乐观锁的典型代表：就是cas，利用cas进行无锁化机制加锁，var5 是操作前读取的内存值，while中的var1+var2 是预估值，如果预估值 &#x3D;&#x3D; 内存值，则代表中间没有被人修改过，此时就将新值去替换 内存值其中do while 是为了在操作失败时，再次进行自旋操作，即把之前的逻辑再操作一次。乐观锁实现方式： 版本号方式 ： 会有一个版本号，每次操作数据会对版本号+1， 请求进来会查询数据 并且查询到这个版本号 当他做修改的时候 也就是更新库存的时候 需要将这个版本号作为他的条件 判断一开始查询到的版本号和现在数据库的版本号是否一致 一致再去执行 不一致意味着有线程先更改过了 这个时候需要重试或者抛出异常 (以上图片) CAS法： 对于以上版本法 我们发现 完全可以使用 stack作为version的替代 只需要判断 where 现在的stack &#x3D; 一开始查询的stack 一样的话说明无人更改 不一样说明有问题 代码实现乐观锁 ***CAS方式修改代码方案一、 12345// 库存减一boolean success = iSeckillVoucherService.update().setSql(&quot;stock = stock - 1&quot;).eq(&quot;voucher_id&quot;, voucherId).eq(&quot;stock&quot;, voucher.getStock()).update(); // 只增加一个条件 执行库存减一的时候判断stock是否和一开始查询的一样 以上逻辑的核心含义是：只要我扣减库存时的库存和之前我查询到的库存是一样的，就意味着没有人在中间修改过库存，那么此时就是安全的，但是以上这种方式通过测试发现会有很多失败的情况，失败的原因在于：在使用乐观锁过程中假设100个线程同时都拿到了100的库存，然后大家一起去进行扣减，但是100个人中只有1个人能扣减成功，其他的人在处理时，他们在扣减时，库存已经被修改过了，所以此时其他线程都会失败 使用乐观锁的问题： 假设多个线程一起进入 这个时候第一个成功了 其他同时进入的线程务必会失败 因为库存不一致 这个时候就会使得我们售卖不完 成功率太低修改代码方案二、之前的方式要修改前后都保持一致，但是这样我们分析过，成功的概率太低，所以我们的乐观锁需要变一下，改成stock大于0 即可 123boolean success = seckillVoucherService.update().setSql(&quot;stock= stock -1&quot;).eq(&quot;voucher_id&quot;, voucherId).update().gt(&quot;stock&quot;,0); //where id = ? and stock &gt; 0 知识小扩展：针对cas中的自旋压力过大，我们可以使用Longaddr这个类去解决Java8 提供的一个对AtomicLong改进后的一个类，LongAdder大量线程并发更新一个原子性的时候，天然的问题就是自旋，会导致并发性问题，当然这也比我们直接使用syn来的好所以利用这么一个类，LongAdder来进行优化如果获取某个值，则会对cell和base的值进行递增，最后返回一个完整的值 以上的订单 没有依靠于redis，而是直接访问数据库 这样如果并发量大 就会使得数据库压力过大 一人一单***需求：修改秒杀业务，要求同一个优惠券，一个用户只能下一单现在的问题在于：优惠卷是为了引流，但是目前的情况是，一个人可以无限制的抢这个优惠卷（黄牛是吧），所以我们应当增加一层逻辑，让一个用户只能下一个单，而不是让一个用户下多个单具体操作逻辑如下：比如时间是否充足，如果时间充足，则进一步判断库存是否足够，然后再根据优惠卷id和用户id查询是否已经下过这个订单，如果下过这个订单，则不再下单，否则进行下单 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public Result seckillVoucher(Long voucherId) &#123; // 获取又优惠券信息 SeckillVoucher voucher = iSeckillVoucherService.getById(voucherId); if (voucher == null) &#123; return Result.fail(&quot;无此优惠券&quot;); &#125; // 有优惠券 判断抢购时间是否开始 if (voucher.getBeginTime().isAfter(LocalDateTime.now())) &#123; // 没有开始 return Result.fail(&quot;失败&quot;); &#125; // 判断秒杀是否已经结束 if (voucher.getEndTime().isBefore(LocalDateTime.now())) &#123; return Result.fail(&quot;失败&quot;); &#125; // 可以抢购 判断库足是否充足 需要从数据库中查 Integer stock = voucher.getStock(); if (stock &lt; 1) &#123; return Result.fail(&quot;失败&quot;); &#125; --------------------------------------------------------------- // 一人一单 判断用户是否下过单了 UserDTO user = UserHolder.getUser(); Long userId = user.getId(); int count = iSeckillVoucherService.query().eq(&quot;user_id&quot;, userId).eq(&quot;voucher_id&quot;, voucherId).count(); if (count &gt; 0) &#123; // 说明此用户 已经买过了 return Result.fail(&quot;有问题&quot;); &#125; ---------------------------------------------------------------- // 库存减一 boolean success = iSeckillVoucherService.update() .setSql(&quot;stock = stock - 1&quot;) .eq(&quot;voucher_id&quot;, voucherId).gt(&quot;stock&quot;, 0).update(); if (!success) &#123; return Result.fail(&quot;error&quot;); &#125; // 创建订单 加入到订单表中 VoucherOrder voucherOrder = new VoucherOrder(); // 需要有订单id 这个id需要是全局唯一的 long orderId = redisIdWorker.nextId(&quot;order&quot;); voucherOrder.setId(orderId); // 用户id 代金券id voucherOrder.setVoucherId(voucherId); voucherOrder.setUserId(userId); save(voucherOrder); return Result.ok(orderId); &#125; 存在问题：现在的问题还是和之前一样(一个请求过来判断可以下单 在他还没有更改的时候 另一个进入 执行了 这里就会出现多次购买的问题)，并发过来，查询数据库，都不存在订单，所以我们还是需要加锁，但是乐观锁比较适合更新数据，而现在是插入数据，所以我们需要使用悲观锁操作注意：在这里提到了非常多的问题，我们需要慢慢的来思考，首先我们的初始方案是封装了一个createVoucherOrder方法，同时为了确保他线程安全，在方法上添加了一把synchronized 锁 123456789101112131415161718192021222324252627282930313233343536@Transactionalpublic synchronized Result createVoucherOrder(Long voucherId) &#123; Long userId = UserHolder.getUser().getId(); // 5.1.查询订单 int count = query().eq(&quot;user_id&quot;, userId).eq(&quot;voucher_id&quot;, voucherId).count(); // 5.2.判断是否存在 if (count &gt; 0) &#123; // 用户已经购买过了 return Result.fail(&quot;用户已经购买过一次！&quot;); &#125; // 6.扣减库存 boolean success = seckillVoucherService.update() .setSql(&quot;stock = stock - 1&quot;) // set stock = stock - 1 .eq(&quot;voucher_id&quot;, voucherId).gt(&quot;stock&quot;, 0) // where id = ? and stock &gt; 0 .update(); if (!success) &#123; // 扣减失败 return Result.fail(&quot;库存不足！&quot;); &#125; // 7.创建订单 VoucherOrder voucherOrder = new VoucherOrder(); // 7.1.订单id long orderId = redisIdWorker.nextId(&quot;order&quot;); voucherOrder.setId(orderId); // 7.2.用户id voucherOrder.setUserId(userId); // 7.3.代金券id voucherOrder.setVoucherId(voucherId); save(voucherOrder); // 7.返回订单id return Result.ok(orderId);&#125; ，但是这样添加锁，锁的粒度太粗了，在使用锁过程中，控制锁粒度 是一个非常重要的事情，因为如果锁的粒度太大，会导致每个线程进来都会锁住，所以我们需要去控制锁的粒度，以下这段代码需要修改为：intern() 这个方法是从常量池中拿到数据，如果我们直接使用userId.toString() 他拿到的对象实际上是不同的对象，new出来的对象，我们使用锁必须保证锁必须是同一把，所以我们需要使用intern()方法 12345678910111213141516171819202122232425262728293031323334353637@Transactionalpublic Result createVoucherOrder(Long voucherId) &#123;Long userId = UserHolder.getUser().getId();synchronized(userId.toString().intern())&#123; // 5.1.查询订单 int count = query().eq(&quot;user_id&quot;, userId).eq(&quot;voucher_id&quot;, voucherId).count(); // 5.2.判断是否存在 if (count &gt; 0) &#123; // 用户已经购买过了 return Result.fail(&quot;用户已经购买过一次！&quot;); &#125; // 6.扣减库存 boolean success = seckillVoucherService.update() .setSql(&quot;stock = stock - 1&quot;) // set stock = stock - 1 .eq(&quot;voucher_id&quot;, voucherId).gt(&quot;stock&quot;, 0) // where id = ? and stock &gt; 0 .update(); if (!success) &#123; // 扣减失败 return Result.fail(&quot;库存不足！&quot;); &#125; // 7.创建订单 VoucherOrder voucherOrder = new VoucherOrder(); // 7.1.订单id long orderId = redisIdWorker.nextId(&quot;order&quot;); voucherOrder.setId(orderId); // 7.2.用户id voucherOrder.setUserId(userId); // 7.3.代金券id voucherOrder.setVoucherId(voucherId); save(voucherOrder); // 7.返回订单id return Result.ok(orderId);&#125;&#125; 但是以上代码还是存在问题，问题的原因在于当前方法被spring的事务控制，如果你在方法内部加锁，可能会导致当前方法事务还没有提交，但是锁已经释放也会导致问题，所以我们选择将当前方法整体包裹起来，确保事务不会出现问题：如下：在seckillVoucher 方法中，添加以下逻辑，这样就能保证事务的特性，同时也控制了锁的粒度 123456789// 只有将锁加在这里才能实现一人一单UserDTO user = UserHolder.getUser();Long userId = user.getId();synchronized (userId.toString().intern()) &#123; // 这里有一个问题 事务会失效 因为 spring事务时使用的代理 而这里是 this.createVoucherOrder调用的 // 另外需要导入依赖 和开启注解去暴露对象 IVoucherOrderService proxy = (IVoucherOrderService) AopContext.currentProxy(); return proxy.createVoucherOrder(voucherId);&#125; 虽然先做到了一人一单 但是问题是 现在只是在单机项目中如果将是在集群下改不和处理？ 集群环境下的并发问题通过加锁可以解决在单机情况下的一人一单安全问题，但是在集群模式下就不行了。有关锁失效原因分析由于现在我们部署了多个tomcat，每个tomcat都有一个属于自己的jvm，那么假设在服务器A的tomcat内部，有两个线程，这两个线程由于使用的是同一份代码，那么他们的锁对象是同一个，是可以实现互斥的，但是如果现在是服务器B的tomcat内部，又有两个线程，但是他们的锁对象写的虽然和服务器A一样，但是锁对象却不是同一个，所以线程3和线程4可以实现互斥，但是却无法和线程1和线程2实现互斥，这就是 集群环境下，syn锁失效的原因，在这种情况下，我们就需要使用分布式锁来解决这个问题。jvm内部有锁监视器 他会去监控锁而分布式 多个tomcat的情况下 每个服务器有自己的锁监视 他们之间不互通这时候就需要使用分布式锁去实现 分布式锁 ***基本原理和实现方式对比分布式锁：满足分布式系统或集群模式下多进程可见并且互斥的锁。分布式锁的核心思想就是让大家都使用同一把锁 只有获得锁的人才能进入，只要大家使用的是同一把锁，那么我们就能锁住线程，不让线程进行，让程序串行执行，这就是分布式锁的核心思路集群中的服务器都使用同一把锁 那么分布式锁他应该满足一些什么样的条件呢？ 可见性：多个线程都能看到相同的结果，注意：这个地方说的可见性并不是并发编程中指的内存可见性，只是说多个进程之间都能感知到变化的意思 互斥：互斥是分布式锁的最基本的条件，使得程序串行执行 高可用：程序不易崩溃，时时刻刻都保证较高的可用性 高性能：由于加锁本身就让性能降低，所有对于分布式锁本身需要他就较高的加锁性能和释放锁性能 安全性：安全也是程序中必不可少的一环 常见的分布式锁有三种Mysql：mysql本身就带有锁机制，但是由于mysql性能本身一般，所以采用分布式锁的情况下，其实使用mysql作为分布式锁比较少见Redis：redis作为分布式锁是非常常见的一种使用方式，现在企业级开发中基本都使用redis或者zookeeper作为分布式锁，利用setnx这个方法，如果插入key成功，则表示获得到了锁，如果有人插入成功，其他人插入失败则表示无法获得到锁，利用这套逻辑来实现分布式锁Zookeeper：zookeeper也是企业级开发中较好的一个实现分布式锁的方案，由于本套视频并不讲解zookeeper的原理和分布式锁的实现，所以不过多阐述 Redis分布式锁的实现核心思路 ***实现分布式锁时需要实现的两个基本方法： 获取锁： 互斥：确保只能有一个线程获取锁 非阻塞：尝试一次，成功返回true，失败返回false 释放锁：（一定要释放） 手动释放 超时释放：获取锁时添加一个超时时间 （一定要设置 不然没释放锁的时候服务宕机了 那没有请求能够进来了） 注意要保证获取锁跟设置过期时间的原子性 核心思路：我们利用redis 的setNx 方法，当有多个线程进入时，我们就利用该方法，第一个线程进入时，redis 中就有这个key 了，返回了1，如果结果是1，则表示他抢到了锁，那么他去执行业务，然后再删除锁，退出锁逻辑，没有抢到锁的哥们，等待一定时间后重试或者直接返回失败即可 获取锁释放锁 1234567891011121314151617181920212223public class SimpleRedisLock implements ILock &#123; private String name; private StringRedisTemplate stringRedisTemplate; public SimpleRedisLock(String name, StringRedisTemplate stringRedisTemplate) &#123; this.name = name; this.stringRedisTemplate = stringRedisTemplate; &#125; public static final String KEY_PREFIX = &quot;lock:&quot;; @Override public boolean tryLock(long timeoutSec) &#123; // 获取访问线程的名字 作为锁的value long threadId = Thread.currentThread().getId(); // key不写死 由使用者来指定 Boolean success = stringRedisTemplate.opsForValue().setIfAbsent(KEY_PREFIX + name, threadId + &quot;&quot;, timeoutSec, TimeUnit.SECONDS); return Boolean.TRUE.equals(success); &#125; @Override public void unLock() &#123; stringRedisTemplate.delete(KEY_PREFIX + name); &#125;&#125; 修改原本的一人一单业务 12345678910111213141516171819202122232425262728293031323334353637383940public Result seckillVoucher(Long voucherId) &#123;// 获取又优惠券信息SeckillVoucher voucher = iSeckillVoucherService.getById(voucherId);if (voucher == null) &#123; return Result.fail(&quot;无此优惠券&quot;);&#125;// 有优惠券 判断抢购时间是否开始if (voucher.getBeginTime().isAfter(LocalDateTime.now())) &#123; // 没有开始 return Result.fail(&quot;失败&quot;);&#125;// 判断秒杀是否已经结束if (voucher.getEndTime().isBefore(LocalDateTime.now())) &#123; return Result.fail(&quot;失败&quot;);&#125;// 可以抢购 判断库足是否充足 需要从数据库中查Integer stock = voucher.getStock();if (stock &lt; 1) &#123; return Result.fail(&quot;失败&quot;);&#125;UserDTO user = UserHolder.getUser();Long userId = user.getId();SimpleRedisLock simpleRedisLock = new SimpleRedisLock(&quot;order:&quot; + userId, stringRedisTemplate);// 这是初步的分布式锁 无论是那个服务器的请求 都面对的是一台redis 在访问之前都需要通过唯一的redis获取锁boolean islock = simpleRedisLock.tryLock(10);if (!islock) &#123; // 获取锁失败 返回错误 return Result.fail(&quot;只允许下一单&quot;);&#125;try &#123; // 这里有一个问题 事务会失效 因为 spring事务时使用的代理 而这里是 this.createVoucherOrder调用的 // 另外需要导入依赖 和开启注解去暴露对象 IVoucherOrderService proxy = (IVoucherOrderService) AopContext.currentProxy(); return proxy.createVoucherOrder(voucherId);&#125; finally &#123; // 释放锁 simpleRedisLock.unLock();&#125;&#125; 以上使用分布式锁解决 集群问题以上的问题在于： 如果获取到锁的线程 他业务执行过长 锁都过期了他还没有执行完 这样就会有线程安全问题 假设另一个线程进来了 老线程执行完 释放了锁 但是这个锁是别人的（释放错了） 又有新的线程进来了 … 这样线程安全问题再次发生因此需要有 Redis分布式锁误删情况说明 ***逻辑说明：持有锁的线程在锁的内部出现了阻塞，导致他的锁自动释放，这时其他线程，线程2来尝试获得锁，就拿到了这把锁，然后线程2在持有锁执行过程中，线程1反应过来，继续执行，而线程1执行过程中，走到了删除锁逻辑，此时就会把本应该属于线程2的锁进行删除，这就是误删别人锁的情况说明解决方案：解决方案就是在每个线程释放锁的时候，去判断一下当前这把锁是否属于自己，如果属于自己，则不进行锁的删除，假设还是上边的情况，线程1卡顿，锁自动释放，线程2进入到锁的内部执行逻辑，此时线程1反应过来，然后删除锁，但是线程1，一看当前这把锁不是属于自己，于是不进行删除锁逻辑，当线程2走到删除锁逻辑时，如果没有卡过自动释放锁的时间点，则判断当前这把锁是属于自己的，于是删除这把锁。 解决Redis分布式锁误删问题 ***需求：修改之前的分布式锁实现，满足：在获取锁时存入线程标示（可以用UUID表示 保证唯一性）在释放锁时先获取锁中的线程标示，判断是否与当前线程标示一致 如果一致则释放锁 如果不一致则不释放锁 核心逻辑：在存入锁时，放入自己线程的标识（在上面的项目中 我们使用的是线程的id作为锁的value 这不一定唯一有重复的可能 因此我们使用 UUID+线程id），在删除锁时，判断当前这把锁的标识是不是自己存入的，如果是，则进行删除，如果不是，则不进行删除。 代码实现 业务代码不变 只需要更改锁 12345678910111213141516171819202122232425262728293031private String name; private StringRedisTemplate stringRedisTemplate; public SimpleRedisLock(String name, StringRedisTemplate stringRedisTemplate) &#123; this.name = name; this.stringRedisTemplate = stringRedisTemplate; &#125; public static final String KEY_PREFIX = &quot;lock:&quot;; /** * 使用UUID在加上threadId 使得锁的value一定唯一 使得每个线程删除自己锁之前可以这把锁是不是自己占有 */ public static final String ID_PREFIX = UUID.randomUUID().toString(true) + &quot;-&quot;; @Override public boolean tryLock(long timeoutSec) &#123; // 获取访问线程的名字 加上UUID 作为锁的value 保证唯一 String threadId = ID_PREFIX + Thread.currentThread().getId(); Boolean success = stringRedisTemplate.opsForValue().setIfAbsent(KEY_PREFIX + name, threadId, timeoutSec, TimeUnit.SECONDS); return Boolean.TRUE.equals(success); &#125; @Override public void unLock() &#123; String threadId = ID_PREFIX + Thread.currentThread().getId(); // 获取锁 String lock = stringRedisTemplate.opsForValue().get(KEY_PREFIX + name); // 是否是自己的锁 if (threadId.equals(lock)) &#123; // 一致才删除锁 stringRedisTemplate.delete(KEY_PREFIX + name); &#125; &#125; 有关代码实操说明：在我们修改完此处代码后，我们重启工程，然后启动两个线程，第一个线程持有锁后，手动释放锁，第二个线程 此时进入到锁内部，再放行第一个线程，此时第一个线程由于锁的value值并非是自己，所以不能释放锁，也就无法删除别人的锁，此时第二个线程能够正确释放锁，通过这个案例初步说明我们解决了锁误删的问题。 以上仍然有问题假设线程1确定是自己的锁了 在准备释放锁的时候阻塞（jvm垃圾回收器阻塞了线程） 锁过期了 这个时候第二个线程进入 就有可能删错锁 分布式锁的原子性问题 ***更为极端的误删逻辑说明：线程1现在持有锁之后，在执行业务逻辑过程中，他正准备删除锁，而且已经走到了条件判断的过程中，比如他已经拿到了当前这把锁确实是属于他自己的，正准备删除锁，但是此时他的锁到期了，那么此时线程2进来，但是线程1他会接着往后执行，当他卡顿结束后，他直接就会执行删除锁那行代码，相当于条件判断并没有起到作用，这就是删锁时的原子性问题，之所以有这个问题，是因为线程1的拿锁，比锁，删锁，实际上并不是原子性的，我们要防止刚才的情况发生，因此如何保证 删除锁的一系列操作（查询锁，判断锁是否是自己， 删除锁）的原子性 Lua脚本解决多条命令原子性问题 ***Redis提供了Lua脚本功能，在一个脚本中编写多条Redis命令，确保多条命令执行时的原子性。Lua是一种编程语言，它的基本语法大家可以参考网站：https://www.runoob.com/lua/lua-tutorial.html，这里重点介绍Redis提供的调用函数，我们可以使用lua去操作redis，又能保证他的原子性，这样就可以实现拿锁比锁删锁是一个原子性动作了，作为Java程序员这一块并不作一个简单要求，并不需要大家过于精通，只需要知道他有什么作用即可。这里重点介绍Redis提供的调用函数，语法如下：redis.call(‘命令名称’, ‘key’, ‘其它参数’, …)例如，我们要执行set name jack，则脚本是这样： 12# 执行 set name jackredis.call(&#x27;set&#x27;, &#x27;name&#x27;, &#x27;jack&#x27;) 例如，我们要先执行set name Rose，再执行get name，则脚本如下： 123456# 先执行 set name jackredis.call(&#x27;set&#x27;, &#x27;name&#x27;, &#x27;Rose&#x27;)# 再执行 get namelocal name = redis.call(&#x27;get&#x27;, &#x27;name&#x27;)# 返回return name 写好脚本以后，需要用Redis命令来调用脚本接下来我们来回一下我们释放锁的逻辑：释放锁的业务流程是这样的 1、获取锁中的线程标示 2、判断是否与指定的标示（当前线程标示）一致 3、如果一致则释放锁（删除） 4、如果不一致则什么都不做如果用Lua脚本来表示则是这样的：最终我们操作redis的拿锁比锁删锁的lua脚本就会变成这样 12345678-- 这里的 KEYS[1] 就是锁的key，这里的ARGV[1] 就是当前线程标示-- 获取锁中的标示，判断是否与当前线程标示一致if (redis.call(&#x27;GET&#x27;, KEYS[1]) == ARGV[1]) then -- 一致，则删除锁 return redis.call(&#x27;DEL&#x27;, KEYS[1])end-- 不一致，则直接返回return 0 利用Java代码调用Lua脚本改造分布式锁lua脚本本身并不需要大家花费太多时间去研究，只需要知道如何调用，大致是什么意思即可，所以在笔记中并不会详细的去解释这些lua表达式的含义。我们的RedisTemplate中，可以利用execute方法去执行lua脚本，参数对应关系就如下图Java代码 123456789101112131415private static final DefaultRedisScript&lt;Long&gt; UNLOCK_SCRIPT;static &#123; UNLOCK_SCRIPT = new DefaultRedisScript&lt;&gt;(); UNLOCK_SCRIPT.setLocation(new ClassPathResource(&quot;unlock.lua&quot;)); UNLOCK_SCRIPT.setResultType(Long.class);&#125;public void unlock() &#123; // 调用lua脚本 stringRedisTemplate.execute( UNLOCK_SCRIPT, Collections.singletonList(KEY_PREFIX + name), ID_PREFIX + Thread.currentThread().getId());&#125;经过以上代码改造后，我们就能够实现 拿锁比锁删锁的原子性动作了~ 小总结：基于Redis的分布式锁实现思路： 利用set nx ex获取锁(nx保证互斥 只有一个进入 ex是过期方式无法释放锁的情况)，并设置过期时间，保存线程标示 释放锁时先判断线程标示是否与自己一致，一致则删除锁 （防止堵塞导致的误删） 特性： 利用set nx满足互斥性 利用set ex保证故障时锁依然能释放，避免死锁，提高安全性 利用Redis集群保证高可用和高并发特性 笔者总结：我们一路走来，利用添加过期时间，防止死锁问题的发生，但是有了过期时间之后，可能出现误删别人锁的问题，这个问题我们开始是利用删之前 通过拿锁，比锁，删锁这个逻辑来解决的，也就是删之前判断一下当前这把锁是否是属于自己的，但是现在还有原子性问题，也就是我们没法保证拿锁比锁删锁是一个原子性的动作，最后通过lua表达式来解决这个问题但是目前还剩下一个问题锁不住，什么是锁不住呢，你想一想，如果当过期时间到了之后，我们可以给他续期一下，比如续个30s，就好像是网吧上网， 网费到了之后，然后说，来，网管，再给我来10块的，是不是后边的问题都不会发生了，那么续期问题怎么解决呢，可以依赖于我们接下来要学习redission啦测试逻辑：第一个线程进来，得到了锁，手动删除锁，模拟锁超时了，其他线程会执行lua来抢锁，当第一天线程利用lua删除锁时，lua能保证他不能删除他的锁，第二个线程删除锁时，利用lua同样可以保证不会删除别人的锁，同时还能保证原子性。 现在的问题是：（redission解决） Redis 如果是集群（而不是只有一个 Redis），如果分布式锁的数据不同步怎么办？ 红锁 锁的续期 分布式锁-redission *** 最终方案分布式锁-redission功能介绍基于setnx实现的分布式锁存在下面的问题：（以上我们自己写了锁的实现，但是实际上已经有现成的帮我们做了）重入问题：重入问题是指 获得锁的线程可以再次进入到相同的锁的代码块中，可重入锁的意义在于防止死锁，比如HashTable这样的代码中，他的方法都是使用synchronized修饰的，假如他在一个方法内，调用另一个方法，那么此时如果是不可重入的，不就死锁了吗？所以可重入锁他的主要意义是防止死锁，我们的synchronized和Lock锁都是可重入的。（假设方法A去调用方法B 此时A去获取锁调用B B也要去获取锁 结果等待锁的释放 这样就出现了死锁 这种情况下就需要锁是可以重入的 允许同一个线程重复获取锁）不可重试：是指目前的分布式只能尝试一次，我们认为合理的情况是：当线程在获得锁失败后，他应该能再次尝试获得锁。超时释放：我们在加锁时增加了过期时间，这样的我们可以防止死锁，但是如果卡顿的时间超长，虽然我们采用了lua表达式防止删锁的时候，误删别人的锁，但是毕竟没有锁住，有安全隐患主从一致性： 如果Redis提供了主从集群，当我们向集群写数据时，主机需要异步的将数据同步给从机，而万一在同步过去之前，主机宕机了，就会出现死锁问题。那么什么是Redission呢Redisson是一个在Redis的基础上实现的Java驻内存数据网格（In-Memory Data Grid）。它不仅提供了一系列的分布式的Java常用对象，还提供了许多分布式服务，其中就包含了各种分布式锁的实现。说白了，他是一个在redis之上实现的一个分布式工具的集合 什么都有Redission提供了分布式锁的多种多样的功能 分布式锁 可重入锁 红锁 连锁 读写锁 信号量 闭锁 … Redisson 入门 ***Redisson（redis的儿子）—— 用来解决redis在分布式情况下遇到的问题 分布式锁只是他的多数工具的一个所以说 以上我们自己实现的分布式锁 只是为了懂得实现原理 可以直接用redisson的现成实现 他帮我们实现了很多（例如以上我们自己写的分布式 获取锁 释放锁操作） 引入依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.redisson&lt;/groupId&gt; &lt;artifactId&gt;redisson&lt;/artifactId&gt; &lt;version&gt;3.13.6&lt;/version&gt;&lt;/dependency&gt; 配置Redisson客户端： 12345678910111213@Configurationpublic class RedissonConfig &#123; @Bean public RedissonClient redissonClient()&#123; // 配置 Config config = new Config(); config.useSingleServer().setAddress(&quot;redis://192.168.150.101:6379&quot;) .setPassword(&quot;123321&quot;); // 创建RedissonClient对象 return Redisson.create(config); &#125;&#125; 如何使用Redission的分布式锁 123456789101112131415161718192021@Resourceprivate RedissionClient redissonClient;@Testvoid testRedisson() throws Exception&#123;//获取锁(可重入)，指定锁的名称RLock lock = redissonClient.getLock(&quot;anyLock&quot;);//尝试获取锁，参数分别是：获取锁的最大等待时间(期间会重试)，锁自动释放时间，时间单位boolean isLock = lock.tryLock(1,10,TimeUnit.SECONDS);//判断获取锁成功if(isLock)&#123; try&#123; System.out.println(&quot;执行业务&quot;); &#125;finally&#123; //释放锁 lock.unlock(); &#125;&#125;&#125; 在 VoucherOrderServiceImpl注入RedissonClient 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748 @Resource private RedissonClient redissonClient; @Override public Result seckillVoucher(Long voucherId) &#123; // 获取又优惠券信息 SeckillVoucher voucher = iSeckillVoucherService.getById(voucherId); if (voucher == null) &#123; return Result.fail(&quot;无此优惠券&quot;); &#125; // 有优惠券 判断抢购时间是否开始 if (voucher.getBeginTime().isAfter(LocalDateTime.now())) &#123; // 没有开始 return Result.fail(&quot;失败&quot;); &#125; // 判断秒杀是否已经结束 if (voucher.getEndTime().isBefore(LocalDateTime.now())) &#123; return Result.fail(&quot;失败&quot;); &#125; // 可以抢购 判断库足是否充足 需要从数据库中查 Integer stock = voucher.getStock(); if (stock &lt; 1) &#123; return Result.fail(&quot;失败&quot;); &#125;--------------------------------------------------------- // 只有将锁加在这里才能实现一人一单 UserDTO user = UserHolder.getUser(); Long userId = user.getId(); //SimpleRedisLock simpleRedisLock = new SimpleRedisLock(&quot;order:&quot; + userId, stringRedisTemplate); // 使用redisson 获取锁 指定锁的名称 RLock lock = redissonClient.getLock(&quot;lock:order:&quot; + userId); // 尝试获取锁(可重入锁)，参数分别是：获取锁的最大等待时间(期间会重试)，锁自动释放时间，时间单位 // 如果不写参数 默认是 -1 30 秒 -1就是不等待直接返回 boolean islock = lock.tryLock();--------------------------------------------------------- if (!islock) &#123; // 获取锁失败 返回错误 return Result.fail(&quot;只允许下一单&quot;); &#125; try &#123; // 这里有一个问题 事务会失效 因为 spring事务时使用的代理 而这里是 this.createVoucherOrder调用的 // 另外需要导入依赖 和开启注解去暴露对象 IVoucherOrderService proxy = (IVoucherOrderService) AopContext.currentProxy(); return proxy.createVoucherOrder(voucherId); &#125; finally &#123; // 释放锁 lock.unlock(); &#125; &#125; redission可重入锁原理 ***在Lock锁中，他是借助于底层的一个voaltile的一个state变量来记录重入的状态的，比如当前没有人持有这把锁，那么state&#x3D;0，假如有人持有这把锁，那么state&#x3D;1，如果持有这把锁的人再次持有这把锁，那么state就会+1 ，如果是对于synchronized而言，他在c语言代码中会有一个count，原理和state类似，也是重入一次就加一，释放一次就-1 ，直到减少成0 时，表示当前这把锁没有被人持有。在redission中，我们的也支持支持可重入锁在分布式锁中，他采用hash结构用来存储锁，其中大key表示表示这把锁是否存在，用小key表示当前这把锁被哪个线程持有，所以接下来我们一起分析一下当前的这个lua表达式这个地方一共有3个参数KEYS[1] ： 锁名称ARGV[1]： 锁失效时间ARGV[2]： id + “:” + threadId; 锁的小keyexists: 判断数据是否存在 name：是lock是否存在,如果&#x3D;&#x3D;0，就表示当前这把锁不存在redis.call(‘hset’, KEYS[1], ARGV[2], 1);此时他就开始往redis里边去写数据 ，写成一个hash结构Lock{id + “:” + threadId : 1}如果当前这把锁存在，则第一个条件不满足，再判断redis.call(&#39;hexists&#39;, KEYS[1], ARGV[2]) == 1此时需要通过大key+小key判断当前这把锁是否是属于自己的，如果是自己的，则进行redis.call(&#39;hincrby&#39;, KEYS[1], ARGV[2], 1)将当前这个锁的value进行+1 ，redis.call(‘pexpire’, KEYS[1], ARGV[1]); 然后再对其设置过期时间，如果以上两个条件都不满足，则表示当前这把锁抢锁失败，最后返回pttl，即为当前这把锁的失效时间如果小伙帮们看了前边的源码， 你会发现他会去判断当前这个方法的返回值是否为null，如果是null，则对应则前两个if对应的条件，退出抢锁逻辑，如果返回的不是null，即走了第三个分支，在源码处会进行while(true)的自旋抢锁。 1234567891011&quot;if (redis.call(&#x27;exists&#x27;, KEYS[1]) == 0) then &quot; + &quot;redis.call(&#x27;hset&#x27;, KEYS[1], ARGV[2], 1); &quot; + &quot;redis.call(&#x27;pexpire&#x27;, KEYS[1], ARGV[1]); &quot; + &quot;return nil; &quot; + &quot;end; &quot; + &quot;if (redis.call(&#x27;hexists&#x27;, KEYS[1], ARGV[2]) == 1) then &quot; + &quot;redis.call(&#x27;hincrby&#x27;, KEYS[1], ARGV[2], 1); &quot; + &quot;redis.call(&#x27;pexpire&#x27;, KEYS[1], ARGV[1]); &quot; + &quot;return nil; &quot; + &quot;end; &quot; + &quot;return redis.call(&#x27;pttl&#x27;, KEYS[1]);&quot; 说白了 **判断这个锁有人的情况下 是不是自己线程 如果是也会让他获取线程（通过计数state字段 用来记录重新获取了几次锁 ）此时锁的value有两个字段 用hash结构来存储 当前线程重复进入了多少次就记录多少次（+1） 删除锁的时候就不能直接删掉了 需要从里到外一层层删除也就是(state字段-1) 当字段是0就一定到了最外层 那就可以执行删除锁（delete）命令 **另外 我们使用的是hash结构 不可以直接设置nx和过期时间 只能先判断锁 在设置过期时间 这就有需要考虑原子问题（lua脚本来写） 以上的逻辑就是 redisson实现可重入锁的原理（追溯redisson的tryLock()方法源码 他底层就是使用的lua脚本加以上逻辑来实现）核心就是： hash中的两个字段 通过+1 -1的方式控制可重入 如果我们自己做 以上的操作 非常麻烦 redisson封装直接用！ 拒绝自己写 知道逻辑就行 redission锁重试 ***以上解决了 锁重入的问题 现在来看锁重试 和看门狗以上在于如果不是自己的锁直接结束 而可重试锁 可以让当前线程去重试获取锁 锁重试：利用信号量和PubSub（发布-订阅）方式来实现等待、唤醒、获取锁失败的重试机制（也就是第一次获取锁以后不直接返回失败 而是等待 直到释放锁的人发布了可获取信号（publish）等待的捕捉到再去重新尝试锁 如果失败再去等待 当然也不是无限的获取等待 根据等待时间来 超过了就返回失败 ） 实现也很简单redisson trylock方法 加入等待时间参数 抢锁过程中，获得当前线程，通过tryAcquire进行抢锁，该抢锁逻辑和之前逻辑相同 所以如果返回是null，则代表着当前这哥们已经抢锁完毕，或者可重入完毕，但是如果以上两个条件都不满足，则进入到第三个条件，返回的是锁的失效时间，可以自行往下翻一点点，你能发现有个while( true) 再次进行tryAcquire进行抢锁 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980 //源码 public boolean tryLock(long waitTime, long leaseTime, TimeUnit unit) throws InterruptedException &#123; //1.将等待时间转换为毫秒数,获取当前的线程 long time = unit.toMillis(waitTime); long current = System.currentTimeMillis(); long threadId = Thread.currentThread().getId(); //2.尝试获取锁,返回null 代表没有锁,返回有值标识锁的过期时间 Long ttl = tryAcquire(waitTime, leaseTime, unit, threadId); // 3.成功获取锁 if (ttl == null) &#123; return true; &#125; //4.尝试获取锁耗时超过了等待时间,确认失败 time -= System.currentTimeMillis() - current; if (time &lt;= 0) &#123; acquireFailed(waitTime, unit, threadId); return false; &#125; current = System.currentTimeMillis(); //5.消息队列 订阅了其他线程释放锁的信号 //在unlock 脚本中 有一个 redis.call(&#x27;publish&#x27;,key[2],argv[1]) RFuture&lt;RedissonLockEntry&gt; subscribeFuture = subscribe(threadId); //6.当这个future 在指定时间内完成,返回true,否则false if (!subscribeFuture.await(time, TimeUnit.MILLISECONDS)) &#123; if (!subscribeFuture.cancel(false)) &#123; //等到最大等待时间结束,还没有等到,取消订阅,返回false subscribeFuture.onComplete((res, e) -&gt; &#123; if (e == null) &#123; unsubscribe(subscribeFuture, threadId); &#125; &#125;); &#125; acquireFailed(waitTime, unit, threadId); return false; &#125; //7.再次判断时间是否超出 try &#123; time -= System.currentTimeMillis() - current; if (time &lt;= 0) &#123; acquireFailed(waitTime, unit, threadId); return false; &#125; //todo 8.开始锁重试 while (true) &#123; long currentTime = System.currentTimeMillis(); ttl = tryAcquire(waitTime, leaseTime, unit, threadId); // lock acquired if (ttl == null) &#123; return true; &#125; time -= System.currentTimeMillis() - currentTime; if (time &lt;= 0) &#123; acquireFailed(waitTime, unit, threadId); return false; &#125; // waiting for message currentTime = System.currentTimeMillis(); //9.利用信号量来进行获取 if (ttl &gt;= 0 &amp;&amp; ttl &lt; time) &#123; subscribeFuture.getNow().getLatch().tryAcquire(ttl, TimeUnit.MILLISECONDS); &#125; else &#123; subscribeFuture.getNow().getLatch().tryAcquire(time, TimeUnit.MILLISECONDS); &#125; time -= System.currentTimeMillis() - currentTime; if (time &lt;= 0) &#123; acquireFailed(waitTime, unit, threadId); return false; &#125; &#125; &#125; finally &#123; //取消订阅 unsubscribe(subscribeFuture, threadId); &#125;// return get(tryLockAsync(waitTime, leaseTime, unit)); &#125; 看门狗机制 续期*** redisson 中提供的续期机制 每隔一段时间延长一下 防止锁的误释放他是确保拿到锁的线程一定完成业务不会因为阻塞而锁过期 不出现锁过期其他线程进入的线程安全问题 我们只需要通过它的API中的trylock和unlock即可完成分布式锁，他帮我们考虑了很多细节：1：redisson所有指令都通过lua脚本执行，redis支持lua脚本原子性执行2：redisson设置一个key的默认过期时间为30s,如果某个客户端持有一个锁超过了30s怎么办？ redisson中有一个watchdog的概念，翻译过来就是看门狗，它会在你获取锁之后，每隔10秒帮你把key的超时时间设为30s 这样的话，就算一直持有锁也不会出现key过期了，其他线程获取到锁的问题了。3：redisson的“看门狗”逻辑保证了没有死锁发生。 (如果机器宕机了，看门狗也就没了。此时就不会延长key的过期时间，到了30s之后就会自动过期了，其他线程可以获取到锁) **简单理解为他就是一个 监控线程 **如果方法还没执行完，就帮你重置 redis 锁的过期时间。原理： 监听当前线程，默认过期时间是 30 秒，每 10 秒续期一次（补到 30 秒） 如果线程挂掉（注意 debug 模式也会被它当成服务器宕机），则不会续期 场景Redisson锁重试的问题是解决了, 但是总会发生一些问题, 如果我们的业务阻塞超时了ttl到期了, 别的线程看见我们的ttl到期了, 他重试他就会拿到本该属于我们的锁, 这时候就有安全问题了, 所以该怎么解决? （在之前我们自己手动实现的时候 使用lua脚本去保证删锁时候判断是否是自己的锁的原子性 当时的解决了因为超时释放解决的误删问题 但是超时还是会发生 虽然不会误删 但是会有别的问题 因此需要引入更完善的方法） 我们必须确保锁是业务执行完释放的, 而不是因为阻塞而释放的 123456789101112131415161718192021222324252627 private &lt;T&gt; RFuture&lt;Long&gt; tryAcquireAsync(long waitTime, long leaseTime, TimeUnit unit, long threadId) &#123; //自定义了时间 if (leaseTime != -1) &#123; // leaseTime是-1是开门狗机制开启的条件 return tryLockInnerAsync(waitTime, leaseTime, unit, threadId, RedisCommands.EVAL_LONG); &#125; //2.默认过期实践 30s 看门狗 RFuture&lt;Long&gt; ttlRemainingFuture = tryLockInnerAsync(waitTime, commandExecutor.getConnectionManager().getCfg().getLockWatchdogTimeout(),//3. 当ttlRemainingFuture的异步尝试获取锁完成以后, //先判断执行过程中是否有异常, 如果有异常就直接返回了结束执行.//如果没有发生异常, 则判断ttlRemaining(剩余有效期)是否为空, //为空的话就代表获取锁成功, 执行锁到期续约的核心方法scheduleExpectationRenew去续期 TimeUnit.MILLISECONDS, threadId, RedisCommands.EVAL_LONG); ttlRemainingFuture.onComplete((ttlRemaining, e) -&gt; &#123; if (e != null) &#123; return; &#125; // lock acquired if (ttlRemaining == null) &#123; //更新有效期,内部源码 通过定时任务每隔10s,定时重置有效期 scheduleExpirationRenewal(threadId); &#125; &#125;); return ttlRemainingFuture; &#125; **那么什么时候释放锁呢? **当然是在释放锁的时候, 在释放锁的时候取消重置定时任务 以下是可重试锁和看门狗机制的原理图总结： 可重入：利用hash结构记录线程id和重入次数 可重试：利用信号量和PubSub（发布-订阅）方式来实现等待、唤醒、获取锁失败的重试机制（也就是第一次获取锁以后不直接返回失败 而是等待 直到释放锁的人发布了可获取信号（publish）等待的捕捉到再去重新尝试锁 如果失败再去等待 当然也不是无限的获取等待 根据等待时间来 超过了就返回失败 ） 实现也很简单redisson trylock方法 加入等待时间参数 看门狗：为了防止当前线程还没执行完锁到期了 别的线程进入获取锁 出现了线程安全问题 看门狗隔一段时间续期锁的过期时间 使得业务可以执行完 说明：由于以上中已经说明了有关tryLock的源码解析以及其看门狗原理，所以笔者在这里给大家分析lock()方法的源码解析，希望大家在学习过程中，能够掌握更多的知识抢锁过程中，获得当前线程，通过tryAcquire进行抢锁，该抢锁逻辑和之前逻辑相同1、先判断当前这把锁是否存在，如果不存在，插入一把锁，返回null2、判断当前这把锁是否是属于当前线程，如果是，则返回null所以如果返回是null，则代表着当前这哥们已经抢锁完毕，或者可重入完毕，但是如果以上两个条件都不满足，则进入到第三个条件，返回的是锁的失效时间，同学们可以自行往下翻一点点，你能发现有个while( true) 再次进行tryAcquire进行抢锁 123456long threadId = Thread.currentThread().getId();Long ttl = tryAcquire(-1, leaseTime, unit, threadId);// lock acquiredif (ttl == null) &#123; return;&#125; 接下来会有一个条件分支，因为lock方法有重载方法，一个是带参数，一个是不带参数，如果带带参数传入的值是-1，如果传入参数，则leaseTime是他本身，所以如果传入了参数，此时leaseTime !&#x3D; -1 则会进去抢锁，抢锁的逻辑就是之前说的那三个逻辑 123if (leaseTime != -1) &#123; return tryLockInnerAsync(waitTime, leaseTime, unit, threadId, RedisCommands.EVAL_LONG);&#125; 如果是没有传入时间，则此时也会进行抢锁， 而且抢锁时间是默认看门狗时间 commandExecutor.getConnectionManager().getCfg().getLockWatchdogTimeout()ttlRemainingFuture.onComplete((ttlRemaining, e) 这句话相当于对以上抢锁进行了监听，也就是说当上边抢锁完毕后，此方法会被调用，具体调用的逻辑就是去后台开启一个线程，进行续约逻辑，也就是看门狗线程 1234567891011121314RFuture&lt;Long&gt; ttlRemainingFuture = tryLockInnerAsync(waitTime, commandExecutor.getConnectionManager().getCfg().getLockWatchdogTimeout(), TimeUnit.MILLISECONDS, threadId, RedisCommands.EVAL_LONG);ttlRemainingFuture.onComplete((ttlRemaining, e) -&gt; &#123; if (e != null) &#123; return; &#125; // lock acquired if (ttlRemaining == null) &#123; scheduleExpirationRenewal(threadId); &#125;&#125;);return ttlRemainingFuture; 此逻辑就是续约逻辑，注意看commandExecutor.getConnectionManager().newTimeout（） 此方法Method( new TimerTask() {},参数2 ，参数3 )指的是：通过参数2，参数3 去描述什么时候去做参数1的事情，现在的情况是：10s之后去做参数一的事情因为锁的失效时间是30s，当10s之后，此时这个timeTask 就触发了，他就去进行续约，把当前这把锁续约成30s，如果操作成功，那么此时就会递归调用自己，再重新设置一个timeTask()，于是再过10s后又再设置一个timerTask，完成不停的续约那么大家可以想一想，假设我们的线程出现了宕机他还会续约吗？当然不会，因为没有人再去调用renewExpiration这个方法，所以等到时间之后自然就释放了。 1234567891011121314151617181920212223242526272829303132333435private void renewExpiration() &#123; ExpirationEntry ee = EXPIRATION_RENEWAL_MAP.get(getEntryName()); if (ee == null) &#123; return; &#125; Timeout task = commandExecutor.getConnectionManager().newTimeout(new TimerTask() &#123; @Override public void run(Timeout timeout) throws Exception &#123; ExpirationEntry ent = EXPIRATION_RENEWAL_MAP.get(getEntryName()); if (ent == null) &#123; return; &#125; Long threadId = ent.getFirstThreadId(); if (threadId == null) &#123; return; &#125; RFuture&lt;Boolean&gt; future = renewExpirationAsync(threadId); future.onComplete((res, e) -&gt; &#123; if (e != null) &#123; log.error(&quot;Can&#x27;t update lock &quot; + getName() + &quot; expiration&quot;, e); return; &#125; if (res) &#123; // reschedule itself renewExpiration(); &#125; &#125;); &#125; &#125;, internalLockLeaseTime / 3, TimeUnit.MILLISECONDS); ee.setTimeout(task);&#125; redission锁的MutiLock原理 连锁***为了提高redis的可用性，我们会搭建集群或者主从，以主从为例,此时我们去写命令，写在主机上， 主机会将数据同步给从机，但是假设在主机还没有来得及把数据写入到从机去的时候，此时主机宕机，哨兵会发现主机宕机，并且选举一个slave变成master，而此时新的master中实际上并没有锁信息，此时锁信息就已经丢掉了。为了解决这个问题，redission提出来了MutiLock锁，使用这把锁咱们就不使用主从了，每个节点的地位都是一样的， 这把锁加锁的逻辑需要写入到每一个主丛节点上，只有所有的服务器都写入成功，此时才是加锁成功，假设现在某个节点挂了，那么他去获得锁的时候，只要有一个节点拿不到，都不能算是加锁成功，就保证了加锁的可靠性。原理： 将所有的集群都当成独立节点而不是主从 加锁需要写到所有的节点上 所有的服务器都写入成功才算成功 当我们去设置了多个锁时，redission会将多个锁添加到一个集合中，然后用while循环去不停去尝试拿锁，但是会有一个总共的加锁时间，这个时间是用需要加锁的个数 * 1500ms ，假设有3个锁，那么时间就是4500ms，假设在这4500ms内，所有的锁都加锁成功， 那么此时才算是加锁成功，如果在4500ms有线程加锁失败，则会再次去进行重试. Redisson 的 multiLock 原理:多个独立的redis节点,必须在所有节点都获取重入锁,才算获取成功; 缺陷:运维成本高 优化秒杀 ***我们来回顾一下下单流程当用户发起请求，此时会请求nginx，nginx会访问到tomcat，而tomcat中的程序，会进行串行操作，分成如下几个步骤1、查询优惠卷2、判断秒杀库存是否足够3、查询订单4、校验是否是一人一单5、扣减库存6、创建订单在这六步操作中，又有很多操作是要去操作数据库的，而且还是一个线程串行执行， 这样就会导致我们的程序执行的很慢，所以我们需要异步程序执行，那么如何加速呢？在这里笔者想给大家分享一下课程内没有的思路，看看有没有小伙伴这么想，比如，我们可以不可以使用异步编排来做，或者说我开启N多线程，N多个线程，一个线程执行查询优惠卷，一个执行判断扣减库存，一个去创建订单等等，然后再统一做返回，这种做法和课程中有哪种好呢？答案是课程中的好，因为如果你采用我刚说的方式，如果访问的人很多，那么线程池中的线程可能一下子就被消耗完了，而且你使用上述方案，最大的特点在于，你觉得时效性会非常重要，但是你想想是吗？并不是，比如我只要确定他能做这件事，然后我后边慢慢做就可以了，我并不需要他一口气做完这件事，所以我们应当采用的是课程中，类似消息队列的方式来完成我们的需求，而不是使用线程池或者是异步编排的方式来完成这个需求 优化方案：***我们将耗时比较短的逻辑判断放入到redis中，比如是否库存足够，比如是否一人一单，这样的操作，只要这种逻辑可以完成，就意味着我们是一定可以下单完成的，我们只需要进行快速的逻辑判断，根本就不用等下单逻辑走完，我们直接给用户返回成功， 再在后台开一个线程，后台线程慢慢的去执行queue里边的消息，这样程序不就超级快了吗？而且也不用担心线程池消耗殆尽的问题，因为这里我们的程序中并没有手动使用任何线程池，当然这里边有两个难点 第一个难点是我们怎么在redis中去快速校验一人一单，还有库存判断第二个难点是由于我们校验和tomct下单是两个线程，那么我们如何知道到底哪个单他最后是否成功，或者是下单完成，为了完成这件事我们在redis操作完之后，我们会将一些信息返回给前端，同时也会把这些信息丢到异步queue中去，后续操作中，可以通过这个id来查询我们tomcat中的下单逻辑是否完成了。我们现在来看看整体思路：当用户下单之后，判断库存是否充足只需要到redis中去根据key找对应的value是否大于0即可，如果不充足，则直接结束，如果充足，继续在redis中判断用户是否可以下单，如果set集合中没有这条数据，说明他可以下单，如果set集合中没有这条记录，则将userId和优惠卷存入到redis中，并且返回0，整个过程需要保证是原子性的，我们可以使用lua来操作（判断库存是否充足 使用set结构来记录用户是否下过单）当以上判断逻辑走完之后，我们可以判断当前redis中返回的结果是否是0 ，如果是0，则表示可以下单，则将之前说的信息存入到到queue中去，然后返回，然后再来个线程异步的下单，前端可以通过返回的订单id来判断是否下单成功。 说白了 先将预先的数据加载在redis中 由redis去处理秒杀（库存是否充足，用户是否重复） 在根据redis中最终的购买的用户，优惠券id的结果 开启独立线程进行业务操作写入数据库 这样就大大减少了等待时间 Redis完成秒杀资格判断***实现逻辑： 首先将数据加载到redis中 所有请求通过redis去判断秒杀的资格 决定请求是否成功（库存是否充足，是否重复下单）（使用lua脚本来保证原子性） 抢购成功(执行lua脚本 根据结果 判断是否是0–可下单，1—库存不足，2—不可重复下单)，将优惠券id和用户id封装后存入阻塞队列 开启异步线程 不断从阻塞队列中获取信息，实现异步下单功能 就像是点餐时候 先下单 给你小票 厨师根据小票一个个去做 代码实现 12345678910111213141516@Override @Transactional public void addSeckillVoucher(Voucher voucher) &#123; // 保存优惠券 save(voucher); // 保存秒杀信息 SeckillVoucher seckillVoucher = new SeckillVoucher(); seckillVoucher.setVoucherId(voucher.getId()); seckillVoucher.setStock(voucher.getStock()); seckillVoucher.setBeginTime(voucher.getBeginTime()); seckillVoucher.setEndTime(voucher.getEndTime()); seckillVoucherService.save(seckillVoucher); // 保存秒杀库存到Redis中 // 不用设置过期因为这是个秒杀业务 我们手动删除稳妥一些 stringRedisTemplate.opsForValue().set(SECKILL_STOCK_KEY + voucher.getId(), voucher.getStock().toString()); &#125; 1234567891011121314151617181920212223242526272829303132-- 1.参数列表-- 1.1.优惠券idlocal voucherId = ARGV[1]-- 1.2.用户idlocal userId = ARGV[2]-- 1.3.订单idlocal orderId = ARGV[3]-- 2.数据key-- 2.1.库存keylocal stockKey = &#x27;seckill:stock:&#x27; .. voucherId-- 2.2.订单keylocal orderKey = &#x27;seckill:order:&#x27; .. voucherId-- 3.脚本业务-- 3.1.判断库存是否充足 get stockKeyif(tonumber(redis.call(&#x27;get&#x27;, stockKey)) &lt;= 0) then -- 3.2.库存不足，返回1 return 1end-- 3.2.判断用户是否下单 SISMEMBER orderKey userIdif(redis.call(&#x27;sismember&#x27;, orderKey, userId) == 1) then -- 3.3.存在，说明是重复下单，返回2 return 2end-- 3.4.扣库存 incrby stockKey -1redis.call(&#x27;incrby&#x27;, stockKey, -1)-- 3.5.下单（保存用户）sadd orderKey userIdredis.call(&#x27;sadd&#x27;, orderKey, userId)-- 3.6.发送消息到队列中， XADD stream.orders * k1 v1 k2 v2 ...redis.call(&#x27;xadd&#x27;, &#x27;stream.orders&#x27;, &#x27;*&#x27;, &#x27;userId&#x27;, userId, &#x27;voucherId&#x27;, voucherId, &#x27;id&#x27;, orderId)return 0 当以上lua表达式执行完毕后 我们就获取到了可下单结果，剩下的就是根据步骤3,4来执行我们接下来的任务了 1234567891011121314151617181920212223242526272829303132333435363738private static final DefaultRedisScript&lt;Long&gt; SECKILL_SCRIPT;static &#123; SECKILL_SCRIPT = new DefaultRedisScript&lt;&gt;(); SECKILL_SCRIPT.setLocation(new ClassPathResource(&quot;seckill.lua&quot;)); SECKILL_SCRIPT.setResultType(Long.class);&#125; // 阻塞队列 当一个线程尝试从这里获取的时候 如果没有元素线程就会被阻塞 直到他有元素 才会被唤醒 private BlockingQueue&lt;VoucherOrder&gt; orderTasks =new ArrayBlockingQueue&lt;&gt;(1024 * 1024); @Override public Result seckillVoucher(Long voucherId) &#123; //获取用户 Long userId = UserHolder.getUser().getId(); long orderId = redisIdWorker.nextId(&quot;order&quot;); // 1.执行lua脚本 Long result = stringRedisTemplate.execute( SECKILL_SCRIPT, Collections.emptyList(), voucherId.toString(), userId.toString(), String.valueOf(orderId) ); int r = result.intValue(); // 2.判断结果是否为0 if (r != 0) &#123; // 2.1.不为0 ，代表没有购买资格 return Result.fail(r == 1 ? &quot;库存不足&quot; : &quot;不能重复下单&quot;); &#125; // 保存阻塞队列 VoucherOrder voucherOrder = new VoucherOrder(); // 2.3.订单id voucherOrder.setId(orderId); // 2.4.用户id voucherOrder.setUserId(userId); // 2.5.代金券id voucherOrder.setVoucherId(voucherId); // 2.6.放入阻塞队列 orderTasks.add(voucherOrder); // 3.返回订单id return Result.ok(orderId); &#125; 以上根据redis处理lua脚本（实现了判断业务让redis去处理） 直接返回了可下单资格 现在需要开启异步线程从阻塞队列中一个个获取实现基于阻塞队列实现秒杀优化* 关键** 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109//异步处理线程池private static final ExecutorService SECKILL_ORDER_EXECUTOR = Executors.newSingleThreadExecutor();// spring注解 在类初始化之后执行，因为当这个类初始化好了之后，随时都是有可能要执行的@PostConstructprivate void init() &#123; // submit方法执行就会调用 线程的run方法 SECKILL_ORDER_EXECUTOR.submit(new VoucherOrderHandler());&#125;// 用于线程池处理的任务// 当初始化完毕后，就会去从对列中去拿信息 private class VoucherOrderHandler implements Runnable&#123; @Override public void run() &#123; while (true)&#123; try &#123; // 1.获取队列中的订单信息 VoucherOrder voucherOrder = orderTasks.take(); // 2.创建订单 handleVoucherOrder(voucherOrder); &#125; catch (Exception e) &#123; log.error(&quot;处理订单异常&quot;, e); &#125; &#125; &#125; private void handleVoucherOrder(VoucherOrder voucherOrder) &#123; //1.获取用户 Long userId = voucherOrder.getUserId(); // 2.创建锁对象 RLock redisLock = redissonClient.getLock(&quot;lock:order:&quot; + userId); // 3.尝试获取锁 boolean isLock = redisLock.lock(); // 4.判断是否获得锁成功 if (!isLock) &#123; // 获取锁失败，直接返回失败或者重试 log.error(&quot;不允许重复下单！&quot;); return; &#125; try &#123; //注意：由于是spring的事务是放在threadLocal中，此时的是多线程，事务会失效 proxy.createVoucherOrder(voucherOrder); &#125; finally &#123; // 释放锁 redisLock.unlock(); &#125; &#125; // 阻塞队列 当一个线程尝试从这里获取的时候 如果没有元素线程就会被阻塞 直到他有元素 才会被唤醒 private BlockingQueue&lt;VoucherOrder&gt; orderTasks =new ArrayBlockingQueue&lt;&gt;(1024 * 1024); @Override public Result seckillVoucher(Long voucherId) &#123; Long userId = UserHolder.getUser().getId(); long orderId = redisIdWorker.nextId(&quot;order&quot;); // 1.执行lua脚本 Long result = stringRedisTemplate.execute( SECKILL_SCRIPT, Collections.emptyList(), voucherId.toString(), userId.toString(), String.valueOf(orderId) ); int r = result.intValue(); // 2.判断结果是否为0 if (r != 0) &#123; // 2.1.不为0 ，代表没有购买资格 return Result.fail(r == 1 ? &quot;库存不足&quot; : &quot;不能重复下单&quot;); &#125; VoucherOrder voucherOrder = new VoucherOrder(); // 2.3.订单id long orderId = redisIdWorker.nextId(&quot;order&quot;); voucherOrder.setId(orderId); // 2.4.用户id voucherOrder.setUserId(userId); // 2.5.代金券id voucherOrder.setVoucherId(voucherId); // 2.6.放入阻塞队列 orderTasks.add(voucherOrder); //3.获取代理对象 proxy = (IVoucherOrderService)AopContext.currentProxy(); //4.返回订单id return Result.ok(orderId); &#125; @Transactional public void createVoucherOrder(VoucherOrder voucherOrder) &#123; Long userId = voucherOrder.getUserId(); // 5.1.查询订单 int count = query().eq(&quot;user_id&quot;, userId).eq(&quot;voucher_id&quot;, voucherOrder.getVoucherId()).count(); // 5.2.判断是否存在 if (count &gt; 0) &#123; // 用户已经购买过了 log.error(&quot;用户已经购买过了&quot;); return ; &#125; // 6.扣减库存 boolean success = seckillVoucherService.update() .setSql(&quot;stock = stock - 1&quot;) // set stock = stock - 1 .eq(&quot;voucher_id&quot;, voucherOrder.getVoucherId()).gt(&quot;stock&quot;, 0) // where id = ? and stock &gt; 0 .update(); if (!success) &#123; // 扣减失败 log.error(&quot;库存不足&quot;); return ; &#125; save(voucherOrder); &#125; 小总结：秒杀业务的优化思路是什么？ 先利用Redis完成库存余量、一人一单判断，有资格直接结束 完成抢单业务，响应时间就大大减少 再将下单业务放入阻塞队列，利用独立线程异步下单 基于阻塞队列的异步秒杀存在哪些问题？ 内存限制问题（我们的阻塞队列是自己制定的 它的大小也是自己指定的 万一他满了 剩下的不就溢出了吗） 数据安全问题（如果队列中取数据的时候 发生错误了 这个任务就不会执行 任务离开队列就再也不会执行了） 如何解决？ 消息队列 Redis消息队列 ***redis自己的消息队列 一般用其他的消息队列（rabbitmq，kafka） 认识消息队列什么是消息队列：字面意思就是存放消息的队列。最简单的消息队列模型包括3个角色： 消息队列：存储和管理消息，也被称为消息代理（Message Broker） 生产者：发送消息到消息队列 消费者：从消息队列获取消息并处理消息 使用队列的好处在于 解耦：所谓解耦，举一个生活中的例子就是：快递员(生产者)把快递放到快递柜里边(Message Queue)去，我们(消费者)从快递柜里边去拿东西，这就是一个异步，如果耦合，那么这个快递员相当于直接把快递交给你，这事固然好，但是万一你不在家，那么快递员就会一直等你，这就浪费了快递员的时间，所以这种思想在我们日常开发中，是非常有必要的。这种场景在我们秒杀中就变成了：我们下单之后，利用redis去进行校验下单条件，再通过队列把消息发送出去，然后再启动一个线程去消费这个消息，完成解耦，同时也加快我们的响应速度。这里我们可以使用一些现成的mq，比如kafka，rabbitmq等等，但是呢，如果没有安装mq，我们也可以直接使用redis提供的mq方案，降低我们的部署和学习成本。 Redis消息队列-基于List实现消息队列基于List结构模拟消息队列 基本满足但是也有问题消息队列（Message Queue），字面意思就是存放消息的队列。而Redis的list数据结构是一个双向链表，很容易模拟出队列效果。队列是入口和出口不在一边，因此我们可以利用：LPUSH 结合 RPOP、或者 RPUSH 结合 LPOP来实现。不过要注意的是，当队列中没有消息时RPOP或LPOP操作会返回null，并不像JVM的阻塞队列那样会阻塞并等待消息。因此这里应该使用BRPOP或者BLPOP来实现阻塞效果。 基于List的消息队列有哪些优缺点？优点： 利用Redis存储，不受限于JVM内存上限 基于Redis的持久化机制，数据安全性有保证 可以满足消息有序性 缺点： 无法避免消息丢失 只支持单消费者 Redis消息队列-基于PubSub的消息队列不太行PubSub（发布订阅）是Redis2.0版本引入的消息传递模型。顾名思义，消费者可以订阅一个或多个channel，生产者向对应channel发送消息后，所有订阅者都能收到相关消息。SUBSCRIBE channel [channel] ：订阅一个或多个频道 PUBLISH channel msg ：向一个频道发送消息 PSUBSCRIBE pattern[pattern] ：订阅与pattern格式匹配的所有频道 基于PubSub的消息队列有哪些优缺点？优点： 采用发布订阅模型，支持多生产、多消费 缺点： 不支持数据持久化 无法避免消息丢失 消息堆积有上限，超出时数据丢失 Redis消息队列-基于Stream的消息队列 **Stream 是 Redis 5.0 引入的一种新数据类型，可以实现一个功能非常完善的消息队列。单消费模式注意：当我们指定起始ID为$时，代表读取最新的消息，如果我们处理一条消息的过程中，又有超过1条以上的消息到达队列，则下次获取时也只能获取到最新的一条，会出现漏读消息的问题STREAM类型消息队列的XREAD命令特点： 消息可回溯 一个消息可以被多个消费者读取 可以阻塞读取 有消息漏读的风险 基于Stream的消息队列-消费者组消费者组（Consumer Group）：将多个消费者划分到一个组中，监听同一个队列。具备下列特点：创建消费者组：key：队列名称groupName：消费者组名称ID：起始ID标示，$代表队列中最后一个消息，0则代表队列中第一个消息MKSTREAM：队列不存在时自动创建队列其它常见命令：删除指定的消费者组XGROUP DESTORY key groupName给指定的消费者组添加消费者XGROUP CREATECONSUMER key groupname consumername删除消费者组中的指定消费者XGROUP DELCONSUMER key groupname consumername从消费者组读取消息：XREADGROUP GROUP group consumer [COUNT count] [BLOCK milliseconds] [NOACK] STREAMS key [key ...] ID [ID ...] group：消费组名称 consumer：消费者名称，如果消费者不存在，会自动创建一个消费者 count：本次查询的最大数量 BLOCK milliseconds：当没有消息时最长等待时间 NOACK：无需手动ACK，获取到消息后自动确认 STREAMS key：指定队列名称 ID：获取消息的起始ID： “&gt;”：从下一个未消费的消息开始其它：根据指定id从pending-list中获取已消费但未确认的消息，例如0，是从pending-list中的第一个消息开始消费者监听消息的基本思路：列的XREADGROUP命令特点： 消息可回溯 可以多消费者争抢消息，加快消费速度 可以阻塞读取 没有消息漏读的风险 有消息确认机制，保证消息至少被消费一次 基于Redis的Stream结构作为消息队列，实现异步秒杀下单***将用户下单从原本的阻塞队列变为stream的消息队列中需求： 创建一个Stream类型的消息队列，名为stream.orders 修改之前的秒杀下单Lua脚本，在认定有抢购资格后，直接向stream.orders中添加消息，内容包含voucherId、userId、orderId（将原本的存入阻塞队列的加入到消息队列中） 项目启动时，开启一个线程任务，尝试获取stream.orders中的消息，完成下单12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667private class VoucherOrderHandler implements Runnable &#123; @Override public void run() &#123; while (true) &#123; try &#123; // 1.获取消息队列中的订单信息 XREADGROUP GROUP g1 c1 COUNT 1 BLOCK 2000 STREAMS s1 &gt; List&lt;MapRecord&lt;String, Object, Object&gt;&gt; list = stringRedisTemplate.opsForStream().read( Consumer.from(&quot;g1&quot;, &quot;c1&quot;), StreamReadOptions.empty().count(1).block(Duration.ofSeconds(2)), StreamOffset.create(&quot;stream.orders&quot;, ReadOffset.lastConsumed()) ); // 2.判断订单信息是否为空 if (list == null || list.isEmpty()) &#123; // 如果为null，说明没有消息，继续下一次循环 continue; &#125; // 解析数据 MapRecord&lt;String, Object, Object&gt; record = list.get(0); Map&lt;Object, Object&gt; value = record.getValue(); VoucherOrder voucherOrder = BeanUtil.fillBeanWithMap(value, new VoucherOrder(), true); // 3.创建订单 createVoucherOrder(voucherOrder); // 4.确认消息 XACK stringRedisTemplate.opsForStream().acknowledge(&quot;s1&quot;, &quot;g1&quot;, record.getId()); &#125; catch (Exception e) &#123; log.error(&quot;处理订单异常&quot;, e); //处理异常消息 handlePendingList(); &#125; &#125; &#125; private void handlePendingList() &#123; while (true) &#123; try &#123; // 1.获取pending-list中的订单信息 XREADGROUP GROUP g1 c1 COUNT 1 BLOCK 2000 STREAMS s1 0 List&lt;MapRecord&lt;String, Object, Object&gt;&gt; list = stringRedisTemplate.opsForStream().read( Consumer.from(&quot;g1&quot;, &quot;c1&quot;), StreamReadOptions.empty().count(1), StreamOffset.create(&quot;stream.orders&quot;, ReadOffset.from(&quot;0&quot;)) ); // 2.判断订单信息是否为空 if (list == null || list.isEmpty()) &#123; // 如果为null，说明没有异常消息，结束循环 break; &#125; // 解析数据 MapRecord&lt;String, Object, Object&gt; record = list.get(0); Map&lt;Object, Object&gt; value = record.getValue(); VoucherOrder voucherOrder = BeanUtil.fillBeanWithMap(value, new VoucherOrder(), true); // 3.创建订单 createVoucherOrder(voucherOrder); // 4.确认消息 XACK stringRedisTemplate.opsForStream().acknowledge(&quot;s1&quot;, &quot;g1&quot;, record.getId()); &#125; catch (Exception e) &#123; log.error(&quot;处理pendding订单异常&quot;, e); try&#123; Thread.sleep(20); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 总结：秒杀使用 lua+消息队列+redis 达人探店 set sortset 点赞发布探店笔记发布探店笔记探店笔记类似点评网站的评价，往往是图文结合。对应的表有两个：tb_blog：探店笔记表，包含笔记中的标题、文字、图片等tb_blog_comments：其他用户对探店笔记的评价 1234567891011121314151617181920212223@Slf4j@RestController@RequestMapping(&quot;upload&quot;)public class UploadController &#123; @PostMapping(&quot;blog&quot;) public Result uploadImage(@RequestParam(&quot;file&quot;) MultipartFile image) &#123; try &#123; // 获取原始文件名称 String originalFilename = image.getOriginalFilename(); // 生成新文件名 String fileName = createNewFileName(originalFilename); // 保存文件 image.transferTo(new File(SystemConstants.IMAGE_UPLOAD_DIR, fileName)); // 返回结果 log.debug(&quot;文件上传成功，&#123;&#125;&quot;, fileName); return Result.ok(fileName); &#125; catch (IOException e) &#123; throw new RuntimeException(&quot;文件上传失败&quot;, e); &#125; &#125;&#125; 注意：在实际开发中图片一般会放在nginx上或者是云存储上。 123456789101112131415161718@RestController@RequestMapping(&quot;/blog&quot;)public class BlogController &#123; @Resource private IBlogService blogService; @PostMapping public Result saveBlog(@RequestBody Blog blog) &#123; //获取登录用户 UserDTO user = UserHolder.getUser(); blog.setUpdateTime(user.getId()); //保存探店博文 blogService.saveBlog(blog); //返回id return Result.ok(blog.getId()); &#125;&#125; 查看探店笔记123456789101112@Overridepublic Result queryBlogById(Long id) &#123; // 1.查询blog Blog blog = getById(id); if (blog == null) &#123; return Result.fail(&quot;笔记不存在！&quot;); &#125; // 2.查询blog有关的用户 queryBlogUser(blog); return Result.ok(blog);&#125; 点赞功能***初始代码 123456@GetMapping(&quot;/likes/&#123;id&#125;&quot;)public Result queryBlogLikes(@PathVariable(&quot;id&quot;) Long id) &#123; //修改点赞数量 blogService.update().setSql(&quot;liked = liked +1 &quot;).eq(&quot;id&quot;,id).update(); return Result.ok();&#125; 问题分析：这种方式会导致一个用户无限点赞，明显是不合理的造成这个问题的原因是，我们现在的逻辑，发起请求只是给数据库+1，所以才会出现这个问题 完善点赞功能需求： 同一个用户只能点赞一次，再次点击则取消点赞 如果当前用户已经点赞，则点赞按钮高亮显示（前端已实现，判断字段Blog类的isLike属性） 实现步骤： 给Blog类中添加一个isLike字段，标示是否被当前用户点赞 修改点赞功能，利用Redis的set集合判断是否点赞过，未点赞过则点赞数+1，已点赞过则点赞数-1 修改根据id查询Blog的业务，判断当前登录用户是否点赞过，赋值给isLike字段 修改分页查询Blog业务，判断当前登录用户是否点赞过，赋值给isLike字段 为什么采用set集合：因为我们的数据是不能重复的，当用户操作过之后，无论他怎么操作，都是不会增加新元素的 1234567891011121314151617181920212223242526@Override public Result likeBlog(Long id) &#123; // 1.获取登录用户 Long userId = UserHolder.getUser().getId(); // 2.判断当前登录用户是否已经点赞 String key = BLOG_LIKED_KEY + id; Boolean isMember = stringRedisTemplate.opsForSet().isMember(key, userId.toString()); if (BooleanUtil.isFalse(isMember)) &#123; //3.如果未点赞，可以点赞 //3.1 数据库点赞数+1 boolean isSuccess = update().setSql(&quot;liked = liked + 1&quot;).eq(&quot;id&quot;, id).update(); //3.2 保存用户到Redis的set集合 if (isSuccess) &#123; stringRedisTemplate.opsForSet().add(key, userId.toString()); &#125; &#125; else &#123; //4.如果已点赞，取消点赞 //4.1 数据库点赞数-1 boolean isSuccess = update().setSql(&quot;liked = liked - 1&quot;).eq(&quot;id&quot;, id).update(); //4.2 把用户从Redis的set集合移除 if (isSuccess) &#123; stringRedisTemplate.opsForSet().remove(key, userId.toString()); &#125; &#125; return Result.ok(); &#125; 点赞排行榜***在探店笔记的详情页面，应该把给该笔记点赞的人显示出来，比如最早点赞的TOP5，形成点赞排行榜：之前的点赞是放到set集合，但是set集合是不能排序的，所以这个时候，咱们可以采用一个可以排序的set集合，就是咱们的sortedSet 修改点赞代码 1234567891011121314151617181920212223242526272829303132333435363738394041@Override public Result likeBlog(Long id) &#123; // 1.获取登录用户 Long userId = UserHolder.getUser().getId(); // 2.判断当前登录用户是否已经点赞 String key = BLOG_LIKED_KEY + id; Double score = stringRedisTemplate.opsForZSet().score(key, userId.toString()); if (score == null) &#123; // 3.如果未点赞，可以点赞 // 3.1.数据库点赞数 + 1 boolean isSuccess = update().setSql(&quot;liked = liked + 1&quot;).eq(&quot;id&quot;, id).update(); // 3.2.保存用户到Redis的set集合 zadd key value score if (isSuccess) &#123; stringRedisTemplate.opsForZSet().add(key, userId.toString(), System.currentTimeMillis()); &#125; &#125; else &#123; // 4.如果已点赞，取消点赞 // 4.1.数据库点赞数 -1 boolean isSuccess = update().setSql(&quot;liked = liked - 1&quot;).eq(&quot;id&quot;, id).update(); // 4.2.把用户从Redis的set集合移除 if (isSuccess) &#123; stringRedisTemplate.opsForZSet().remove(key, userId.toString()); &#125; &#125; return Result.ok(); &#125; private void isBlogLiked(Blog blog) &#123; // 1.获取登录用户 UserDTO user = UserHolder.getUser(); if (user == null) &#123; // 用户未登录，无需查询是否点赞 return; &#125; Long userId = user.getId(); // 2.判断当前登录用户是否已经点赞 String key = &quot;blog:liked:&quot; + blog.getId(); Double score = stringRedisTemplate.opsForZSet().score(key, userId.toString()); blog.setIsLike(score != null); &#125; 点赞列表查询列表BlogController 12345@GetMapping(&quot;/likes/&#123;id&#125;&quot;)public Result queryBlogLikes(@PathVariable(&quot;id&quot;) Long id) &#123; return blogService.queryBlogLikes(id);&#125; BlogService 1234567891011121314151617181920@Overridepublic Result queryBlogLikes(Long id) &#123;String key = BLOG_LIKED_KEY + id;// 1.查询top5的点赞用户 zrange key 0 4Set&lt;String&gt; top5 = stringRedisTemplate.opsForZSet().range(key, 0, 4);if (top5 == null || top5.isEmpty()) &#123; return Result.ok(Collections.emptyList());&#125;// 2.解析出其中的用户idList&lt;Long&gt; ids = top5.stream().map(Long::valueOf).collect(Collectors.toList());String idStr = StrUtil.join(&quot;,&quot;, ids);// 3.根据用户id查询用户 WHERE id IN ( 5 , 1 ) ORDER BY FIELD(id, 5, 1)List&lt;UserDTO&gt; userDTOS = userService.query().in(&quot;id&quot;, ids).last(&quot;ORDER BY FIELD(id,&quot; + idStr + &quot;)&quot;).list().stream().map(user -&gt; BeanUtil.copyProperties(user, UserDTO.class)).collect(Collectors.toList());// 4.返回return Result.ok(userDTOS);&#125; 总结： 单纯的点赞使用 redis set结构就可以实现 但是如果要加入排行榜 就是用 sortSet 实现排行可以使用时间戳作为value 查询的时候可以使用这个时间戳作为排序的参照 好友关注关注和取消关注针对用户的操作：可以对用户进行关注和取消关注功能。 实现思路：需求：基于该表数据结构，实现两个接口： 关注和取关接口 判断是否关注的接口 关注是User之间的关系，是博主与粉丝的关系，数据库中有一张tb_follow表来标示（多对多关系表）只需要去添加删除关系表 来映射关注和取关 123456789101112131415161718192021222324252627282930313233取消关注service@Overridepublic Result isFollow(Long followUserId) &#123; // 1.获取登录用户 Long userId = UserHolder.getUser().getId(); // 2.查询是否关注 select count(*) from tb_follow where user_id = ? and follow_user_id = ? Integer count = query().eq(&quot;user_id&quot;, userId).eq(&quot;follow_user_id&quot;, followUserId).count(); // 3.判断 return Result.ok(count &gt; 0); &#125; 关注service @Override public Result follow(Long followUserId, Boolean isFollow) &#123; // 1.获取登录用户 Long userId = UserHolder.getUser().getId(); String key = &quot;follows:&quot; + userId; // 1.判断到底是关注还是取关 if (isFollow) &#123; // 2.关注，新增数据 Follow follow = new Follow(); follow.setUserId(userId); follow.setFollowUserId(followUserId); boolean isSuccess = save(follow); &#125; else &#123; // 3.取关，删除 delete from tb_follow where user_id = ? and follow_user_id = ? remove(new QueryWrapper&lt;Follow&gt;() .eq(&quot;user_id&quot;, userId).eq(&quot;follow_user_id&quot;, followUserId)); &#125; return Result.ok(); &#125; 共同关注共同关注如何实现：需求：利用Redis中恰当的数据结构，实现共同关注功能。在博主个人页面展示出当前用户与博主的共同关注呢。当然是使用我们之前学习过的set集合咯，在set集合中，有交集并集补集的api，我们可以把两人的关注的人分别放入到一个set集合中，然后再通过api去查看这两个set集合中的交集数据。 我们先来改造当前的关注列表改造原因是因为我们需要在用户关注了某位用户后，需要将数据放入到set集合中，方便后续进行共同关注，同时当取消关注时，也需要从set集合中进行删除 123456789101112131415161718192021222324252627@Overridepublic Result follow(Long followUserId, Boolean isFollow) &#123; // 1.获取登录用户 Long userId = UserHolder.getUser().getId(); String key = &quot;follows:&quot; + userId; // 1.判断到底是关注还是取关 if (isFollow) &#123; // 2.关注，新增数据 Follow follow = new Follow(); follow.setUserId(userId); follow.setFollowUserId(followUserId); boolean isSuccess = save(follow); if (isSuccess) &#123; // 把关注用户的id，放入redis的set集合 sadd userId followerUserId stringRedisTemplate.opsForSet().add(key, followUserId.toString()); &#125; &#125; else &#123; // 3.取关，删除 delete from tb_follow where user_id = ? and follow_user_id = ? boolean isSuccess = remove(new QueryWrapper&lt;Follow&gt;() .eq(&quot;user_id&quot;, userId).eq(&quot;follow_user_id&quot;, followUserId)); if (isSuccess) &#123; // 把关注用户的id从Redis集合中移除 stringRedisTemplate.opsForSet().remove(key, followUserId.toString()); &#125; &#125; return Result.ok();&#125; 具体的关注代码：FollowServiceImpl 123456789101112131415161718192021@Overridepublic Result followCommons(Long id) &#123;// 1.获取当前用户Long userId = UserHolder.getUser().getId();String key = &quot;follows:&quot; + userId;// 2.求交集String key2 = &quot;follows:&quot; + id;Set&lt;String&gt; intersect = stringRedisTemplate.opsForSet().intersect(key, key2);if (intersect == null || intersect.isEmpty()) &#123; // 无交集 return Result.ok(Collections.emptyList());&#125;// 3.解析id集合List&lt;Long&gt; ids = intersect.stream().map(Long::valueOf).collect(Collectors.toList());// 4.查询用户List&lt;UserDTO&gt; users = userService.listByIds(ids).stream().map(user -&gt; BeanUtil.copyProperties(user, UserDTO.class)).collect(Collectors.toList());return Result.ok(users);&#125; 好友关注-Feed流实现方案***当我们关注了用户后，这个用户发了动态，那么我们应该把这些数据推送给用户，这个需求，其实我们又把他叫做Feed流，关注推送也叫做Feed流，直译为投喂。为用户持续的提供“沉浸式”的体验，通过无限下拉刷新获取新的信息。（短视频的刷新）对于传统的模式的内容解锁：我们是需要用户去通过搜索引擎或者是其他的方式去解锁想要看的内容（主动寻找）对于新型的Feed流的的效果：不需要我们用户再去推送信息，而是系统分析用户到底想要什么，然后直接把内容推送给用户，从而使用户能够更加的节约时间，不用主动去寻找。（系统匹配） Feed流的实现有两种模式：Feed流产品有两种常见模式：Timeline：不做内容筛选，简单的按照内容发布时间排序，常用于好友或关注。例如朋友圈 优点：信息全面，不会有缺失。并且实现也相对简单 缺点：信息噪音较多，用户不一定感兴趣，内容获取效率低 智能排序：利用智能算法屏蔽掉违规的、用户不感兴趣的内容。推送用户感兴趣信息来吸引用户（抖音） 优点：投喂用户感兴趣信息，用户粘度很高，容易沉迷 缺点：如果算法不精准，可能起到反作用本例中的个人页面，是基于关注的好友来做Feed流，因此采用Timeline的模式。该模式的实现方案有三种 我们本次针对好友的操作，采用的就是Timeline的方式，只需要拿到我们关注用户的信息，然后按照时间排序即可，因此采用Timeline的模式。该模式的实现方案有三种： 拉模式 推模式 推拉结合 拉模式：也叫做读扩散 （发送的存在自己发件箱 用户直接去拉取）该模式的核心含义就是：当张三和李四和王五发了消息后，都会保存在自己的邮箱中，假设赵六要读取信息，那么他会从读取他自己的收件箱，此时系统会从他关注的人群中，把他关注人的信息全部都进行拉取，然后在进行排序优点：比较节约空间，因为赵六在读信息时，并没有重复读取，而且读取完之后可以把他的收件箱进行清楚。缺点：比较延迟，当用户读取数据时才去关注的人里边去读取数据，假设用户关注了大量的用户，那么此时就会拉取海量的内容，对服务器压力巨大。 推模式：也叫做写扩散。推模式是没有写邮箱的，当张三写了一个内容，此时会主动的把张三写的内容发送到他的粉丝收件箱中去，假设此时李四再来读取，就不用再去临时拉取了优点：时效快，不用临时拉取缺点：内存压力大，假设一个大V写信息，很多人关注他， 就会写很多分数据到粉丝那边去 推拉结合模式***：也叫做读写混合，兼具推和拉两种模式的优点。（对于活跃的粉丝用户 使用推模式推送 对于不那么活跃的 他需要时再去拉去）推拉模式是一个折中的方案，站在发件人这一段，如果是个普通的人，那么我们采用写扩散的方式，直接把数据写入到他的粉丝中去，因为普通的人他的粉丝关注量比较小，所以这样做没有压力，如果是大V，那么他是直接将数据先写入到一份到发件箱里边去，然后再直接写一份到活跃粉丝收件箱里边去，现在站在收件人这端来看，如果是活跃粉丝，那么大V和普通的人发的都会直接写入到自己收件箱里边来，而如果是普通的粉丝，由于他们上线不是很频繁，所以等他们上线时，再从发件箱里边去拉信息。 推送到粉丝收件箱 ***使用推模式实现（一发布直接推送到各个粉丝）这里采用sortedset而不是list 这里数据是有变化的 所以最好使用sortedset需求： 修改新增探店笔记的业务，在保存blog到数据库的同时，推送到粉丝的收件箱 收件箱满足可以根据时间戳排序，必须用Redis的数据结构实现 查询收件箱数据时，可以实现分页查询 Feed流中的数据会不断更新，所以数据的角标也在变化，因此不能采用传统的分页模式。传统了分页在feed流是不适用的，因为我们的数据会随时发生变化假设在t1 时刻，我们去读取第一页，此时page &#x3D; 1 ，size &#x3D; 5 ，那么我们拿到的就是106 这几条记录，假设现在t2时候又发布了一条记录，此时t3 时刻，我们来读取第二页，读取第二页传入的参数是page&#x3D;2 ，size&#x3D;5 ，那么此时读取到的第二页实际上是从6 开始，然后是62 ，那么我们就读取到了重复的数据，所以feed流的分页，不能采用原始方案来做。 Feed流的滚动分页我们需要记录每次操作的最后一条，然后从这个位置开始去读取数据举个例子：我们从t1时刻开始，拿第一页数据，拿到了10~6，然后记录下当前最后一次拿取的记录，就是6，t2时刻发布了新的记录，此时这个11放到最顶上，但是不会影响我们之前记录的6，此时t3时刻来拿第二页，第二页这个时候拿数据，还是从6后一点的5去拿，就拿到了5-1的记录。我们这个地方可以采用sortedSet来做，可以进行范围查询，并且还可以记录当前获取数据时间戳最小值，就可以实现滚动分页了 核心的思想：就是我们在保存完探店笔记后，获得到当前笔记的粉丝，然后把数据推送到粉丝的redis中去。 12345678910111213141516171819202122232425@Override public Result saveBlog(Blog blog) &#123; // 1.获取登录用户 UserDTO user = UserHolder.getUser(); blog.setUserId(user.getId()); // 2.保存探店笔记 boolean isSuccess = save(blog); if(!isSuccess)&#123; return Result.fail(&quot;新增笔记失败!&quot;); &#125; // 3.查询笔记作者的所有粉丝 select * from tb_follow where follow_user_id = ? List&lt;Follow&gt; follows = followService.query().eq(&quot;follow_user_id&quot;, user.getId()) .list(); // 4.推送笔记id给所有粉丝 for (Follow follow : follows) &#123; // 4.1.获取粉丝id Long userId = follow.getUserId(); // 4.2.推送 String key = FEED_KEY + userId; // 使用sortedset stringRedisTemplate.opsForZSet().add(key, blog.getId().toString(), System.currentTimeMillis()); &#125; // 5.返回id return Result.ok(blog.getId()); &#125; 现在已经推送了 存入了redis中 现在就需要用户去查看 也就是从redis中去取 实现分页查询收邮箱 *****滚动分页 *****如何使用sortedset达到滚动分页的效果呢 需求：在个人主页的“关注”卡片中，查询并展示推送的Blog信息：具体操作如下：1、每次查询完成后，我们要分析出查询出数据的最小时间戳，这个值会作为下一次查询的条件2、我们需要找到与上一次查询相同的查询个数作为偏移量，下次查询时，跳过这些查询过的数据，拿到我们需要的数据综上：我们的请求参数中就需要携带 lastId：上一次查询的最小时间戳 和偏移量这两个参数。这两个参数第一次会由前端来指定，以后的查询就根据后台结果作为条件，再次传递到后台。一、定义出来具体的返回值实体类 123456@Datapublic class ScrollResult &#123; private List&lt;?&gt; list; private Long minTime; private Integer offset;&#125; BlogController注意：RequestParam 表示接受url地址栏传参的注解，当方法上参数的名称和url地址栏不相同时，可以通过RequestParam 来进行指定 12345@GetMapping(&quot;/of/follow&quot;)public Result queryBlogOfFollow( @RequestParam(&quot;lastId&quot;) Long max, @RequestParam(value = &quot;offset&quot;, defaultValue = &quot;0&quot;) Integer offset)&#123; return blogService.queryBlogOfFollow(max, offset);&#125; BlogServiceImpl 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849@Overridepublic Result queryBlogOfFollow(Long max, Integer offset) &#123; // 1.获取当前用户 Long userId = UserHolder.getUser().getId(); // 2.查询收件箱 ZREVRANGEBYSCORE key Max Min LIMIT offset count String key = FEED_KEY + userId; Set&lt;ZSetOperations.TypedTuple&lt;String&gt;&gt; typedTuples = stringRedisTemplate.opsForZSet() .reverseRangeByScoreWithScores(key, 0, max, offset, 2); // 3.非空判断 if (typedTuples == null || typedTuples.isEmpty()) &#123; return Result.ok(); &#125; // 4.解析数据：blogId、minTime（时间戳）、offset // 这里的实现是重点 List&lt;Long&gt; ids = new ArrayList&lt;&gt;(typedTuples.size()); long minTime = 0; // 2 时间戳 也就是上面的max int os = 1; // 2 分页开始 for (ZSetOperations.TypedTuple&lt;String&gt; tuple : typedTuples) &#123; // 5 4 4 2 2 // 4.1.获取id ids.add(Long.valueOf(tuple.getValue())); // 4.2.获取分数(时间戳） long time = tuple.getScore().longValue(); if(time == minTime)&#123; os++; &#125;else&#123; minTime = time; os = 1; &#125; &#125; os = minTime == max ? os : os + offset; // 5.根据id查询blog String idStr = StrUtil.join(&quot;,&quot;, ids); List&lt;Blog&gt; blogs = query().in(&quot;id&quot;, ids).last(&quot;ORDER BY FIELD(id,&quot; + idStr + &quot;)&quot;).list(); for (Blog blog : blogs) &#123; // 5.1.查询blog有关的用户 queryBlogUser(blog); // 5.2.查询blog是否被点赞 isBlogLiked(blog); &#125; // 6.封装并返回 ScrollResult r = new ScrollResult(); r.setList(blogs); r.setOffset(os); r.setMinTime(minTime); return Result.ok(r);&#125; 附近商户 基于地理位置查询附近GEO数据结构的基本用法GEO就是Geolocation的简写形式，代表地理坐标。Redis在3.2版本中加入了对GEO的支持，允许存储地理坐标信息，帮助我们根据经纬度来检索数据。常见的命令有： GEOADD：添加一个地理空间信息，包含：经度（longitude）、纬度（latitude）、值（member） GEODIST：计算指定的两个点之间的距离并返回 GEOHASH：将指定member的坐标转为hash字符串形式并返回 GEOPOS：返回指定member的坐标 GEORADIUS：指定圆心、半径，找到该圆内包含的所有member，并按照与圆心之间的距离排序后返回。6.以后已废弃 GEOSEARCH：在指定范围内搜索member，并按照与指定点之间的距离排序后返回。范围可以是圆形或矩形。6.2.新功能 GEOSEARCHSTORE：与GEOSEARCH功能一致，不过可以把结果存储到一个指定的key。 6.2.新功能 导入店铺数据到GEO将每个商铺的具体地理位置导入到redis 这样才能根据位置来计算 当我们点击美食之后，会出现一系列的商家，商家中可以按照多种排序方式，我们此时关注的是距离，这个地方就需要使用到我们的GEO，向后台传入当前app收集的地址(我们此处是写死的) ，以当前坐标作为圆心，同时绑定相同的店家类型type，以及分页信息，把这几个条件传入后台，后台查询出对应的数据再返回。我们要做的事情是：将数据库表中的数据导入到redis中去，redis中的GEO，GEO在redis中就一个menber和一个经纬度，我们把x和y轴传入到redis做的经纬度位置去，但我们不能把所有的数据都放入到menber中去，毕竟作为redis是一个内存级数据库，如果存海量数据，redis还是力不从心，所以我们在这个地方存储他的id即可。但是这个时候还有一个问题，就是在redis中并没有存储type，所以我们无法根据type来对数据进行筛选，所以我们可以按照商户类型做分组，类型相同的商户作为同一组，以typeId为key存入同一个GEO集合中即可 1234567891011121314151617181920212223242526@Testvoid loadShopData() &#123; // 1.查询店铺信息 List&lt;Shop&gt; list = shopService.list(); // 2.把店铺分组，按照typeId分组，typeId一致的放到一个集合 Map&lt;Long, List&lt;Shop&gt;&gt; map = list.stream().collect(Collectors.groupingBy(Shop::getTypeId)); // 3.分批完成写入Redis for (Map.Entry&lt;Long, List&lt;Shop&gt;&gt; entry : map.entrySet()) &#123; // 3.1.获取类型id Long typeId = entry.getKey(); String key = SHOP_GEO_KEY + typeId; // 3.2.获取同类型的店铺的集合 List&lt;Shop&gt; value = entry.getValue(); List&lt;RedisGeoCommands.GeoLocation&lt;String&gt;&gt; locations = new ArrayList&lt;&gt;(value.size()); // 3.3.写入redis GEOADD key 经度 纬度 member for (Shop shop : value) &#123; // stringRedisTemplate.opsForGeo().add(key, new Point(shop.getX(), shop.getY()), shop.getId().toString()); locations.add(new RedisGeoCommands.GeoLocation&lt;&gt;( shop.getId().toString(), new Point(shop.getX(), shop.getY()) )); &#125; stringRedisTemplate.opsForGeo().add(key, locations); &#125;&#125; 实现附近商户功能SpringDataRedis的2.3.9版本并不支持Redis 6.2提供的GEOSEARCH命令，因此我们需要提示其版本，修改自己的POM第一步：导入pom 123456789101112131415161718192021222324&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;spring-data-redis&lt;/artifactId&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;artifactId&gt;lettuce-core&lt;/artifactId&gt; &lt;groupId&gt;io.lettuce&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-redis&lt;/artifactId&gt; &lt;version&gt;2.6.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.lettuce&lt;/groupId&gt; &lt;artifactId&gt;lettuce-core&lt;/artifactId&gt; &lt;version&gt;6.1.6.RELEASE&lt;/version&gt;&lt;/dependency&gt; 第二步：ShopController 123456789@GetMapping(&quot;/of/type&quot;)public Result queryShopByType( @RequestParam(&quot;typeId&quot;) Integer typeId, @RequestParam(value = &quot;current&quot;, defaultValue = &quot;1&quot;) Integer current, @RequestParam(value = &quot;x&quot;, required = false) Double x, @RequestParam(value = &quot;y&quot;, required = false) Double y) &#123; return shopService.queryShopByType(typeId, current, x, y);&#125; ShopServiceImpl 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354@Overridepublic Result queryShopByType(Integer typeId, Integer current, Double x, Double y) &#123; // 1.判断是否需要根据坐标查询 if (x == null || y == null) &#123; // 不需要坐标查询，按数据库查询 Page&lt;Shop&gt; page = query() .eq(&quot;type_id&quot;, typeId) .page(new Page&lt;&gt;(current, SystemConstants.DEFAULT_PAGE_SIZE)); // 返回数据 return Result.ok(page.getRecords()); &#125; // 2.计算分页参数 int from = (current - 1) * SystemConstants.DEFAULT_PAGE_SIZE; int end = current * SystemConstants.DEFAULT_PAGE_SIZE; // 3.查询redis、按照距离排序、分页。结果：shopId、distance String key = SHOP_GEO_KEY + typeId; GeoResults&lt;RedisGeoCommands.GeoLocation&lt;String&gt;&gt; results = stringRedisTemplate.opsForGeo() // GEOSEARCH key BYLONLAT x y BYRADIUS 10 WITHDISTANCE .search( key, GeoReference.fromCoordinate(x, y), new Distance(5000), RedisGeoCommands.GeoSearchCommandArgs.newGeoSearchArgs().includeDistance().limit(end) ); // 4.解析出id if (results == null) &#123; return Result.ok(Collections.emptyList()); &#125; List&lt;GeoResult&lt;RedisGeoCommands.GeoLocation&lt;String&gt;&gt;&gt; list = results.getContent(); if (list.size() &lt;= from) &#123; // 没有下一页了，结束 return Result.ok(Collections.emptyList()); &#125; // 4.1.截取 from ~ end的部分 List&lt;Long&gt; ids = new ArrayList&lt;&gt;(list.size()); Map&lt;String, Distance&gt; distanceMap = new HashMap&lt;&gt;(list.size()); list.stream().skip(from).forEach(result -&gt; &#123; // 4.2.获取店铺id String shopIdStr = result.getContent().getName(); ids.add(Long.valueOf(shopIdStr)); // 4.3.获取距离 Distance distance = result.getDistance(); distanceMap.put(shopIdStr, distance); &#125;); // 5.根据id查询Shop String idStr = StrUtil.join(&quot;,&quot;, ids); List&lt;Shop&gt; shops = query().in(&quot;id&quot;, ids).last(&quot;ORDER BY FIELD(id,&quot; + idStr + &quot;)&quot;).list(); for (Shop shop : shops) &#123; shop.setDistance(distanceMap.get(shop.getId().toString()).getValue()); &#125; // 6.返回 return Result.ok(shops);&#125; 用户签到 BitMap BitMap功能演示我们针对签到功能完全可以通过mysql来完成，比如说以下这张表用户一次签到，就是一条记录，假如有1000万用户，平均每人每年签到次数为10次，则这张表一年的数据量为 1亿条每签到一次需要使用（8 + 8 + 1 + 1 + 3 + 1）共22 字节的内存，一个月则最多需要600多字节我们如何能够简化一点呢？其实可以考虑小时候一个挺常见的方案，就是小时候，咱们准备一张小小的卡片，你只要签到就打上一个勾，我最后判断你是否签到，其实只需要到小卡片上看一看就知道了我们可以采用类似这样的方案来实现我们的签到需求。我们按月来统计用户签到信息，签到记录为1，未签到则记录为0.把每一个bit位对应当月的每一天，形成了映射关系。用0和1标示业务状态，这种思路就称为位图（BitMap）。这样我们就用极小的空间，来实现了大量数据的表示Redis中是利用string类型数据结构实现BitMap，因此最大上限是512M，转换为bit则是 2^32个bit位。 BitMap的操作命令有： SETBIT：向指定位置（offset）存入一个0或1 GETBIT ：获取指定位置（offset）的bit值 BITCOUNT ：统计BitMap中值为1的bit位的数量 BITFIELD ：操作（查询、修改、自增）BitMap中bit数组中的指定位置（offset）的值 BITFIELD_RO ：获取BitMap中bit数组，并以十进制形式返回 BITOP ：将多个BitMap的结果做位运算（与 、或、异或） BITPOS ：查找bit数组中指定范围内第一个0或1出现的位置 实现签到功能需求：实现签到接口，将当前用户当天签到信息保存到Redis中（用户点击签到 修改redis中的bitmap 按照年月为key value是当前月份的天）思路：我们可以把年和月作为bitMap的key，然后保存到一个bitMap中，每次签到就到对应的位上把数字从0变成1，只要对应是1，就表明说明这一天已经签到了，反之则没有签到。我们通过接口文档发现，此接口并没有传递任何的参数，没有参数怎么确实是哪一天签到呢？这个很容易，可以通过后台代码直接获取即可，然后到对应的地址上去修改bitMap。代码UserController 1234@PostMapping(&quot;/sign&quot;)public Result sign()&#123; return userService.sign();&#125; UserServiceImpl 123456789101112131415@Overridepublic Result sign() &#123; // 1.获取当前登录用户 Long userId = UserHolder.getUser().getId(); // 2.获取日期 LocalDateTime now = LocalDateTime.now(); // 3.拼接key String keySuffix = now.format(DateTimeFormatter.ofPattern(&quot;:yyyyMM&quot;)); String key = USER_SIGN_KEY + userId + keySuffix; // 4.获取今天是本月的第几天 int dayOfMonth = now.getDayOfMonth(); // 5.写入Redis SETBIT key offset 1 stringRedisTemplate.opsForValue().setBit(key, dayOfMonth - 1, true); return Result.ok();&#125; 签到统计问题1：什么叫做连续签到天数？从最后一次签到开始向前统计，直到遇到第一次未签到为止，计算总的签到次数，就是连续签到天数。Java逻辑代码：获得当前这个月的最后一次签到数据，定义一个计数器，然后不停的向前统计，直到获得第一个非0的数字即可，每得到一个非0的数字计数器+1，直到遍历完所有的数据，就可以获得当前月的签到总天数了问题2：如何得到本月到今天为止的所有签到数据？BITFIELD key GET u[dayOfMonth] 0假设今天是10号，那么我们就可以从当前月的第一天开始，获得到当前这一天的位数，是10号，那么就是10位，去拿这段时间的数据，就能拿到所有的数据了，那么这10天里边签到了多少次呢？统计有多少个1即可。问题3：如何从后向前遍历每个bit位？注意：bitMap返回的数据是10进制，哪假如说返回一个数字8，那么我哪儿知道到底哪些是0，哪些是1呢？我们只需要让得到的10进制数字和1做与运算就可以了，因为1只有遇见1 才是1，其他数字都是0 ，我们把签到结果和1进行与操作，每与一次，就把签到结果向右移动一位，依次内推，我们就能完成逐个遍历的效果了。需求：实现下面接口，统计当前用户截止当前时间在本月的连续签到天数有用户有时间我们就可以组织出对应的key，此时就能找到这个用户截止这天的所有签到记录，再根据这套算法，就能统计出来他连续签到的次数了 代码UserController 1234@GetMapping(&quot;/sign/count&quot;)public Result signCount()&#123; return userService.signCount();&#125; UserServiceImpl 1234567891011121314151617181920212223242526272829303132333435363738394041@Overridepublic Result signCount() &#123; // 1.获取当前登录用户 Long userId = UserHolder.getUser().getId(); // 2.获取日期 LocalDateTime now = LocalDateTime.now(); // 3.拼接key String keySuffix = now.format(DateTimeFormatter.ofPattern(&quot;:yyyyMM&quot;)); String key = USER_SIGN_KEY + userId + keySuffix; // 4.获取今天是本月的第几天 int dayOfMonth = now.getDayOfMonth(); // 5.获取本月截止今天为止的所有的签到记录，返回的是一个十进制的数字 BITFIELD sign:5:202203 GET u14 0 List&lt;Long&gt; result = stringRedisTemplate.opsForValue().bitField( key, BitFieldSubCommands.create() .get(BitFieldSubCommands.BitFieldType.unsigned(dayOfMonth)).valueAt(0) ); if (result == null || result.isEmpty()) &#123; // 没有任何签到结果 return Result.ok(0); &#125; Long num = result.get(0); if (num == null || num == 0) &#123; return Result.ok(0); &#125; // 6.循环遍历 int count = 0; while (true) &#123; // 6.1.让这个数字与1做与运算，得到数字的最后一个bit位 // 判断这个bit位是否为0 if ((num &amp; 1) == 0) &#123; // 如果为0，说明未签到，结束 break; &#125;else &#123; // 如果不为0，说明已签到，计数器+1 count++; &#125; // 把数字右移一位，抛弃最后一个bit位，继续下一个bit位 num &gt;&gt;&gt;= 1; &#125; return Result.ok(count);&#125; UV统计HyperLogLog首先我们搞懂两个概念： UV：全称Unique Visitor，也叫独立访客量，是指通过互联网访问、浏览这个网页的自然人。1天内同一个用户多次访问该网站，只记录1次。 PV：全称Page View，也叫页面访问量或点击量，用户每访问网站的一个页面，记录1次PV，用户多次打开页面，则记录多次PV。往往用来衡量网站的流量。 通常来说UV会比PV大很多，所以衡量同一个网站的访问量，我们需要综合考虑很多因素，所以我们只是单纯的把这两个值作为一个参考值UV统计在服务端做会比较麻烦，因为要判断该用户是否已经统计过了，需要将统计过的用户信息保存。但是如果每个访问的用户都保存到Redis中，数据量会非常恐怖，那怎么处理呢？Hyperloglog(HLL)是从Loglog算法派生的概率算法，用于确定非常大的集合的基数，而不需要存储其所有值。相关算法原理大家可以参考：https://juejin.cn/post/6844903785744056333#heading-0Redis中的HLL是基于string结构实现的，单个HLL的内存永远小于16kb，内存占用低的令人发指！作为代价，其测量结果是概率性的，有小于0.81％的误差。不过对于UV统计来说，这完全可以忽略。 测试百万数据的统计 redis最佳实践键值设计keykey： 遵循格式： 【业务名称】：【数据名】：【id】 长度不超过44字节 不包含特殊字符 优点： 可读性强 避免key冲突 方便管理 更节省空间：key是string类型，底层编码包含int、embstr、raw三种。embstr在小于44字节的时候使用，采用连续内存空间，内存占用更小 bigkeybigkey通常以key的大小和key中成员的数量来综合判定，例如： key的本身数据量过大：一个String类型的key，它的值为5MB key的成员数过多：一个zset类型的key，他的成员数量为10000个 key的成员数据量过大：一个hash类型的key，他的成员数量虽然只有1000但是他们的value总和是100MB 推荐值： 单个key的value小于10KB 对于集合类型的key，建议元素数量小于1000 BigKey的危害网络阻塞对BigKey执行读请求时，少量的QPS就可能导致带宽使用率被占满，导致Redis实例，乃至所在物理机变慢数据倾斜BigKey所在的Redis实例内存使用率远超其他实例，无法使数据分片的内存资源达到均衡Redis阻塞对元素较多的hash、list、zset等做运算会耗时较久，使主线程被阻塞CPU压力对BigKey的数据序列化和反序列化会导致CPU的使用率飙升，影响Redis实例和本机其它应用 如何发现BigKeyredis-cli –bigkeys利用redis-cli提供的–bigkeys参数，可以遍历分析所有key，并返回Key的整体统计信息与每个数据的Top1的big keyscan扫描自己编程，利用scan扫描Redis中的所有key，利用strlen、hlen等命令判断key的长度（此处不建议使用MEMORY USAGE）第三方工具利用第三方工具，如 Redis-Rdb-Tools 分析RDB快照文件，全面分析内存使用情况网络监控自定义工具，监控进出Redis的网络数据，超出预警值时主动警告 如何删除BigKeyBigKey内存占用较多，即便时删除这样的key也需要耗费很长时间，导致Redis主线程阻塞，引发一系列问题。 redis 3.0 及以下版本如果是集合类型，则遍历BigKey的元素，先逐个删除子元素，最后删除BigKeyRedis 4.0以后Redis在4.0后提供了异步删除的命令：unlink 恰当的数据类型例1：比如存储一个User对象，我们有三种存储方式： json字符串 字段打散 hash 例2：假如有hash类型的key，其中有100万对field和value，field是自增id，这个key存在什么问题？如何优化？存在的问题：（1）hash的entry数量超过500时，会使用哈希表而不是ziplist，内存占用较多（2）可以通过hash-max-ziplist-entries配置entry上限。但是如果entry过多会导致BigKey问题方案二：拆分为string类型存在的问题：（1）string结构底层没有太多内存优化，内存占用较多（2）想要批量获取这些数据比较麻烦方案三：拆分为小的hash，将 id&#x2F;100 作为key，将 id %100 作为field，这样每100个元素为一个Hash 总结Key的最佳实践： 固定格式：[业务名]:[数据名]:[id] 足够简短：不超过44字节 不包含特殊字符 Value的最佳实践： 合理的拆分数据，拒绝BigKey 选择合适数据结构 Hash结构的entry数量不要超过1000 设置合理的超时时间 批处理集群下的批处理单机mset和pipeline在集群中出现的问题如MSET或Pipeline这样的批处理需要在一次请求中携带多条命令，而此时如果Redis是一个集群，那批处理命令的多个key必须落在一个插槽中，否则就会导致执行失败。原因是,redis集群定义了16384个插槽,不同key的slot不同,如果使用传统批处理,批处理中的命令会落在不同的节点,导致批处理失去意义。解决方法故解决问题,需要避免服务端计算slot,由客户端计算slot,实现批处理而springRedisTemplate实现的集群下的批处理操作正是采用并行slot,推荐使用 1stringRedisTemplate.opsForValue( ).multiSet() ; 总结 批处理应注意一次不要携带过多命令,以免网络阻塞 Pipeline多个命令之间不具备原子性 单机批处理与集群批处理不同,需采取对应的方案 服务器端优化持久化配置Redis的持久化虽然可以保证数据安全，但也会带来很多额外的开销，因此持久化请遵循下列建议: 用来做缓存的Redis实例尽量不要开启持久化功能 建议关闭RDB持久化功能，使用AOF持久化 利用脚本定期在slave节点做RDB，实现数据备份 设置合理的rewrite阈值，避免频繁的bgrewrite 当RDB与AOF都在进行io操作时,或者redis进行了AOF的rewrite,可能引起AOF的io阻塞,导致主线程阻塞,因此应配置no-appendfsync-on-rewrite &#x3D; yes，禁止在rewrite期间做aof，避免因AOF引起的阻塞 慢查询在redis执行时耗时超过某个阈值的命令,成为慢查询慢查询的阈值可以通过配置指定: slowlog-log-slower-than:慢查询阈值，单位是微秒。默认是10000，建议1000慢查询会被放入慢查询日志中，日志的长度有上限，可以通过配置指定: slowlog-max-len:慢查询日志（本质是一个队列）的长度。默认是128，建议1000 命令及安全配置为了避免这样的漏洞，这里给出一些建议: Redis一定要设置密码 禁止线上使用下面命令: keys、flushall、flushdb、config set等命令。可以利用配置文件修改rename-command禁用 修改配置文件bind:限制网卡，禁止外网网卡访问 开启防火墙 不要使用Root账户启动Redis 尽量不使用默认的端口 使用info memory,memory xxx 命令进行redis内存信息的查看 内存缓冲区常见的有三种: 复制缓冲区:主从复制的repl_backlog_buf，如果太小可能导致频繁的全量复制，影响性能。通过repl-backlog-size来设置，默认1mb AOF缓冲区:AOF刷盘之前的缓存区域，AOF执行rewrite的缓冲区。无法设置容量上限 客户端缓冲区:分为输入缓冲区和输出缓冲区，输入缓冲区最大1G且不能设置。输出缓冲区可以设置 集群优化集群完整性问题为了保证高可用特性，这里建议将cluster-require-full-coverage配置为false 12#当此配置开启时,当集群中有一个插槽不可用(如集群某个节点宕机),那整个集群将不可用cluster-require-full-coverage yes 集群带宽问题集群节点之间会不断的互相Ping来确定集群中其它节点的状态。每次Ping携带的信息至少包括: 插槽信息 集群状态信息 集群中节点越多，集群状态信息数据量也越大，10个节点的相关信息可能达到1kb，此时每次集群互通需要的带宽会非常高。 解决途径 避免大集群，集群节点数不要太多，最好少于1000，如果业务庞大，则建立多个集群 避免在单个物理机中运行太多Redis实例 配置合适的cluster-node-timeout值(定期进行节点间ping的时间) 集群还是主从集群虽然具备高可用特性，能实现自动故障恢复，但是如果使用不当，也会存在一些问题: 集群完整性问题 集群带宽问题 数据倾斜问题 客户端性能问题 命令的集群兼容性问题 lua和事务问题 单体Redis(主从Redis)已经能达到万级别的QPS，并且也具备很强的高可用特性。如果主从能满足业务需求的情况下，尽量不搭建Redis集群 集群模式lua问题补充在集群下,它会将数据自动分布到不同的节点(虚拟的16384个slot)它数据的路由分发,是通过计算key,所以只要key一样,则一定会被分到同一个slot上面的参数keys的个数为1,肯定存储在一个slot里面，但是如果是多个key且不存在同一个slot里面则会报错， 1redis.clients.jedis.exceptions.JedisDataException: ERR &#x27;EVAL&#x27; command keys must in same slot** 解决方案:设置hashtag,在设置key的时候使用{xxxxx}的方式可以将该数据存在用一个slot里面，但是，注意如果同一个slot里面的数据过多且同时操作查询频繁的数据就会影响到效率问题(数据倾斜)","categories":[{"name":"中间件","slug":"中间件","permalink":"http://example.com/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[]},{"title":"分布式","slug":"分布式","date":"2023-07-31T09:29:24.176Z","updated":"2023-07-31T09:29:49.961Z","comments":true,"path":"2023/07/31/分布式/","link":"","permalink":"http://example.com/2023/07/31/%E5%88%86%E5%B8%83%E5%BC%8F/","excerpt":"","text":"springcloud认识微服务几种架构模式单体架构：将业务的所有功能集中在一个项目中开发，打成一个包部署。单体架构的优缺点如下：优点： 架构简单 部署成本低 缺点： 耦合度高（维护困难、升级困难） 分布式架构：根据业务功能对系统做拆分，每个业务功能模块作为独立项目开发，称为一个服务。每个功能变为独立项目开发分布式架构的优缺点：优点： 降低服务耦合 有利于服务升级和拓展 缺点： 服务调用关系错综复杂 分布式架构虽然降低了服务耦合，但是服务拆分时也有很多问题需要思考： 服务拆分的粒度如何界定？ 服务之间如何调用？ 服务的调用关系如何管理？ 人们需要制定一套行之有效的标准来约束分布式架构。 另外并不是所有的项目都上分布式就是好 要有选择性 小型的完全单机部署就可以 比如说个人博客，ｘｘ管理系统等等这些完全不需要分布式 分布式例如说 淘宝 京东 这种人流量大 访问请求大的 同时** springcloud只是分布式的其中一部分 它解决了拆分以后的质量问题 而一个分布式架构 还存在很多的问题 所以 springcloud ！&#x3D; 分布式** 微服务微服务的架构特征： 单一职责：微服务拆分粒度更小，每一个服务都对应唯一的业务能力，做到单一职责 自治：团队独立、技术独立、数据独立，独立部署和交付 面向服务：服务提供统一标准的接口，与语言和技术无关 隔离性强：服务调用做好隔离、容错、降级，避免出现级联问题 通过网关将请求路由到不同的服务 原本一个项目中多个功能 现在把功能拆分 变为多个服务微服务的上述特性其实是在给分布式架构制定一个标准，进一步降低服务之间的耦合度，提供服务的独立性和灵活性。做到高内聚，低耦合。因此，可以认为微服务是一种经过良好架构设计的分布式架构方案 。但方案该怎么落地？选用什么样的技术栈？全球的互联网公司都在积极尝试自己的微服务落地方案。其中在Java领域最引人注目的就是SpringCloud提供的方案了。 SpringCloudSpringCloud是目前国内使用最广泛的微服务框架。官网地址：https://spring.io/projects/spring-cloud。SpringCloud集成了各种微服务功能组件，并基于SpringBoot实现了这些组件的自动装配，从而提供了良好的开箱即用体验。其中常见的组件包括：另外，SpringCloud底层是依赖于SpringBoot的，并且有版本的兼容关系，如下：我们课堂学习的版本是 Hoxton.SR10，因此对应的SpringBoot版本是2.3.x版本。 总结单体架构：简单方便，高度耦合，扩展性差，适合小型项目。例如：学生管理系统分布式架构：松耦合，扩展性好，但架构复杂，难度大。适合大型互联网项目，例如：京东、淘宝微服务：一种良好的分布式架构方案 ①优点：拆分粒度更小、服务更独立、耦合度更低 ②缺点：架构非常复杂，运维、监控、部署难度提高SpringCloud是微服务架构的一站式解决方案，集成了各种优秀微服务功能组件 服务拆分和远程调用这里我总结了微服务拆分时的几个原则： 不同微服务，不要重复开发相同业务 微服务数据独立，不要访问其它微服务的数据库 (每个服务只对应他服务的数据库) 微服务可以将自己的业务暴露为接口，供其它微服务调用 导入项目 分层 按照微服务的单一职责 一个功能分为一个项目 每个功能有一个数据库cloud-demo：父工程，管理依赖 order-service：订单微服务，负责订单相关业务 user-service：用户微服务，负责用户相关业务 要求： 订单微服务和用户微服务都必须有各自的数据库，相互独立 订单服务和用户服务都对外暴露Restful的接口 订单服务如果需要查询用户信息，只能调用用户服务的Restful接口，不能查询用户数据库 代码实现导入数据库 两个数据库对应两个项目 导入项目 运行项目 这个初始项目是基于springboot 的然后发起请求得到数据为 12345678&#123; &quot;id&quot;: 101, &quot;price&quot;: 699900, &quot;name&quot;: &quot;Apple 苹果 iPhone 12 &quot;, &quot;num&quot;: 1, &quot;userId&quot;: 1, &quot;user&quot;: null&#125; user为null 因为后端没有传递userid的参数 因此无法获取 如何解决？分别发起请求 可以获得 但是如果有两个功能之间有依赖的话如何处理例如 这里根据用户id查询订单信息 首先需要获取到用户id 如何获取？解决： 我们可以在订单服务中 发起请求获取用户信息，然后再根据这个信息去查询如何在客户端内发起请求？其实我们用过的例如huTool等等 这里使用RestTemplate大概的步骤是这样的： 注册一个RestTemplate的实例到Spring容器 修改order-service服务中的OrderService类中的queryOrderById方法，根据Order对象中的userId查询User 将查询的User填充到Order对象，一起返回 首先创建RestTemplate对象 123456789101112@MapperScan(&quot;cn.itcast.order.mapper&quot;)@SpringBootApplicationpublic class OrderApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(OrderApplication.class, args); &#125; @Bean public RestTemplate restTemplate() &#123; return new RestTemplate(); &#125;&#125; 123456789101112131415161718192021@Servicepublic class OrderService &#123; @Resource private OrderMapper orderMapper; @Resource private RestTemplate restTemplate; public Order queryOrderById(Long orderId) &#123; // 1.查询订单 Order order = orderMapper.findById(orderId); // 发起请求 获取用户信息 // 请求地址 String url = &quot;http://localhost:8081/user/&quot; + order.getUserId(); // 调用 User user = restTemplate.getForObject(url, User.class); // 包装返回 order.setUser(user); // 4.返回 return order; &#125;&#125; 此刻调用成功 我们可以看到 以上的运行就是将原本的一个项目中的功能 将他们拆分分为多个单一服务 每一个对应一个数据库 只专注于他们自己 而不涉及其他别人的业务逻辑，另外需要将自己的接口暴露给外部调用才行 提供者与消费者在服务调用关系中，会有两个不同的角色：服务提供者：一次业务中，被其它微服务调用的服务。（提供接口给其它微服务）服务消费者：一次业务中，调用其它微服务的服务。（调用其它微服务提供的接口）但是，服务提供者与服务消费者的角色并不是绝对的，而是相对于业务而言。如果服务A调用了服务B，而服务B又调用了服务C，服务B的角色是什么？ 对于A调用B的业务而言：A是服务消费者，B是服务提供者 对于B调用C的业务而言：B是服务消费者，C是服务提供者 因此，服务B既可以是服务提供者，也可以是服务消费者。 根据身份的不同因此不同情况下角色不同 Eureka注册中心关于我们以上代码的问题： 我们将请求写死在了项目中 如果这个请求是使用集群的方式 部署在了多个服务器上 我们如何选择？ order-service在发起远程调用的时候，该如何得知user-service实例的ip地址和端口？ order-service如何得知某个user-service实例是否依然健康，是不是已经宕机？ 以上就需要引入注册中心的概念： 服务的提供者将自身信息告诉注册中心 消费者可以从注册中心获取信息服务提供者在启动时就会将信息告知注册中心 注意：消费者同时也可能是提供者 因此也会告知注册中心他的信息 问题1：order-service如何得知user-service实例地址？获取地址信息的流程如下： user-service服务实例启动后，将自己的信息注册到eureka-server（Eureka服务端）。这个叫服务注册 eureka-server保存服务名称到服务实例地址列表的映射关系 order-service根据服务名称，拉取实例地址列表。这个叫服务发现或服务拉取 问题2：order-service如何从多个user-service实例中选择具体的实例？ order-service从实例列表中利用负载均衡算法选中一个实例地址 向该实例地址发起远程调用 问题3：order-service如何得知某个user-service实例是否依然健康，是不是已经宕机？ user-service会每隔一段时间（默认30秒）向eureka-server发起请求，报告自己状态，称为心跳 当超过一定时间没有发送心跳时，eureka-server会认为微服务实例故障，将该实例从服务列表中剔除 order-service拉取服务时，就能将故障实例排除了 注意：一个微服务，既可以是服务提供者，又可以是服务消费者，因此eureka将服务注册、服务发现等功能统一封装到了eureka-client端 实现： 搭建eureka-server注册中心服务端：eureka-server，这必须是一个独立的微服务 因此需要单独开启一个项目 创建一个maven项目 引入依赖 （这里我们在父工程下 因此不需要导入版本） 另外这里导入的是server服务器端 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt;&lt;/dependency&gt; 编写启动类 12345678910111213package cn.itcast.eureka;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;@SpringBootApplication@EnableEurekaServerpublic class EurekaApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaApplication.class, args); &#125;&#125; 编写配置类（eureka自己也是微服务 因此也会把自己注册到注册中心） 12345678910server: port: 10086 # tomcat的端口# 这里的名称 和以下的配置 都是为了做服务的注册spring: application: name: eureka-server # 微服务名称 每一个微服务的项目都有自己的名称eureka: client: service-url: # eureka的地址信息 defaultZone: http://127.0.0.1:10086/eureka # 如果这里是集群那就写集群的地址 我们这里写本机 打开地址http://localhost:10086/application就是服务名称 status就是服务地址我们将eureka自己注册到了注册中心 因为他也是一个微服务 接着需要将其他的微服务注册到注册中心 服务注册将原本的两个微服务项目注册到eureka注册中心 引入依赖 （这里是客户端） 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt; 配置文件 1234567spring: application: name: orderservice # 指定名称eureka: client: service-url: # 配置注册中心的地址 项目启动 就会将项目的信息放入注册中心中 defaultZone: http://127.0.0.1:10086/eureka 另一个项目和以上一样 注意的是服务名称要改变启动项目 根据配置自动注册信息 注册中心中就有了我们的设置 服务发现现在我们需要改造项目 原本写死的url 转为从注册中心拉取 将原本的url具体地址 改为 设置的微服务名 原本的 12// 请求地址 String url = &quot;http://localhost:8081/user/&quot; + order.getUserId(); 现在的 12// 请求地址 String url = &quot;http://userservice/user/&quot; + order.getUserId(); 为了实现负载均衡 要在RestTemplate属性上加入@LoadBalanced注解12345@Bean @LoadBalanced public RestTemplate restTemplate() &#123; return new RestTemplate(); &#125; 重新发起调用 请求成功 这里就存在一个问题 我们什么都没写 为什么它能够根据userservice 就能转换为地址？另外我们添加了@LoadBalanced注解，即可实现负载均衡功能，这是什么原理呢？ 负载均衡 Ribbon原理SpringCloud底层其实是利用了一个名为Ribbon的组件，来实现负载均衡功能的。order-service通过RestTemplate发起请求以后 Ribbon就会根据其中的微服务名称信息 去找eureka注册中心获取具体地址信息 再去做负载均衡处理加了@LoadBalanced注解 就意味着这个RestTemplate对象发起的请求要被Ribbon处理 SpringCloudRibbon的底层采用了一个拦截器，拦截了RestTemplate发出的请求，对地址做了修改。请求进来被拦截（要加**@LoadBalanced注解 **注解） 拦截器去注册中心拉去所有的信息 并且根据IRule去进行负载均衡处理 例如轮询 随机 基本流程如下： 拦截我们的RestTemplate请求http://userservice/user/1 RibbonLoadBalancerClient会从请求url中获取服务名称，也就是user-service DynamicServerListLoadBalancer根据user-service到eureka拉取服务列表 eureka返回列表，localhost:8081、localhost:8082 IRule利用内置负载均衡规则，从列表中选择一个，例如localhost:8081 RibbonLoadBalancerClient修改请求地址，用localhost:8081替代userservice，得到http://localhost:8081/user/1，发起真实请求 负载均衡策略负载均衡的规则都定义在IRule接口中，而IRule有很多不同的实现类不同规则的含义如下： 内置负载均衡规则类 规则描述 RoundRobinRule 简单轮询服务列表来选择服务器。它是Ribbon默认的负载均衡规则。 AvailabilityFilteringRule 对以下两种服务器进行忽略： （1）在默认情况下，这台服务器如果3次连接失败，这台服务器就会被设置为“短路”状态。短路状态将持续30秒，如果再次连接失败，短路的持续时间就会几何级地增加。 （2）并发数过高的服务器。如果一个服务器的并发连接数过高，配置了AvailabilityFilteringRule规则的客户端也会将其忽略。并发连接数的上限，可以由客户端的..ActiveConnectionsLimit属性进行配置。 WeightedResponseTimeRule 为每一个服务器赋予一个权重值。服务器响应时间越长，这个服务器的权重就越小。这个规则会随机选择服务器，这个权重值会影响服务器的选择。 ZoneAvoidanceRule 以区域可用的服务器为基础进行服务器的选择。使用Zone对服务器进行分类，这个Zone可以理解为一个机房、一个机架等。而后再对Zone内的多个服务做轮询。 BestAvailableRule 忽略那些短路的服务器，并选择并发数较低的服务器。 RandomRule 随机选择一个可用的服务器。 RetryRule 重试机制的选择逻辑 默认的实现就是ZoneAvoidanceRule，是一种轮询方案一般来说默认就可以了 自定义负载均衡策略通过定义IRule实现可以修改负载均衡规则，有两种方式： 代码方式：在order-service中的OrderApplication类中，定义一个新的IRule： 1234@Beanpublic IRule randomRule()&#123; return new RandomRule();&#125; 配置文件方式：在order-service的application.yml文件中，添加新的配置也可以修改规则： 123userservice: # 给某个微服务配置负载均衡规则，这里是userservice服务 ribbon: NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RandomRule # 负载均衡规则 注意，一般用默认的负载均衡规则，不做修改。 饥饿加载Ribbon默认是采用懒加载，即第一次访问时才会去创建LoadBalanceClient，请求时间会很长。而饥饿加载则会在项目启动时创建，降低第一次访问的耗时，通过下面配置开启饥饿加载： 1234ribbon: eager-load: enabled: true clients: userservice 如果是多个那就是 123456ribbon: eager-load: enabled: true clients: - userservice - xxx Nacos 注册中心 ***比起Eureka更顶安装 win 下载安装包https://github.com/alibaba/nacos/releases我们使用的是2.2.2版本 （版本和依赖之间一定要选择对不然可能会不兼容）Nacos的默认端口是8848bin目录下可直接启动startup.cmd -m standalone standalone是单机启动的意思 还有集群的方式 启动以后，在浏览器输入地址：http://127.0.0.1:8848/nacos即可成功访问 使用Nacos修改原本项目原来我们用的是Eureka作为注册中心 两步走： 导入依赖 修改配置 springcloud下的都是有一套注册中心接口的规范 因此是一样的所以Nacos执行也是一样 导入依赖 修改配置 注意：不要忘了注释掉eureka的依赖。 导入依赖父工程使用在cloud-demo父工程的pom文件中的中引入SpringCloudAlibaba的依赖： 1234567&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt; &lt;version&gt;2.2.6.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt;&lt;/dependency&gt; 然后在user-service和order-service中的pom文件中引入nacos-discovery依赖： 1234&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;&lt;/dependency&gt; 修改配置在user-service和order-service的application.yml中添加nacos地址： 1234spring: cloud: nacos: server-addr: localhost:8848 注意：不要忘了注释掉eureka的地址 重启微服务后，登录nacos管理页面，可以看到微服务信息 服务分级存储模型一个服务可以有多个实例，例如我们的user-service，可以有: 127.0.0.1:8081 127.0.0.1:8082 127.0.0.1:8083 假如这些实例分布于全国各地的不同机房，例如： 127.0.0.1:8081，在上海机房 127.0.0.1:8082，在上海机房 127.0.0.1:8083，在杭州机房 Nacos就将同一机房内的实例 划分为一个集群。也就是说，user-service是服务，一个服务可以包含多个集群，如杭州、上海，每个集群下可以有多个实例，形成分级模型，如图：微服务互相访问时，应该尽可能访问同集群实例，因为本地访问速度更快。当本集群内不可用时，才访问其它集群。例如：杭州机房内的order-service应该优先访问同机房的user-service。 给user-service配置集群我们创建两个user-service的实例 并且使用为他们配置集群 使得这些实例在同一集群下修改user-service的application.yml文件，添加集群配置： 123456spring: cloud: nacos: server-addr: localhost:8848 discovery: cluster-name: HZ # 集群名称 重启两个user-service实例后，我们可以在nacos控制台看到下面结果：我们再次复制一个user-service启动配置，添加属性：-Dserver.port&#x3D;8083 -Dspring.cloud.nacos.discovery.cluster-name&#x3D;SH再次启动： 同集群优先的负载均衡默认的ZoneAvoidanceRule并不能实现根据同集群优先来实现负载均衡。 依旧使用的轮询方式 这样使得我们为服务分级失去了意义因此Nacos中提供了一个**NacosRule**的实现，可以优先从同集群中挑选实例。(他只是多了一层集群内访问 而多个实例还是用的是随机的负载均衡) 如果集群中没有 才会去跨集群访问1）给order-service配置集群信息修改order-service的application.yml文件，添加集群配置： 123456spring: cloud: nacos: server-addr: localhost:8848 discovery: cluster-name: HZ # 集群名称 2）修改负载均衡规则修改order-service的application.yml文件，修改负载均衡规则： 123userservice: ribbon: NFLoadBalancerRuleClassName: com.alibaba.cloud.nacos.ribbon.NacosRule # 负载均衡规则 权重配置实际部署中会出现这样的场景：服务器设备性能有差异，部分实例所在机器性能较好，另一些较差，我们希望性能好的机器承担更多的用户请求。但默认情况下NacosRule是同集群内随机挑选，不会考虑机器的性能问题。因此，Nacos提供了权重配置来控制访问频率，权重越大则访问频率越高。在nacos控制台，找到user-service的实例列表，点击编辑，即可修改权重： 注意：如果权重修改为0，则该实例永远不会被访问 想想如果有一天项目需要升级 我们可以将其中一个实例的权重改为0 让他偷偷修改 最后在开放一点点用户进入测试 就不用加班更改了 环境隔离Nacos提供了namespace来实现环境隔离功能。 nacos中可以有多个namespace namespace下可以有group、service等 不同namespace之间相互隔离，例如不同namespace的服务互相不可见 多个namespace是相互不可见的 service就是服务他下面就是集群 这种隔离是用来做区分环境（生产，开发环境，线上等等）的，而服务划分 实例划分是根据业务去划分 创建命名空间默认有一份public 而我们不设置所有的实例 都在命名空间里 默认情况下，所有service、data、group都在同一个namespace，名为public： 我们可以点击页面新增按钮，添加一个namespace：这个空间id默认使用的UUID生成新建一个namespace以后就需要去将我们的实例配置到命名空间中 这个命名空间id很重要这就需要去yml文件中配置 给微服务配置namespace给微服务配置namespace只能通过修改配置来实现。例如，修改order-service的application.yml文件： 1234567spring: cloud: nacos: server-addr: localhost:8848 discovery: cluster-name: HZ namespace: 492a7d5d-237b-46a1-a99a-fa8e98e4b0f9 # 命名空间，填ID 重启order-service后，访问控制台，可以看到而再次访问网站的时候 已经走不通了java.lang.IllegalStateException: No instances available for userservice因为userservice在另一个namespace上 两个namespace相互不交互（天人永隔） 因此无法访问 Nacos与Eureka的区别Nacos的服务实例分为两种l类型： 临时实例：如果实例宕机超过一定时间，会从服务列表剔除，默认的类型。 一般默认即可 非临时实例：如果实例宕机，不会从服务列表剔除，也可以叫永久实例。不直接剔除 而是等他好 配置一个服务实例为永久实例： 12345spring: cloud: nacos: discovery: ephemeral: false # 设置为非临时实例 Nacos和Eureka整体结构类似，服务注册、服务拉取、心跳等待，但是也存在一些差异：这里nacos是定时拉取到缓存 而不是发起请求时去拉取 这样注册中心的压力太大，每个30s拉取一次， Nacos与eureka的共同点 都支持服务注册和服务拉取 都支持服务提供者心跳方式做健康检测 Nacos与Eureka的区别 Nacos支持服务端主动检测提供者状态：临时实例采用心跳模式，非临时实例采用主动检测模式 临时实例心跳不正常会被剔除，非临时实例则不会被剔除 Nacos支持服务列表变更的消息推送模式，服务列表更新更及时 Nacos集群默认采用AP方式，当集群中存在非临时实例时，采用CP模式；Eureka采用AP方式 A：高可用 C：一致性 P：分区容错性 Nacos配置管理Nacos除了可以做注册中心，同样可以做配置管理来使用。 统一配置管理当微服务部署的实例越来越多，达到数十、数百时，逐个修改微服务配置就会让人抓狂，而且很容易出错。我们需要一种统一配置管理方案，可以集中管理所有实例的配置。Nacos一方面可以将配置集中管理，另一方可以在配置变更时，及时通知微服务，实现配置的热更新。改一份配置 修改全部 实现热更新 配置 注意：项目的核心配置，需要热更新的配置才有放到nacos管理的必要。基本不会变更的一些配置还是保存在微服务本地比较好。 从微服务拉取配置微服务要拉取nacos中管理的配置，并且与本地的application.yml配置合并，才能完成项目启动。但如果尚未读取application.yml，又如何得知nacos地址呢？因此spring引入了一种新的配置文件：bootstrap.yaml文件，会在application.yml之前被读取，流程如下： 1）引入nacos-config依赖首先，在user-service服务中，引入nacos-config的客户端依赖： 12345&lt;!--nacos配置管理依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt;&lt;/dependency&gt; 2）添加bootstrap.yaml 优先级高于application文件然后，在user-service中添加一个bootstrap.yaml文件，内容如下： 12345678910spring: application: name: userservice # 服务名称 profiles: active: dev #开发环境，这里是dev cloud: nacos: server-addr: localhost:8848 # Nacos地址 config: file-extension: yaml # 文件后缀名 这里会根据spring.cloud.nacos.server-addr获取nacos地址，再根据${spring.application.name}-${spring.profiles.active}.${spring.cloud.nacos.config.file-extension}作为文件id，来读取配置。也就是我们指定的配置名称本例中，就是去读取userservice-dev.yaml： 1234567@Value(&quot;$&#123;pattern.dateformat&#125;&quot;) // 这个是获取线上的配置信息private String dateformat;@GetMapping(&quot;/now&quot;)public String now() &#123; return LocalDateTime.now().format(DateTimeFormatter.ofPattern(dateformat));&#125; 通过测试我们获取到了 具体日期 说明userservice从这里获取到了配置信息 将配置交给Nacos管理的步骤①在Nacos中添加配置文件②在微服务中引入nacos的config依赖③在微服务中添加bootstrap.yml，配置nacos地址、当前环境、服务名称、文件后缀名。这些决定了程序启动时去nacos读取哪个文件 接下来我们需要做热部署 当这个配置文件改动 所有项目的部署跟着改动 配置热更新我们最终的目的，是修改nacos中的配置后，微服务中无需重启即可让配置生效，也就是配置热更新。要实现配置热更新，可以使用两种方式： 方式一在@Value注入的变量所在类上添加注解@RefreshScope： 方式二使用一个配置类 在使用的地方注入这个配置类 123456789101112package cn.itcast.user.config;import lombok.Data;import org.springframework.boot.context.properties.ConfigurationProperties;import org.springframework.stereotype.Component;@Component@Data@ConfigurationProperties(prefix = &quot;pattern&quot;)public class PatternProperties &#123; private String dateformat;&#125; 在UserController中使用这个类代替@Value： 123456789101112131415161718@Slf4j@RestController@RequestMapping(&quot;/user&quot;)public class UserController &#123; @Autowired private UserService userService; @Resource private PatternProperties patternProperties; @GetMapping(&quot;now&quot;) public String now()&#123; return LocalDateTime.now().format(DateTimeFormatter.ofPattern(patternProperties.getDateformat())); &#125; // 略&#125; 配置共享其实微服务启动时，会去nacos读取多个配置文件，例如： [spring.application.name]-[spring.profiles.active].yaml，例如：userservice-dev.yaml [spring.application.name].yaml，例如：userservice.yaml 而[spring.application.name].yaml不包含环境，因此可以被多个环境共享。以上的热更新是根据环境来区分并且是经常改动的 这里的配置共享是所有环境共有一份无论profile如何变化，[spring.application.name].yaml这个文件一定会加载，因此多环境共享配置可以写入这个文件下面我们通过案例来测试配置共享 添加一个环境共享配置我们在nacos中添加一个userservice.yaml文件：在user-service中读取共享配置我们启动两个环境测试 都获取到了这个文件的值总结：微服务会从nacos读取的配置文件：①[服务名]-[spring.profile.active].yaml，环境配置②[服务名].yaml，默认配置，多环境共享优先级：[服务名]-[环境].yaml&gt;[服务名].yaml &gt; 本地配置 搭建Nacos集群其中包含3个nacos节点，然后一个负载均衡器代理3个Nacos。这里负载均衡器可以使用nginx。 三个nacos节点的地址： 节点 ip port nacos1 192.168.150.1 8845 nacos2 192.168.150.1 8846 nacos3 192.168.150.1 8847 我们只是用本机模拟 搭建集群步骤搭建集群的基本步骤： 搭建数据库，初始化数据库表结构 下载nacos安装包 配置nacos 启动nacos集群 nginx反向代理 初始化数据库Nacos默认数据存储在内嵌数据库Derby中，不属于生产可用的数据库。官方推荐的最佳实践是使用带有主从的高可用数据库集群这里我们以单点的数据库为例来讲解。首先新建一个数据库，命名为nacos，而后导入nacos&#x2F;conf下的mysql-schema.sql文件到数据库， 下载nacos安装包我们使用的2.2.2版本的 配置Nacos首先先复制一份未更改的进入nacos的conf目录，修改配置文件cluster.conf.example，重命名为cluster.conf：然后添加内容：将节点替换为我们需要的 123127.0.0.1:8845127.0.0.1:8847127.0.0.1:8849 然后修改application.properties文件，添加数据库配置 1234567spring.datasource.platform=mysqldb.num=1db.url.0=jdbc:mysql://127.0.0.1:3306/nacos?characterEncoding=utf8&amp;connectTimeout=1000&amp;socketTimeout=3000&amp;autoReconnect=true&amp;useUnicode=true&amp;useSSL=false&amp;serverTimezone=UTCdb.user.0=rootdb.password.0=123 启动将nacos文件夹复制三份，分别命名为：nacos1、nacos2、nacos3然后分别修改三个文件夹中的application.properties中的server.port，分别改为 8845 8847 8849然后分别启动三个nacos节点：startup.cmd 使用nginx反向代理代理这三个nacos节点解压nginx修改conf&#x2F;nginx.conf文件，配置如下： 粘贴在http块的内部集合 1234567891011121314upstream nacos-cluster &#123; # 配置集群 nginx会对这三个地址做负载均衡 server 127.0.0.1:8845; server 127.0.0.1:8847; server 127.0.0.1:8849;&#125;server &#123; listen 80; # 监听80端口的localhost nginx默认就是80端口 所以直接访问localhost就可以被监听 server_name localhost; location /nacos &#123; # 只要访问这个路径给的就是上面的地址 proxy_pass http://nacos-cluster; &#125;&#125; 而后在浏览器访问：http://localhost/nacos即可。代码中application.yml文件配置如下： 1234spring: cloud: nacos: server-addr: localhost:80 # Nacos地址 写为反向代理的nginx地址 OpenFeign远程调用先来看我们以前利用RestTemplate发起远程调用的代码： 1234// 请求地址String url = &quot;http://userservice/user/&quot; + order.getUserId();// 调用User user = restTemplate.getForObject(url, User.class); 存在下面的问题：•代码可读性差，编程体验不统一•参数复杂URL难以维护 （例如说url传参 一次传递十几个参数 这个url就很长了）Feign是一个声明式的http客户端，官方地址：https://github.com/OpenFeign/feign其作用就是帮助我们优雅的实现http请求的发送，解决上面提到的问题。 Fegin的使用步骤1）引入依赖我们在order-service服务的pom文件中引入feign的依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;&lt;/dependency&gt; 2）添加注解在order-service的启动类添加注解开启Feign的功能： 3）编写Feign的客户端1234567891011@FeignClient(&quot;userservice&quot;) // 微服务名称 请求会根据这个命名去寻找具体ip发送请求public interface UserClient &#123; /** * 原本的请求url是 &quot;http://userservice/user/&quot; + order.getUserId(); * 就像是springmvc的请求 这样写以后 * @param id 我们作为请求 需要传递id这个参数 因此调用时候需要输入id值 */ @GetMapping(&quot;/user/&#123;id&#125;&quot;) // 发送get请求 指定路径 并且返回结果使用User对象包装 User findById(@PathVariable(&quot;id&quot;) Long id);&#125; 这个客户端主要是基于SpringMVC的注解来声明远程调用的信息，比如： 服务名称：userservice 请求方式：GET 请求路径：&#x2F;user&#x2F;{id} 请求参数：Long id 返回值类型：User 这样，Feign就可以帮助我们发送http请求，无需自己使用RestTemplate来发送了。 4）测试12345678910111213141516 @Resource private UserClient userClient; public Order queryOrderById(Long orderId) &#123; // 1.查询订单 Order order = orderMapper.findById(orderId); // 发起请求 获取用户信息 // 请求地址// String url = &quot;http://userservice/user/&quot; + order.getUserId();// // 调用// User user = restTemplate.getForObject(url, User.class); User user = userClient.findById(order.getUserId()); // 包装返回 order.setUser(user); // 4.返回 return order; &#125; 5）总结使用Feign的步骤：① 引入依赖② 添加@EnableFeignClients注解③ 编写FeignClient接口④ 使用FeignClient中定义的方法代替RestTemplate 自定义配置Feign可以支持很多的自定义配置，如下表所示： 类型 作用 说明 feign.Logger.Level 修改日志级别 包含四种不同的级别：NONE、BASIC、HEADERS、FULL feign.codec.Decoder 响应结果的解析器 http远程调用的结果做解析，例如解析json字符串为java对象 feign.codec.Encoder 请求参数编码 将请求参数编码，便于通过http请求发送 feign. Contract 支持的注解格式 默认是SpringMVC的注解 feign. Retryer 失败重试机制 请求失败的重试机制，默认是没有，不过会使用Ribbon的重试 一般来说 需要配置的就是日志一般情况下，默认值就能满足我们使用，如果要自定义时，只需要创建自定义的@Bean覆盖默认Bean即可。下面以日志为例来演示如何自定义配置。 配置文件方式基于配置文件修改feign的日志级别可以针对单个服务： 12345feign: client: config: userservice: # 针对某个微服务的配置 loggerLevel: FULL # 日志级别 也可以针对所有服务： 12345feign: client: config: default: # 这里用default就是全局配置，如果是写服务名称，则是针对某个微服务的配置 loggerLevel: FULL # 日志级别 而日志的级别分为四种： NONE：不记录任何日志信息，这是默认值。 BASIC：仅记录请求的方法，URL以及响应状态码和执行时间 HEADERS：在BASIC的基础上，额外记录了请求和响应的头信息 FULL：记录所有请求和响应的明细，包括头信息、请求体、元数据。 Java代码方式也可以基于Java代码来修改日志级别，先声明一个类，然后声明一个Logger.Level的对象： 123456public class DefaultFeignConfiguration &#123; @Bean public Logger.Level feignLogLevel()&#123; return Logger.Level.BASIC; // 日志级别为BASIC &#125;&#125; 如果要全局生效，将其放到启动类的@EnableFeignClients这个注解中：@EnableFeignClients(defaultConfiguration &#x3D; DefaultFeignConfiguration .class)如果是局部生效，则把它放到对应的@FeignClient这个注解中：@FeignClient(value &#x3D; “userservice”, configuration &#x3D; DefaultFeignConfiguration .class) Feign使用优化Feign底层发起http请求，依赖于其它的框架。其底层客户端实现包括：•URLConnection：默认实现，不支持连接池•Apache HttpClient ：支持连接池•OKHttp：支持连接池因此提高Feign的性能主要手段就是使用连接池代替默认的URLConnection。这里我们用Apache的HttpClient来演示。1）引入依赖在order-service的pom文件中引入Apache的HttpClient依赖： 12345&lt;!--httpClient的依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;io.github.openfeign&lt;/groupId&gt; &lt;artifactId&gt;feign-httpclient&lt;/artifactId&gt;&lt;/dependency&gt; 2）配置连接池在order-service的application.yml中添加配置： 123456789feign: client: config: default: # default全局的配置 loggerLevel: BASIC # 日志级别，BASIC就是基本的请求和响应信息 httpclient: enabled: true # 开启feign对HttpClient的支持 max-connections: 200 # 最大的连接数 max-connections-per-route: 50 # 每个路径的最大连接数 接下来，在FeignClientFactoryBean中的loadBalance方法中打断点：Debug方式启动order-service服务，可以看到这里的client，底层就是Apache HttpClient： 总结，Feign的优化：1.日志级别尽量用basic2.使用HttpClient或OKHttp代替URLConnection① 引入feign-httpClient依赖② 配置文件开启httpClient功能，设置连接池参数 最佳实践所谓最近实践，就是使用过程中总结的经验，最好的一种使用方式。自习观察可以发现，Feign的客户端与服务提供者的controller代码非常相似：有没有一种办法简化这种重复的代码编写呢？ 继承方式一样的代码可以通过继承来共享：1）定义一个API接口，利用定义方法，并基于SpringMVC注解做声明。2）Feign客户端和Controller都集成改接口这种问题就是 紧耦合 参数列表是不会被继承的优点： 简单 实现了代码共享 缺点： 服务提供方、服务消费方紧耦合 参数列表中的注解映射并不会继承，因此Controller中必须再次声明方法、参数列表、注解 抽取方式将Feign的Client抽取为独立模块，并且把接口有关的POJO、默认的Feign配置都放到这个模块中，提供给所有消费者使用。例如，将UserClient、User、Feign的默认配置都抽取到一个feign-api包中，所有微服务引用该依赖包，即可直接使用。这种也有问题 可能我们项目只需要一个请求 却要引入很多包 这样就很浪费 实现基于抽取的最佳实践实现最佳实践方式二的步骤如下： 首先创建一个module，命名为feign-api，然后引入feign的starter依赖 将order-service中编写的UserClient、User、DefaultFeignConfiguration都复制到feign-api项目中 在order-service中引入feign-api的依赖 修改order-service中的所有与上述三个组件有关的import部分，改成导入feign-api中的包 重启测试 首先创建一个module，命名为feign-api：在feign-api中然后引入feign的starter依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;&lt;/dependency&gt; 然后，order-service中编写的UserClient、User、DefaultFeignConfiguration都复制到feign-api项目中打包成jar 在order-service中使用feign-api首先，删除order-service中的UserClient、User、DefaultFeignConfiguration等类或接口。在order-service的pom文件中中引入feign-api的依赖： 12345&lt;dependency&gt; &lt;groupId&gt;cn.itcast.demo&lt;/groupId&gt; &lt;artifactId&gt;feign-api&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt;&lt;/dependency&gt; 修改order-service中的所有与上述三个组件有关的导包部分，改成导入feign-api中的包重启后，发现服务报错了：这是spring经典错误 说明有这个类 但是没有加入spring容器中这是因为UserClient现在在cn.itcast.feign.clients包下，而启动类的注解扫描的是cn.itcast.order而order-service的@EnableFeignClients注解是在cn.itcast.order包下，不在同一个包，无法扫描到UserClient。 解决扫描包问题方式一：指定Feign应该扫描的包： 夸大了spring包扫描@EnableFeignClients(basePackages = &quot;cn.itcast.feign.clients&quot;)方式二：指定需要加载的Client接口： 更推荐这种 就只加入自己的需要的class@EnableFeignClients(clients = &#123;UserClient.class&#125;) Gateway服务网关网关的作用： 路由 负载均衡（需要用到注册中心） 统一鉴权 跨域 统一业务处理（接口调用次数+1） 访问控制 发布控制 流量染色(记录是否是网关来的 或者打标机 方便发生错误时定位) 接口保护 限制请求 信息脱敏 降级（熔断） 限流：学习令牌桶算法、学习漏桶算法，学习一下 RedisLimitHandler 超时时间 统一日志 统一文档 路由起到转发的作用，比如有接口 A 和接口 B， 网关会记录这些信息，根据用户访问的地址和参数，转发请求到对应的接口（服务器&#x2F;集群）&#x2F;a &#x3D;&gt; 接口 A&#x2F;b &#x3D;&gt; 接口 BGateway 路由：https://docs.spring.io/spring-cloud-gateway/docs/current/reference/html/#gateway-request-predicates-factories 负载均衡在路由的基础上&#x2F;c &#x3D;&gt; 服务 A &#x2F; 集群 A （随机转发到其中的某一个机器）uri 从固定地址改成 lb:xxxx 统一鉴权判断用户是否有权限进行操作，无论访问什么接口，都统一验证权限，避免重复写验证权限操作。 统一处理跨域网关统一处理跨域，不用在每个项目里单独处理GateWay 处理跨域：https://docs.spring.io/spring-cloud-gateway/docs/current/reference/html/#cors-configuration 统一业务处理把每个项目中都要做的通用逻辑放到上层（网关），统一处理，比如项目的次数统计 访问控制黑白名单，比如限制 DDOS IP 有人恶意的刷次数 直接把这个人ip拉黑 发布控制灰度发布，比如上线新接口，先给新接口分配 20% 的流量，老接口 80%， 再慢慢调整比例A接口升级了 A2.0接口 此时上线新接口给新接口分配 20% 的流量 在慢慢递增 这就是发布控制https://docs.spring.io/spring-cloud-gateway/docs/current/reference/html/#the-weight-route-predicate-factory 123456789101112spring: cloud: gateway: routes: - id: weight_high uri: https://weighthigh.org predicates: - Weight=group1, 8 - id: weight_low uri: https://weightlow.org predicates: - Weight=group1, 2 流量染色本质：给请求（流量）添加一些标识，一般是设置请求头中，添加新的请求头如果恶意用户请求绕过了网关怎么办？浏览染色用来给每个用户打上标识 有这个标识的才能访问另外一种用法是用户访问的具体功能标识 这样出现问题后 根据这个标识一层层去寻找问题来源https://docs.spring.io/spring-cloud-gateway/docs/current/reference/html/#the-addrequestheader-gatewayfilter-factory全局染色：https://docs.spring.io/spring-cloud-gateway/docs/current/reference/html/#default-filters 统一接口保护 限制请求：https://docs.spring.io/spring-cloud-gateway/docs/current/reference/html/#requestheadersize-gatewayfilter-factory限制请求次数等等 信息脱敏：https://docs.spring.io/spring-cloud-gateway/docs/current/reference/html/#the-removerequestheader-gatewayfilter-factory限制信息的脱敏 比如说 请求可能响应ip地址 网关可以从中抹掉 降级（熔断）：https://docs.spring.io/spring-cloud-gateway/docs/current/reference/html/#fallback-headers接口访问失败了 接口可能服务器宕机了 可以给用户返回另外访问的地址 就是一个提示信息 告诉用户一些有利的信息 限流：https://docs.spring.io/spring-cloud-gateway/docs/current/reference/html/#the-requestratelimiter-gatewayfilter-factory让用户每分钟能访问多少次 等等 超时时间：https://docs.spring.io/spring-cloud-gateway/docs/current/reference/html/#http-timeouts-configuration超过这个时间做一些处理 重试（业务保护）：https://docs.spring.io/spring-cloud-gateway/docs/current/reference/html/#the-retry-gatewayfilter-factory 统一日志统一的请求、响应信息记录 类似AOP切面 统一文档将下游项目的文档进行聚合，在一个页面统一查看类似语雀的知识库 知识库里有多不同的项目可以使用 knife4j : https://doc.xiaominfo.com/docs/middleware-sources/aggregation-introduction 网关的分类 全局网关（接入层网关）： 作用是负载均衡、请求日志等，不和业务逻辑绑定 (例如它是最外层的网关 用于将请求分配到不同服务器) 业务网关（微服务网关）： 存在一些业务逻辑，作用是将请求转发到不同的业务&#x2F;项目&#x2F;接口&#x2F;服务 各个微服务独立部署，职责单一，对外提供服务的时候需要有一个东西把业务聚合起来 参考文章：https://blog.csdn.net/qq_21040559&#x2F;article&#x2F;details&#x2F;122961395 什么是网关 初始化网关网关也是一个微服务项目 他也需要加入到注册中心中基本步骤如下： 创建SpringBoot工程gateway，引入网关依赖 编写启动类 编写基础配置和路由规则 启动网关服务进行测试 创建网关工程 加入依赖 并且设置启动类 12345678910&lt;!--网关--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-gateway&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!--nacos服务发现依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;&lt;/dependency&gt; 编写配置文件 指定网关配置(路由) 12345678910111213141516171819server: port: 10010 # 网关端口spring: application: name: gateway # 服务名称 cloud: nacos: server-addr: localhost:8848 # nacos地址 gateway: routes: # 网关的路由配置 - id: user-service # 路由id，自定义，只要唯一即可 # uri: http://127.0.0.1:8081 # 路由的目标地址 http就是固定地址 这种方式基本不用 uri: lb://userservice # 路由的目标地址 lb就是负载均衡，后面跟 微服务名称 predicates: # 路由断言，也就是判断请求是否符合路由规则的条件 符合的就 - Path=/user/** # 这个是按照路径匹配，只要以/user/开头就符合要求 - id: order-service uri: lb://orderservice predicates: - Path=/order/** 我们将符合Path 规则的一切请求，都代理到 uri参数指定的地址。本例中，我们将 &#x2F;user&#x2F;**开头的请求，代理到lb:&#x2F;&#x2F;userservice，lb是负载均衡，根据服务名拉取服务列表，实现负载均衡。 以上的好处 不同服务他们的ip端口可能都不一样 使用网关可以使得用户只需要访问网关请求即可 而不需要针对具体的服务ip地址去访问 总结：网关搭建步骤： 创建项目，引入nacos服务发现和gateway依赖 配置application.yml，包括服务基本信息、nacos地址、路由 路由配置包括： 路由id：路由的唯一标示 路由目标（uri）：路由的目标地址，http代表固定地址，lb代表根据服务名负载均衡 路由断言（predicates）：判断路由的规则， 路由过滤器（filters）：对请求或响应做处理 问题遗留：路由的断言是怎么做的？ 他如何判断请求是否符合我们写的呢？ 断言工厂我们在配置文件中写的断言规则只是字符串，这些字符串会被Predicate Factory（断言工厂）读取并处理，转变为路由判断的条件例如Path&#x3D;&#x2F;user&#x2F;**是按照路径匹配，这个规则是由org.springframework.cloud.gateway.handler.predicate.PathRoutePredicateFactory类来处理的，像这样的断言工厂在SpringCloudGateway还有十几个: 名称 说明 示例 After 是某个时间点后的请求 - After&#x3D;2037-01-20T17:42:47.789-07:00[America&#x2F;Denver] Before 是某个时间点之前的请求 - Before&#x3D;2031-04-13T15:14:47.433+08:00[Asia&#x2F;Shanghai] Between 是某两个时间点之前的请求 - Between&#x3D;2037-01-20T17:42:47.789-07:00[America&#x2F;Denver], 2037-01-21T17:42:47.789-07:00[America&#x2F;Denver] Cookie 请求必须包含某些cookie - Cookie&#x3D;chocolate, ch.p Header 请求必须包含某些header - Header&#x3D;X-Request-Id, \\d+ Host 请求必须是访问某个host（域名） - Host&#x3D;.somehost.org,.anotherhost.org Method 请求方式必须是指定方式 - Method&#x3D;GET,POST Path 请求路径必须符合指定规则 - Path&#x3D;&#x2F;red&#x2F;{segment},&#x2F;blue&#x2F;** Query 请求参数必须包含指定参数 - Query&#x3D;name, Jack或者- Query&#x3D;name RemoteAddr 请求者的ip必须是指定范围 - RemoteAddr&#x3D;192.168.1.1&#x2F;24 Weight 权重处理 我们只需要掌握Path这种路由工程就可以了。 过滤器工厂GatewayFilter是网关中提供的一种过滤器，可以对进入网关的请求和微服务返回的响应做处理： Spring提供了31种不同的路由1器工厂。例如： 名称 说明 AddRequestHeader 给当前请求添加一个请求头 RemoveRequestHeader 移除请求中的一个请求头 AddResponseHeader 给响应结果中添加一个响应头 RemoveResponseHeader 从响应结果中移除有一个响应头 RequestRateLimiter 限制请求的流量 对请求进行一系列的处理， 比如添加请求头、添加请求参数 我们可以通过过滤器设置请求头的信息 以此来验证用户请求是否合法等等 例如用户绕过了网关我们可以直接在请求中判断是否他是否携带了我们添加的请求头请求流程(以下流程图)： 客户端发起请求 Handler Mapping: 根据断言，将请求转发到对应的路由 Web Handler： 处理请求（一层层经过过滤器） 实际调用服务12345678spring: cloud: gateway: routes: - id: add_request_header_route uri: https://example.org filters: - AddRequestHeader=X-Request-red, blue 基本功能：对请求头、请求参数、响应头的增删改查 （请求染色 可以在这个过滤器中加入一个请求头） 添加请求头 12345678spring: cloud: gateway: routes: - id: add_request_header_route uri: https://example.org filters: - AddRequestHeader=X-Request-red, blue 添加请求参数 添加响应头 降级 限流 重试 引入：（降级） 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-circuitbreaker-reactor-resilience4j&lt;/artifactId&gt;&lt;/dependency&gt; 12345678910spring: cloud: gateway: routes: - id: user-service uri: lb://userservice predicates: - Path=/user/** filters: # 过滤器 - AddRequestHeader=Truth, Itcast is freaking awesome! # 添加请求头 默认过滤器如果要对所有的路由都生效，则可以将过滤器工厂写到default下。格式如下： 12345678910spring: cloud: gateway: routes: - id: user-service uri: lb://userservice predicates: - Path=/user/** default-filters: # 默认过滤项 - AddRequestHeader=Truth, Itcast is freaking awesome! # 请求头添加属性 名字叫做Truth 内容是IT... 总结 过滤器的作用是什么？① 对路由的请求或响应做加工处理，比如添加请求头② 配置在路由下的过滤器只对当前路由的请求生效defaultFilters的作用是什么？① 对所有路由都生效的过滤器 全局过滤器上一节学习的过滤器，网关提供了31种，但每一种过滤器的作用都是固定的。如果我们希望拦截请求，做自己的业务逻辑则没办法实现。 全局过滤器作用（函数式 更加灵活）全局过滤器的作用也是处理一切进入网关的请求和微服务响应，与GatewayFilter的作用一样。区别在于GatewayFilter通过配置定义，处理逻辑是固定的；而GlobalFilter的逻辑需要自己写代码实现。定义方式是实现GlobalFilter接口。 12345678910public interface GlobalFilter &#123; /** * 处理当前请求，有必要的话通过&#123;@link GatewayFilterChain&#125;将请求交给下一个过滤器处理 * * @param exchange 请求上下文，里面可以获取Request、Response等信息 * @param chain 用来把请求委托给下一个过滤器 * @return &#123;@code Mono&lt;Void&gt;&#125; 返回标示当前过滤器业务结束 */ Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain);&#125; 在filter中编写自定义逻辑，可以实现下列功能： 登录状态判断 权限校验 请求限流等 自定义全局过滤器需求：定义全局过滤器，拦截请求，判断请求的参数是否满足下面条件： 参数中是否有authorization， authorization参数值是否为admin 如果同时满足则放行，否则拦截实现：在gateway中定义一个过滤器： 12345678910111213141516171819202122232425262728293031package cn.itcast.gateway.filters;import org.springframework.cloud.gateway.filter.GatewayFilterChain;import org.springframework.cloud.gateway.filter.GlobalFilter;import org.springframework.core.annotation.Order;import org.springframework.http.HttpStatus;import org.springframework.stereotype.Component;import org.springframework.web.server.ServerWebExchange;import reactor.core.publisher.Mono;@Order(-1) // 设置优先级 一定要设置@Componentpublic class AuthorizeFilter implements GlobalFilter &#123; @Override public Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain) &#123; // 1.获取请求参数 MultiValueMap&lt;String, String&gt; params = exchange.getRequest().getQueryParams(); // 2.获取authorization参数 String auth = params.getFirst(&quot;authorization&quot;); // 3.校验 if (&quot;admin&quot;.equals(auth)) &#123; // 放行 return chain.filter(exchange); &#125; // 4.拦截 // 4.1.禁止访问，设置状态码 exchange.getResponse().setStatusCode(HttpStatus.FORBIDDEN); // 4.2.结束处理 return exchange.getResponse().setComplete(); &#125;&#125; 过滤器执行顺序请求进入网关会碰到三类过滤器：当前路由的过滤器、DefaultFilter、GlobalFilter请求路由后，会将当前路由过滤器和DefaultFilter、GlobalFilter，合并到一个过滤器链（集合）中，排序后依次执行每个过滤器 排序的规则是什么呢？ 每一个过滤器都必须指定一个int类型的order值，order值越小，优先级越高，执行顺序越靠前。 GlobalFilter通过实现Ordered接口，或者添加@Order注解来指定order值，由我们自己指定 路由过滤器和defaultFilter的order由Spring指定，默认是按照声明顺序从1递增。 当过滤器的order值一样时，会按照 defaultFilter &gt; 路由过滤器 &gt; GlobalFilter的顺序执行。 详细内容，可以查看源码：org.springframework.cloud.gateway.route.RouteDefinitionRouteLocator#getFilters()方法是先加载defaultFilters，然后再加载某个route的filters，然后合并。org.springframework.cloud.gateway.handler.FilteringWebHandler#handle()方法会加载全局过滤器，与前面的过滤器合并后根据order排序，组织过滤器链 网关解决跨域跨域：域名不一致就是跨域，主要包括： 域名不同： www.taobao.com 和 www.taobao.org 和 www.jd.com 和 miaosha.jd.com 域名相同，端口不同：localhost:8080和localhost8081 跨域问题：浏览器禁止请求的发起者与服务端发生跨域ajax请求，请求被浏览器拦截的问题 解决方案：CORS 解决跨域问题在gateway服务的application.yml文件中，添加下面的配置： 和处理注解的配置类设置的参数一样 12345678910111213141516171819spring: cloud: gateway: # 跨域 globalcors: # 全局的跨域处理 add-to-simple-url-handler-mapping: true # 解决options请求被拦截问题 corsConfigurations: &#x27;[/**]&#x27;: allowedOrigins: # 允许哪些网站的跨域请求 - &quot;http://localhost:8090&quot; allowedMethods: # 允许的跨域ajax的请求方式 - &quot;GET&quot; - &quot;POST&quot; - &quot;DELETE&quot; - &quot;PUT&quot; - &quot;OPTIONS&quot; allowedHeaders: &quot;*&quot; # 允许在请求中携带的头信息 allowCredentials: true # 是否允许携带cookie maxAge: 360000 # 这次跨域检测的有效期 在这个期限内 上次请求可以不用经过验证 Docker什么是Docker微服务虽然具备各种各样的优势，但服务的拆分通用给部署带来了很大的麻烦。 分布式系统中，依赖的组件非常多，不同组件之间部署时往往会产生一些冲突。 在数百上千台服务中重复部署，环境不一定一致，会遇到各种问题 比如说 一个服务可能有redis mysql nginx mq等等 他们都部署在linux上 各自依赖的版本不同 如果人工来做太麻烦了 我们需要在linux上下载多种技术的依赖包 本体等等 那得多麻烦~，因此我们需要一个容器来帮助我们拉取这些例如一个项目中，部署时需要依赖于node.js、Redis、RabbitMQ、MySQL等，这些服务部署时所需要的函数库、依赖项各不相同，甚至会有冲突。给部署带来了极大的困难。 而Docker确巧妙的解决了这些问题，Docker是如何实现的呢？Docker为了解决依赖的兼容问题的，采用了两个手段： 将应用的Libs（函数库）、Deps（依赖）、配置与应用一起打包 将每个应用放到一个隔离容器去运行，避免互相干扰 这样打包好的应用包中，既包含应用本身，也保护应用所需要的Libs、Deps，无需再操作系统上安装这些，自然就不存在不同应用之间的兼容问题了。虽然解决了不同应用的兼容问题，但是开发、测试等环境会存在差异，操作系统版本也会有差异，怎么解决这些问题呢？Docker如何解决不同系统环境的问题？ Docker将用户程序与所需要调用的系统(比如Ubuntu)函数库一起打包 Docker运行到不同操作系统时，直接基于打包的函数库，借助于操作系统的Linux内核来运行 小结Docker如何解决大型项目依赖关系复杂，不同组件依赖的兼容性问题？ Docker允许开发中将应用、依赖、函数库、配置一起打包，形成可移植镜像 Docker应用运行在容器中，使用沙箱机制，相互隔离 Docker如何解决开发、测试、生产环境有差异的问题？ Docker镜像中包含完整运行环境，包括系统函数库，仅依赖系统的Linux内核，因此可以在任意Linux操作系统上运行 Docker是一个快速交付应用、运行应用的技术，具备下列优势： 可以将程序及其依赖、运行环境一起打包为一个镜像，可以迁移到任意Linux操作系统 运行时利用沙箱机制形成隔离容器，各个应用互不干扰 启动、移除都可以通过一行命令完成，方便快捷 说白了 docker将所有你需要的东西一起打包给你 就像是他的logo 一个鲸鱼带着所有的文件 一起送给你有点意思吧 不同的箱子代表不同的依赖 他们相互隔离 Docker和虚拟机的差异： docker是一个系统进程；虚拟机是在操作系统中的操作系统 docker体积小、启动速度快、性能好；虚拟机体积大、启动速度慢、性能一般 Docker架构镜像和容器Docker中有几个重要的概念：镜像（Image）：Docker将应用程序及其所需的依赖、函数库、环境、配置等文件打包在一起，称为镜像，他是只读的，不允许任何人修改，需要用的需要自己复制一份然后去执行容器（Container）：镜像中的应用程序运行后形成的进程就是容器，只是Docker会给容器进程做隔离，对外不可见。 一切应用最终都是代码组成，都是硬盘中的一个个的字节形成的文件。只有运行时，才会加载到内存，形成进程。 而镜像，就是把一个应用在硬盘上的文件、及其运行环境、部分系统函数库文件一起打包形成的文件包。这个文件包是只读的。容器呢，就是将这些文件中编写的程序、函数复制一份加载到内存中运行，形成进程，只不过要隔离起来。因此一个镜像可以启动多次，形成多个容器进程。 例如你下载了一个QQ，如果我们将QQ在磁盘上的运行文件及其运行的操作系统依赖打包，形成QQ镜像。然后你可以启动多次，双开、甚至三开QQ，跟多个妹子聊天。 当我们使用docker创建了很多的镜像（mysql redis mq等等） 我们如何让别人可以启用我们创建好的镜像呢？ 我们又如何去使用别人创建好的镜像？类似代码平台github docker也有自己的托管平台 DockerHub开源应用程序非常多，打包这些应用往往是重复的劳动。为了避免这些重复劳动，人们就会将自己打包的应用镜像，例如Redis、MySQL镜像放到网络上，共享使用，就像GitHub的代码共享一样。 DockerHub：DockerHub是一个官方的Docker镜像的托管平台。这样的平台称为Docker Registry。 国内也有类似于DockerHub 的公开服务，比如 网易云镜像服务、阿里云镜像库等。 我们一方面可以将自己的镜像共享到DockerHub，另一方面也可以从DockerHub拉取镜像： 我们要使用Docker来操作镜像、容器，就必须要安装Docker。Docker是一个CS架构的程序，由两部分组成： 服务端(server)：Docker守护进程，负责处理Docker指令，管理镜像、容器等 客户端(client)：通过命令或RestAPI向Docker服务端发送指令。可以在本地或远程向服务端发送指令。 小结镜像： 将应用程序及其依赖、环境、配置打包在一起 容器： 镜像运行起来就是容器，一个镜像可以运行多个容器 Docker结构： 服务端：接收命令或远程请求，操作镜像或容器 客户端：发送命令或者请求到Docker服务端 DockerHub： 一个镜像托管的服务器，类似的还有阿里云镜像服务，统称为DockerRegistry 安装Docker企业部署一般都是采用Linux操作系统，而其中又数CentOS发行版占比最多，因此我们在CentOS下安装Docker。Docker 分为 CE 和 EE 两大版本。CE 即社区版（免费，支持周期 7 个月），EE 即企业版，强调安全，付费使用，支持周期 24 个月。Docker CE 分为 stabletest 和 nightly 三个更新频道。官方网站上有各种环境下的 安装指南，这里主要介绍 Docker CE 在 CentOS上的安装。 Docker CE 支持 64 位版本 CentOS 7，并且要求内核版本不低于 3.10， CentOS 7 满足最低内核的要求，所以我们在CentOS 7安装Docker。 如果之前安装过旧版本的Docker，可以使用下面命令卸载： 1234567891011yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-engine \\ docker-ce 安装首先需要大家虚拟机联网，安装yum工具 123yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 --skip-broken 然后更新本地镜像源： 12345678# 设置docker镜像源yum-config-manager \\ --add-repo \\ https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo sed -i &#x27;s/download.docker.com/mirrors.aliyun.com\\/docker-ce/g&#x27; /etc/yum.repos.d/docker-ce.repoyum makecache fast 然后输入命令：yum install -y docker-cedocker-ce为社区免费版本。稍等片刻，docker即可安装成功。Docker应用需要用到各种端口，逐一去修改防火墙设置。非常麻烦，因此建议大家直接关闭防火墙！启动docker前，一定要关闭防火墙后！！启动docker前，一定要关闭防火墙后！！启动docker前，一定要关闭防火墙后！！ 1234# 关闭systemctl stop firewalld# 禁止开机启动防火墙systemctl disable firewalld 通过命令启动docker： 12345systemctl start docker # 启动docker服务systemctl stop docker # 停止docker服务systemctl restart docker # 重启docker服务 然后输入命令，可以查看docker版本： 1docker -v docker官方镜像仓库网速较差，我们需要设置国内镜像服务：参考阿里云的镜像加速文档：https://cr.console.aliyun.com/cn-hangzhou/instances/mirrors输入以下命令 12345678sudo mkdir -p /etc/dockersudo tee /etc/docker/daemon.json &lt;&lt;-&#x27;EOF&#x27;&#123; &quot;registry-mirrors&quot;: [&quot;https://q3snh14v.mirror.aliyuncs.com&quot;]&#125;EOFsudo systemctl daemon-reloadsudo systemctl restart docker Docker的基本操作镜像名称首先来看下镜像的名称组成： 镜名称一般分两部分组成：[repository]:[tag]。 在没有指定tag时，默认是latest，代表最新版本的镜像 如图：这里的mysql就是repository，5.7就是tag，合一起就是镜像名称，代表5.7版本的MySQL镜像。 镜像命令无非就是crud常见的镜像操作命令如图： 获取镜像： 从本地获取 创建一个Dockerfile的文件执行构建镜像 docker build 从镜像服务器上拉取镜像 docker pull docker push 本地查看镜像docker images本地删除镜像 docker rmi镜像压缩为压缩包 docker save 解压压缩包为镜像 docker load 案例1-拉取、查看镜像去dockerhub上寻找需要拉取的镜像 然后拉取例如拉取nginxdocker pull nginx 不指定就是最新版本 拉取docker images 执行 查看是否拉取成功 案例2-保存、导入镜像docker save -o [保存的目标文件名称] [镜像名称]例如：docker save -o nginx.tar nginx:latest docker rmi nginx:latest 删除docker load -i nginx.tar加载 容器操作容器保护三个状态： 运行：进程正常运行 暂停：进程暂停，CPU不再运行，并不释放内存 停止：进程终止，回收进程占用的内存、CPU等资源 其中： docker run：创建并运行一个容器，处于运行状态 docker pause：让一个运行的容器暂停 docker unpause：让一个容器从暂停状态恢复运行 docker stop：停止一个运行的容器 docker start：让一个停止的容器再次运行 docker rm：删除一个容器 案例-创建并运行一个容器创建并运行nginx容器的命令：docker run --name containerName -p 80:80 -d nginx命令解读： docker run ：创建并运行一个容器 –name : 给容器起一个名字，比如叫做mn -p ：将宿主机端口与容器端口映射，冒号左侧是宿主机端口，右侧是容器端口 -d：后台运行容器 nginx：镜像名称，例如nginx 这里的-p参数，是将容器端口映射到宿主机端口。默认情况下，容器是隔离环境，我们直接访问宿主机的80端口，肯定访问不到容器中的nginx。现在，将容器的80与宿主机的80关联起来，当我们访问宿主机的80端口时，就会被映射到容器的80，这样就能访问到nginx了： 此时开启容器以后 访问ip：80 已经可以访问成功nginx了 使用日志 docker logs -f 名称 可以持续输出日志 案例-进入容器，修改文件需求：进入Nginx容器，修改HTML文件内容，添加“talk is cheap,show me the code!”提示：进入容器要用到docker exec命令。步骤：1）进入容器。进入我们刚刚创建的nginx容器的命令为： 1docker exec -it mn bash 命令解读： docker exec ：进入容器内部，执行一个命令 -it : 给当前进入的容器创建一个标准输入、输出终端，允许我们与容器交互 mn ：要进入的容器的名称 bash：进入容器后执行的命令，bash是一个linux终端交互命令 2）进入nginx的HTML所在目录 &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html容器内部会模拟一个独立的Linux文件系统，看起来如同一个linux服务器一样：nginx的环境、配置、运行文件全部都在这个文件系统中，包括我们要修改的html文件。查看DockerHub网站中的nginx页面，可以知道nginx的html目录位置在&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html我们执行命令，进入该目录：cd &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html查看目录下文件 发现有index.html 如何修改文件呢？ 我们知道docker容器内部看这像Linux但实际上他只是阉割版的 我们的nginx容器只是有他本身需要的东西 因此没有vi修改容器内没有vi命令，无法直接修改，我们用下面的命令来修改：sed -i -e &#39;s#Welcome to nginx#talk is cheap,show me the code!#g&#39; -e &#39;s#&lt;head&gt;#&lt;head&gt;&lt;meta charset=&quot;utf-8&quot;&gt;#g&#39; index.html在浏览器访问自己的虚拟机地址 exit退出修改容器 小结docker run命令的常见参数有哪些？ –name：指定容器名称 -p：指定端口映射 -d：让容器后台运行 查看容器日志的命令： docker logs 添加 -f 参数可以持续查看日志 查看容器状态： docker ps docker ps -a 查看所有容器，包括已经停止的 另外以上的修改容器文件过于麻烦 且没有记录 有没有一种方式能够完成 数据卷（容器数据管理）在之前的nginx案例中，修改nginx的html页面时，需要进入nginx内部。并且因为没有编辑器，修改文件也很麻烦。这就是因为容器与数据（容器内文件）耦合带来的后果。 要解决这个问题，必须将数据与容器解耦，这就要用到数据卷了。 什么是数据卷数据卷（volume）是一个虚拟目录，指向宿主机文件系统中的某个目录。一旦完成数据卷挂载，对容器的一切操作都会作用在数据卷对应的宿主机（linux）目录了。这样我们直接在linux下操作这个虚拟文件就相当于操作docker容器的文件这样，我们操作宿主机的&#x2F;var&#x2F;lib&#x2F;docker&#x2F;volumes&#x2F;html目录，就等于操作容器内的&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html目录了通过数据卷这样虚拟的方式 使得解耦合 数据集操作命令数据卷操作的基本语法如下：docker volume [COMMAND]docker volume命令是数据卷操作，根据命令后跟随的command来确定下一步的操作： create 创建一个volume inspect 显示一个或多个volume的信息 ls 列出所有的volume prune 删除未使用的volume rm 删除一个或多个指定的volume 需求：创建一个数据卷，并查看数据卷在宿主机的目录位置① 创建数据卷 1docker volume create html ② 查看所有数据 1docker volume ls ③ 查看数据卷详细信息卷 1docker volume inspect html 数据卷的作用： 将容器与数据分离，解耦合，方便操作容器内数据，保证数据安全 数据卷操作： docker volume create：创建数据卷 docker volume ls：查看所有数据卷 docker volume inspect：查看数据卷详细信息，包括关联的宿主机目录位置 docker volume rm：删除指定数据卷 docker volume prune：删除所有未使用的数据卷 挂载数据卷我们在创建容器时，可以通过 -v 参数来挂载一个数据卷到某个容器内目录，命令格式如下： 12345docker run \\ --name mn \\ -v html:/root/html \\ -p 8080:80 nginx \\ 这里的-v就是挂载数据卷的命令： -v html:&#x2F;root&#x2F;htm ：把html数据卷挂载到容器内的&#x2F;root&#x2F;html这个目录中 案例-给nginx挂载数据卷需求：创建一个nginx容器，修改容器内的html目录内的index.html内容分析：上个案例中，我们进入nginx容器内部，已经知道nginx的html目录所在位置&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html ，我们需要把这个目录挂载到html这个数据卷上，方便操作其中的内容。提示：运行容器时使用 -v 参数挂载数据卷步骤：① 创建容器并挂载数据卷到容器内的HTML目录docker run --name mn -v html:/usr/share/nginx/html -p 80:80 -d nginx② 进入html数据卷所在位置，并修改HTML内容 123456# 查看html数据卷的位置docker volume inspect html# 进入该目录cd /var/lib/docker/volumes/html/_data# 修改文件vi index.html 案例-给MySQL挂载本地目录容器不仅仅可以挂载数据卷，也可以直接挂载到宿主机目录上。关联关系如下： 带数据卷模式：宿主机目录 –&gt; 数据卷 —&gt; 容器内目录 直接挂载模式：宿主机目录 —&gt; 容器内目录 语法：目录挂载与数据卷挂载的语法是类似的： -v [宿主机目录]:[容器内目录] -v [宿主机文件]:[容器内文件] 需求：创建并运行一个MySQL容器，将宿主机目录直接挂载到容器实现思路如下：1）在将课前资料中的mysql.tar文件上传到虚拟机，通过load命令加载为镜像2）创建目录&#x2F;tmp&#x2F;mysql&#x2F;data3）创建目录&#x2F;tmp&#x2F;mysql&#x2F;conf，将课前资料提供的hmy.cnf文件上传到&#x2F;tmp&#x2F;mysql&#x2F;conf4）去DockerHub查阅资料，创建并运行MySQL容器，要求：① 挂载&#x2F;tmp&#x2F;mysql&#x2F;data到mysql容器内数据存储目录② 挂载&#x2F;tmp&#x2F;mysql&#x2F;conf&#x2F;hmy.cnf到mysql容器的配置文件③ 设置MySQL密码小结docker run的命令中通过 -v 参数挂载文件或目录到容器中： -v volume名称:容器内目录 -v 宿主机文件:容器内文 -v 宿主机目录:容器内目录 数据卷挂载与目录直接挂载的 数据卷挂载耦合度低，由docker来管理目录，但是目录较深，不好找 目录挂载耦合度高，需要我们自己管理目录，不过目录容易寻找查看 Dockerfile自定义镜像常见的镜像在DockerHub就能找到，但是我们自己写的项目就必须自己构建镜像了。而要自定义镜像，就必须先了解镜像的结构才行。 镜像结构**镜像是将应用程序及其需要的系统函数库、环境、配置、依赖打包而成。 **我们以MySQL为例，来看看镜像的组成结构： 简单来说，镜像就是在系统函数库、运行环境基础上，添加应用程序文件、配置文件、依赖文件等组合，然后编写好启动脚本打包在一起形成的文件。我们要构建镜像，其实就是实现上述打包的过程。 镜像是分层结构，每一层称为一个Layer•BaseImage层：包含基本的系统函数库、环境变量、文件系统•Entrypoint：入口，是镜像中应用启动的命令•其它：在BaseImage基础上添加依赖、安装程序、完成整个应用的安装和配置 Dockerfile语法构建自定义的镜像时，并不需要一个个文件去拷贝，打包。我们只需要告诉Docker，我们的镜像的组成，需要哪些BaseImage、需要拷贝什么文件、需要安装什么依赖、启动脚本是什么，将来Docker会帮助我们构建镜像。而描述上述信息的文件就是Dockerfile文件。Dockerfile就是一个文本文件，其中包含一个个的**指令(Instruction)**，用指令来说明要执行什么操作来构建镜像。每一个指令都会形成一层Layer。更新详细语法说明，请参考官网文档： https://docs.docker.com/engine/reference/builder 基于Ubuntu构建Java项目需求：基于Ubuntu镜像构建一个新镜像，运行一个java项目 步骤1：新建一个空文件夹docker-demo 步骤2：拷贝课前资料中的docker-demo.jar文件到docker-demo这个目录 步骤3：拷贝课前资料中的jdk8.tar.gz文件到docker-demo这个目录 步骤4：拷贝课前资料提供的Dockerfile到docker-demo这个目录其中的内容如下： 12345678910111213141516171819202122# 指定基础镜像FROM ubuntu:16.04# 配置环境变量，JDK的安装目录ENV JAVA_DIR=/usr/local# 拷贝jdk和java项目的包COPY ./jdk8.tar.gz $JAVA_DIR/COPY ./docker-demo.jar /tmp/app.jar# 安装JDKRUN cd $JAVA_DIR \\ &amp;&amp; tar -xf ./jdk8.tar.gz \\ &amp;&amp; mv ./jdk1.8.0_144 ./java8# 配置环境变量ENV JAVA_HOME=$JAVA_DIR/java8ENV PATH=$PATH:$JAVA_HOME/bin# 暴露端口EXPOSE 8090# 入口，java项目的启动命令ENTRYPOINT java -jar /tmp/app.jar 步骤5：进入docker-demo将准备好的docker-demo上传到虚拟机任意目录，然后进入docker-demo目录下 步骤6：运行命令：docker build -t javaweb:1.0 . 最后访问 http://192.168.150.101:8090/hello/count，其中的ip改成你的虚拟机ip 基于java8构建Java项目虽然我们可以基于Ubuntu基础镜像，添加任意自己需要的安装包，构建镜像，但是却比较麻烦。所以大多数情况下，我们都可以在一些安装了部分软件的基础镜像上做改造。例如，构建java项目的镜像，可以在已经准备了JDK的基础镜像基础上构建。需求：基于java:8-alpine镜像，将一个Java项目构建为镜像实现思路如下： ① 新建一个空的目录，然后在目录中新建一个文件，命名为Dockerfile ② 拷贝课前资料提供的docker-demo.jar到这个目录中 ③ 编写Dockerfile文件： a ）基于java:8-alpine作为基础镜像 b ）将app.jar拷贝到镜像中 c ）暴露端口 d ）编写入口ENTRYPOINT内容如下：1234FROM java:8-alpineCOPY ./app.jar /tmp/app.jarEXPOSE 8090ENTRYPOINT java -jar /tmp/app.jar ④ 使用docker build命令构建镜像 ⑤ 使用docker run创建容器并运行 小结小结： Dockerfile的本质是一个文件，通过指令描述镜像的构建过程 Dockerfile的第一行必须是FROM，从一个基础镜像来构建 基础镜像可以是基本操作系统，如Ubuntu。也可以是其他人制作好的镜像，例如：java:8-alpine Docker-Compose 集群部署手段Docker Compose可以基于Compose文件帮我们快速的部署分布式应用，而无需手动一个个创建和运行容器！ 初识DockerComposeCompose文件是一个文本文件，通过指令定义集群中的每个容器如何运行。格式如下： 12345678910111213version: &quot;3.8&quot; services: mysql: image: mysql:5.7.25 environment: MYSQL_ROOT_PASSWORD: 123 volumes: - &quot;/tmp/mysql/data:/var/lib/mysql&quot; - &quot;/tmp/mysql/conf/hmy.cnf:/etc/mysql/conf.d/hmy.cnf&quot; web: build: . ports: - &quot;8090:8090&quot; 上面的Compose文件就描述一个项目，其中包含两个容器： mysql：一个基于mysql:5.7.25镜像构建的容器，并且挂载了两个目录 web：一个基于docker build临时构建的镜像容器，映射端口时8090 DockerCompose的详细语法参考官网：https://docs.docker.com/compose/compose-file/其实DockerCompose文件可以看做是将多个docker run命令写到一个文件，只是语法稍有差异。 部署微服务集群需求：将之前学习的cloud-demo微服务集群利用DockerCompose部署实现思路：① 查看课前资料提供的cloud-demo文件夹，里面已经编写好了docker-compose文件② 修改自己的cloud-demo项目，将数据库、nacos地址都命名为docker-compose中的服务名③ 使用maven打包工具，将项目中的每个微服务都打包为app.jar④ 将打包好的app.jar拷贝到cloud-demo中的每一个对应的子目录中⑤ 将cloud-demo上传至虚拟机，利用 docker-compose up -d 来部署 compose文件查看课前资料提供的cloud-demo文件夹，里面已经编写好了docker-compose文件，而且每个微服务都准备了一个独立的目录：内容如下： 123456789101112131415161718192021222324version: &quot;3.2&quot;services: nacos: image: nacos/nacos-server environment: MODE: standalone ports: - &quot;8848:8848&quot; mysql: image: mysql:5.7.25 environment: MYSQL_ROOT_PASSWORD: 123 volumes: - &quot;$PWD/mysql/data:/var/lib/mysql&quot; - &quot;$PWD/mysql/conf:/etc/mysql/conf.d/&quot; userservice: build: ./user-service orderservice: build: ./order-service gateway: build: ./gateway ports: - &quot;10010:10010&quot; 可以看到，其中包含5个service服务： nacos：作为注册中心和配置中心 image: nacos&#x2F;nacos-server： 基于nacos&#x2F;nacos-server镜像构建 environment：环境变量 MODE: standalone：单点模式启动 ports：端口映射，这里暴露了8848端口 mysql：数据库 image: mysql:5.7.25：镜像版本是mysql:5.7.25 environment：环境变量 MYSQL_ROOT_PASSWORD: 123：设置数据库root账户的密码为123 volumes：数据卷挂载，这里挂载了mysql的data、conf目录，其中有我提前准备好的数据 userservice、orderservice、gateway：都是基于Dockerfile临时构建的 查看mysql目录，可以看到其中已经准备好了cloud_order、cloud_user表：查看微服务目录，可以看到都包含Dockerfile文件：内容如下： 123FROM java:8-alpineCOPY ./app.jar /tmp/app.jarENTRYPOINT java -jar /tmp/app.jar 修改微服务配置因为微服务将来要部署为docker容器，而容器之间互联不是通过IP地址，而是通过容器名。这里我们将order-service、user-service、gateway服务的mysql、nacos地址都修改为基于容器名的访问。如下所示： 1234567891011spring: datasource: url: jdbc:mysql://mysql:3306/cloud_order?useSSL=false username: root password: 123 driver-class-name: com.mysql.jdbc.Driver application: name: orderservice cloud: nacos: server-addr: nacos:8848 # nacos服务地址 打包接下来需要将我们的每个微服务都打包。因为之前查看到Dockerfile中的jar包名称都是app.jar，因此我们的每个微服务都需要用这个名称。可以通过修改pom.xml中的打包名称来实现，每个微服务都需要修改： 12345678910&lt;build&gt; &lt;!-- 服务打包的最终名称 --&gt; &lt;finalName&gt;app&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 打包后： 拷贝jar包到部署目录编译打包好的app.jar文件，需要放到Dockerfile的同级目录中。注意：每个微服务的app.jar放到与服务名称对应的目录，别搞错了。user-service：order-service：gateway： 部署最后，我们需要将文件整个cloud-demo文件夹上传到虚拟机中，理由DockerCompose部署。上传到任意目录：部署：进入cloud-demo目录，然后运行下面的命令：docker-compose up -d 搭建私有镜像仓库配置Docker信任地址我们的私服采用的是http协议，默认不被Docker信任，所以需要做一个配置： 12345678# 打开要修改的文件vi /etc/docker/daemon.json# 添加内容：&quot;insecure-registries&quot;:[&quot;http://192.168.150.101:8080&quot;]# 重加载systemctl daemon-reload# 重启dockersystemctl restart docker 带有图形化界面版本使用DockerCompose部署带有图象界面的DockerRegistry，命令如下： 123456789101112131415version: &#x27;3.0&#x27;services: registry: image: registry volumes: - ./registry-data:/var/lib/registry ui: image: joxit/docker-registry-ui:static ports: - 8080:80 environment: - REGISTRY_TITLE=传智教育私有仓库 - REGISTRY_URL=http://registry:5000 depends_on: - registry 推送、拉取镜像推送镜像到私有镜像服务必须先tag，步骤如下：① 重新tag本地镜像，名称前缀为私有仓库的地址：192.168.150.101:8080&#x2F;docker tag nginx:latest 192.168.150.101:8080/nginx:1.0 ② 推送镜像docker push 192.168.150.101:8080/nginx:1.0 ③ 拉取镜像docker pull 192.168.150.101:8080/nginx:1.0 MQ好处： 吞吐量提升：无需等待订阅者处理完成，响应更快速 故障隔离：服务没有直接调用，不存在级联失败问题 调用间没有阻塞，不会造成无效的资源占用 耦合度极低，每个服务都可以灵活插拔，可替换 流量削峰：不管发布事件的流量波动多大，都由Broker接收，订阅者可以按照自己的速度去处理事件 缺点： 架构复杂了，业务没有明显的流程线，不好管理 需要依赖于Broker的可靠、安全、性能 详细的mq看具体的mq章节 这里这做一些填补吞吐量：单位时间内 数据传输的大小MQ，中文是消息队列（MessageQueue），字面来看就是存放消息的队列。也就是事件驱动架构中的Broker。比较常见的MQ实现：ActiveMQ RabbitMQ RocketMQ Kafka 几种常见MQ的对比： RabbitMQ ActiveMQ RocketMQ Kafka 公司&#x2F;社区 Rabbit Apache 阿里 Apache 开发语言 Erlang Java Java Scala&amp;Java 协议支持 AMQP，XMPP，SMTP，STOMP OpenWire,STOMP，REST,XMPP,AMQP 自定义协议 自定义协议 可用性 高 一般 高 高 单机吞吐量 一般 差 高 非常高 消息延迟 微秒级 毫秒级 毫秒级 毫秒以内 消息可靠性 高 一般 高 一般 追求可用性：Kafka、 RocketMQ 、RabbitMQ追求可靠性：RabbitMQ、RocketMQ追求吞吐能力：RocketMQ、Kafka追求消息低延迟：RabbitMQ、Kafka 使用dockers配置rabbitmqdocker pull docker.io/rabbitmq:3.8-management 拉取rabbitmq -management意味着带可视化界面启动docker run --name rabbitmq -d -p 15672:15672 -p 5672:5672 699038cb2b96 密码用户都是 guest 15672是可视化的端口 5672是具体消息端口另外如何已经有rabbitmq占用了 需要删除docker rm rabbitmq 然后重新执行感慨：这个docker真的是太方便了 入门案例官方的HelloWorld是基于最基础的消息队列模型来实现的，只包括三个角色： publisher：消息发布者，将消息发送到队列queue queue：消息队列，负责接受并缓存消息 consumer：订阅队列，处理队列中的消息 publisher实现思路： 建立连接 创建Channel 声明队列 发送消息 关闭连接和channel 代码实现： 123456789101112131415161718192021222324252627282930313233343536373839404142package cn.itcast.mq.helloworld;import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;import org.junit.Test;import java.io.IOException;import java.util.concurrent.TimeoutException;public class PublisherTest &#123; @Test public void testSendMessage() throws IOException, TimeoutException &#123; // 1.建立连接 ConnectionFactory factory = new ConnectionFactory(); // 1.1.设置连接参数，分别是：主机名、端口号、vhost、用户名、密码 factory.setHost(&quot;192.168.150.101&quot;); factory.setPort(5672); factory.setVirtualHost(&quot;/&quot;); factory.setUsername(&quot;itcast&quot;); factory.setPassword(&quot;123321&quot;); // 1.2.建立连接 Connection connection = factory.newConnection(); // 2.创建通道Channel Channel channel = connection.createChannel(); // 3.创建队列 String queueName = &quot;simple.queue&quot;; channel.queueDeclare(queueName, false, false, false, null); // 4.发送消息 String message = &quot;hello, rabbitmq!&quot;; channel.basicPublish(&quot;&quot;, queueName, null, message.getBytes()); System.out.println(&quot;发送消息成功：【&quot; + message + &quot;】&quot;); // 5.关闭通道和连接 channel.close(); connection.close(); &#125;&#125; consumer实现代码思路： 建立连接 创建Channel 声明队列 订阅消息 代码实现： 1234567891011121314151617181920212223242526272829303132333435363738394041package cn.itcast.mq.helloworld;import com.rabbitmq.client.*;import java.io.IOException;import java.util.concurrent.TimeoutException;public class ConsumerTest &#123; public static void main(String[] args) throws IOException, TimeoutException &#123; // 1.建立连接 ConnectionFactory factory = new ConnectionFactory(); // 1.1.设置连接参数，分别是：主机名、端口号、vhost、用户名、密码 factory.setHost(&quot;192.168.150.101&quot;); factory.setPort(5672); factory.setVirtualHost(&quot;/&quot;); factory.setUsername(&quot;itcast&quot;); factory.setPassword(&quot;123321&quot;); // 1.2.建立连接 Connection connection = factory.newConnection(); // 2.创建通道Channel Channel channel = connection.createChannel(); // 3.创建队列 String queueName = &quot;simple.queue&quot;; channel.queueDeclare(queueName, false, false, false, null); // 4.订阅消息 channel.basicConsume(queueName, true, new DefaultConsumer(channel)&#123; @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; // 5.处理消息 String message = new String(body); System.out.println(&quot;接收到消息：【&quot; + message + &quot;】&quot;); &#125; &#125;); System.out.println(&quot;等待接收消息。。。。&quot;); &#125;&#125; 总结基本消息队列的消息发送流程： 建立connection 创建channel 利用channel声明队列 利用channel向队列发送消息 基本消息队列的消息接收流程： 建立connection 创建channel 利用channel声明队列 定义consumer的消费行为handleDelivery() 利用channel将消费者与队列绑定 springAMQPSpringAMQP是基于RabbitMQ封装的一套模板，并且还利用SpringBoot对其实现了自动装配，使用起来非常方便。SpringAmqp的官方地址：https://spring.io/projects/spring-amqpSpringAMQP提供了三个功能： 自动声明队列、交换机及其绑定关系 基于注解的监听器模式，异步接收消息 封装了RabbitTemplate工具，用于发送消息 简单队列实现在父工程mq-demo中引入依赖 12345&lt;!--AMQP依赖，包含RabbitMQ--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;&lt;/dependency&gt; 消息发送首先配置MQ地址，在publisher服务的application.yml中添加配置： 1234567spring: rabbitmq: host: 192.168.70.132 # 主机名 port: 5672 # 端口 virtual-host: / # 虚拟主机 username: guest # 用户名 password: guest # 密码 然后在publisher服务中编写测试类SpringAmqpTest，并利用RabbitTemplate实现消息发送： 1234567891011121314151617181920212223242526package cn.itcast.mq.spring;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.amqp.rabbit.core.RabbitTemplate;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.context.SpringBootTest;import org.springframework.test.context.junit4.SpringRunner;@RunWith(SpringRunner.class)@SpringBootTestpublic class SpringAmqpTest &#123; @Autowired private RabbitTemplate rabbitTemplate; @Test public void testSimpleQueue() &#123; // 队列名称 String queueName = &quot;simple.queue&quot;; // 消息 String message = &quot;hello, spring amqp!&quot;; // 发送消息 rabbitTemplate.convertAndSend(queueName, message); &#125;&#125; 消息接收首先配置MQ地址，在consumer服务的application.yml中添加配置： 1234567spring: rabbitmq: host: 192.168.70.132 # 主机名 port: 5672 # 端口 virtual-host: / # 虚拟主机 username: guest # 用户名 password: guest # 密码 然后在consumer服务的cn.itcast.mq.listener包中新建一个类SpringRabbitListener，代码如下： 12345678910111213package cn.itcast.mq.listener;import org.springframework.amqp.rabbit.annotation.RabbitListener;import org.springframework.stereotype.Component;@Componentpublic class SpringRabbitListener &#123; @RabbitListener(queues = &quot;simple.queue&quot;) public void listenSimpleQueueMessage(String msg) throws InterruptedException &#123; System.out.println(&quot;spring 消费者接收到消息：【&quot; + msg + &quot;】&quot;); &#125;&#125; 总结三步走： 依赖 配置 导入类 实现 SpringAMQP如何接收消息？•引入amqp的starter依赖•配置RabbitMQ地址•定义类，添加@Component注解•类中声明方法，添加@RabbitListener注解，方法参数就时消息 注意：消息一旦消费就会从队列删除，RabbitMQ没有消息回溯功能 WorkQueueWork queues，也被称为（Task queues），任务模型。简单来说就是让多个消费者绑定到一个队列，共同消费队列中的消息。当消息处理比较耗时的时候，可能生产消息的速度会远远大于消息的消费速度。长此以往，消息就会堆积越来越多，无法及时处理。此时就可以使用work 模型，多个消费者共同处理消息处理，速度就能大大提高了。但是真的是这样吗？这样以后实际上会平摊给多个消费者 这是因为他的消费预取限制 在spring中有一个简单的配置，可以解决这个问题。我们修改consumer服务的application.yml文件，添加配置： 12345spring: rabbitmq: listener: simple: prefetch: 1 # 每次只能获取一条消息，处理完成才能获取下一个消息 Work模型的使用： 多个消费者绑定到一个队列，同一条消息只会被一个消费者处理 通过设置prefetch来控制消费者预取的消息数量 以上的问题是 消息给了一个消费者以后就没了 队列中直接剔除了这个消息 无法满足同一消息多个消费者获取 如何解决？ 交换机 发布&#x2F;订阅发布订阅的模型如图：可以看到，在订阅模型中，多了一个exchange角色，而且过程略有变化： Publisher：生产者，也就是要发送消息的程序，但是不再发送到队列中，而是发给X（交换机） Exchange：交换机，图中的X。一方面，接收生产者发送的消息。另一方面，知道如何处理消息，例如递交给某个特别队列、递交给所有队列、或是将消息丢弃。到底如何操作，取决于Exchange的类型。Exchange有以下4种类型： Fanout：广播，将消息交给所有绑定到交换机的队列 Direct：定向 &#x2F; 直连，把消息交给符合指定routing key 的队列 Topic：通配符 &#x2F; 匹配，把消息交给符合routing pattern（路由模式） 的队列 Header：用的很少 基本不用 Consumer：消费者，与以前一样，订阅队列，没有变化 Queue：消息队列也与以前一样，接收消息、缓存消息。 Exchange（交换机）只负责转发消息，不具备存储消息的能力，因此如果没有任何队列与Exchange绑定，或者没有符合路由规则的队列，那么消息会丢失！ FanoutFanout，英文翻译是扇出，我觉得在MQ中叫广播更合适。在广播模式下，消息发送流程是这样的： 1） 可以有多个队列 2） 每个队列都要绑定到Exchange（交换机） 3） 生产者发送的消息，只能发送到交换机，交换机来决定要发给哪个队列，生产者无法决定 4） 交换机把消息发送给绑定过的所有队列 5） 订阅队列的消费者都能拿到消息 我们的计划是这样的： 创建一个交换机 itcast.fanout，类型是Fanout 创建两个队列fanout.queue1和fanout.queue2，绑定到交换机itcast.fanout 生产者 123456789101112131415161718192021222324252627282930313233@Configuration // 声明为配置类注解public class Fanoutconfig &#123; /** * @Bean 让这个对象作为一个bean注入容器 * 创建交换机 */ @Bean public FanoutExchange fanoutExchange() &#123; // 实例化对象 指定名称 return new FanoutExchange(&quot;luhumu.fanout&quot;); &#125; // 声明队列 @Bean public Queue fanoutQueue1() &#123; return new Queue(&quot;fanout.queue1&quot;); &#125; // 绑定队列和交换机 @Bean public Binding fanoutBinding(Queue fanoutQueue1, FanoutExchange fanoutExchange) &#123; return BindingBuilder.bind(fanoutQueue1).to(fanoutExchange); &#125; // 声明队列 @Bean public Queue fanoutQueue2() &#123; return new Queue(&quot;fanout.queue2&quot;); &#125; // 绑定队列和交换机 @Bean public Binding fanoutBinding2(Queue fanoutQueue2, FanoutExchange fanoutExchange) &#123; return BindingBuilder.bind(fanoutQueue2).to(fanoutExchange); &#125;&#125; 12345678910@Testpublic void testSendMessageFanoutQueue() &#123; // 队列名称 String exchange = &quot;luhumu.fanout&quot;; // 消息 String message = &quot;hello, spring amqp!&quot;; // 发送消息 rabbitTemplate.convertAndSend(exchange,&quot;&quot;, message);&#125; 消费者 12345678@RabbitListener(queues = &quot;fanout.queue1&quot;) public void listenerFanoutQueue(String msg) &#123; System.out.println(&quot;接收到的消息为1：&quot;+msg); &#125; @RabbitListener(queues = &quot;fanout.queue2&quot;) public void listenerFanout2Queue(String msg) &#123; System.out.println(&quot;接收到的消息为2：&quot;+msg); &#125; Direct在Fanout模式中，一条消息，会被所有订阅的队列都消费。但是，在某些场景下，我们希望不同的消息被不同的队列消费。这时就要用到Direct类型的Exchange。在Direct模型下： 队列与交换机的绑定，不能是任意绑定了，而是要指定一个RoutingKey（路由key） 消息的发送方在 向 Exchange发送消息时，也必须指定消息的 RoutingKey。 Exchange不再把消息交给每一个绑定的队列，而是根据消息的Routing Key进行判断，只有队列的Routingkey与消息的 Routing key完全一致，才会接收到消息 案例需求如下： 利用@RabbitListener声明Exchange、Queue、RoutingKey 在consumer服务中，编写两个消费者方法，分别监听direct.queue1和direct.queue2 在publisher中编写测试方法，向itcast. direct发送消息 消费者 基于@Bean的方式声明队列和交换机比较麻烦，Spring还提供了基于注解方式来声明。在consumer的SpringRabbitListener中添加两个消费者，同时基于注解来声明队列和交换机： 1234567891011121314151617@RabbitListener(bindings = @QueueBinding( value = @Queue(name = &quot;direct.queue1&quot;), exchange = @Exchange(name = &quot;itcast.direct&quot;, type = ExchangeTypes.DIRECT), key = &#123;&quot;red&quot;, &quot;blue&quot;&#125;))public void listenDirectQueue1(String msg)&#123; System.out.println(&quot;消费者接收到direct.queue1的消息：【&quot; + msg + &quot;】&quot;);&#125;@RabbitListener(bindings = @QueueBinding( value = @Queue(name = &quot;direct.queue2&quot;), exchange = @Exchange(name = &quot;itcast.direct&quot;, type = ExchangeTypes.DIRECT), key = &#123;&quot;red&quot;, &quot;yellow&quot;&#125;))public void listenDirectQueue2(String msg)&#123; System.out.println(&quot;消费者接收到direct.queue2的消息：【&quot; + msg + &quot;】&quot;);&#125; 生产者在publisher服务的SpringAmqpTest类中添加测试方法： 123456789@Testpublic void testSendDirectExchange() &#123; // 交换机名称 String exchangeName = &quot;itcast.direct&quot;; // 消息 String message = &quot;红色警报！日本乱排核废水，导致海洋生物变异，惊现哥斯拉！&quot;; // 发送消息 rabbitTemplate.convertAndSend(exchangeName, &quot;red&quot;, message);&#125; TopicTopic类型的Exchange与Direct相比，都是可以根据RoutingKey把消息路由到不同的队列。只不过Topic类型Exchange可以让队列在绑定Routing key 的时候使用通配符！Routingkey 一般都是有一个或多个单词组成，多个单词之间以”.”分割，例如： item.insert通配符规则：#：匹配一个或多个词*：匹配不多不少恰好1个词举例：item.#：能够匹配item.spu.insert 或者 item.spuitem.*：只能匹配item.spu 在publisher服务的SpringAmqpTest类中添加测试方法： 123456789101112/** * topicExchange */@Testpublic void testSendTopicExchange() &#123; // 交换机名称 String exchangeName = &quot;itcast.topic&quot;; // 消息 String message = &quot;喜报！孙悟空大战哥斯拉，胜!&quot;; // 发送消息 rabbitTemplate.convertAndSend(exchangeName, &quot;china.news&quot;, message);&#125; 在consumer服务的SpringRabbitListener中添加方法： 1234567891011121314151617@RabbitListener(bindings = @QueueBinding( value = @Queue(name = &quot;topic.queue1&quot;), exchange = @Exchange(name = &quot;itcast.topic&quot;, type = ExchangeTypes.TOPIC), key = &quot;china.#&quot;))public void listenTopicQueue1(String msg)&#123; System.out.println(&quot;消费者接收到topic.queue1的消息：【&quot; + msg + &quot;】&quot;);&#125;@RabbitListener(bindings = @QueueBinding( value = @Queue(name = &quot;topic.queue2&quot;), exchange = @Exchange(name = &quot;itcast.topic&quot;, type = ExchangeTypes.TOPIC), key = &quot;#.news&quot;))public void listenTopicQueue2(String msg)&#123; System.out.println(&quot;消费者接收到topic.queue2的消息：【&quot; + msg + &quot;】&quot;);&#125; 消息转换器** **之前说过，Spring会把你发送的消息序列化为字节发送给MQ，接收消息的时候，还会把字节反序列化为Java对象。只不过，默认情况下Spring采用的序列化方式是JDK序列化。众所周知，JDK序列化存在下列问题： 数据体积过大 有安全漏洞 可读性差 我们来测试一下。 测试默认转换器我们修改消息发送的代码，发送一个Map对象： 123456789@Testpublic void testSendMap() throws InterruptedException &#123; // 准备消息 Map&lt;String,Object&gt; msg = new HashMap&lt;&gt;(); msg.put(&quot;name&quot;, &quot;Jack&quot;); msg.put(&quot;age&quot;, 21); // 发送消息 rabbitTemplate.convertAndSend(&quot;simple.queue&quot;,&quot;&quot;, msg);&#125; 停止consumer服务发送消息后查看控制台： 配置JSON转换器显然，JDK序列化方式并不合适。我们希望消息体的体积更小、可读性更高，因此可以使用JSON方式来做序列化和反序列化。在publisher和consumer两个服务中都引入依赖： 12345&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.dataformat&lt;/groupId&gt; &lt;artifactId&gt;jackson-dataformat-xml&lt;/artifactId&gt; &lt;version&gt;2.9.10&lt;/version&gt;&lt;/dependency&gt; 配置消息转换器。在启动类中添加一个Bean即可： 1234@Beanpublic MessageConverter jsonMessageConverter()&#123; return new Jackson2JsonMessageConverter();&#125; 12345@RabbitListener(queues = &quot;object.queue&quot;)public void listenObjectQueue(Map&lt;String, Object&gt; msg) &#123; System.out.println(&quot;收到消息：【&quot; + msg + &quot;】&quot;); &#125; SpringAMQP中消息的序列化和反序列化是怎么实现的？•利用MessageConverter实现的，默认是JDK的序列化•注意发送方与接收方必须使用相同的MessageConverter ESElastic Stack（一套技术栈）官网：https://www.elastic.co/cn/store: 存储 就当作是数据库 用来存数据的ingest：整合 — 帮助es去拿数据的consume： 消费 — 用户可以去从store中取数据包含了数据的整合 &#x3D;&gt; 提取 &#x3D;&gt; 存储 &#x3D;&gt; 使用，一整套！beats：从各种不同类型的文件 &#x2F; 应用来 采集数据 a,b,c,d,e,aa,bb,ccLogstash：从多个采集器或数据源抽取 &#x2F; 转换数据，向 es 输送 aa,bb,cc（将以上采集到的a,b,c,d… 转为aa, bb, cc 发送）elasticsearch（es）：存储、查询数据kibana：可视化 es 的数据学会读文档！！！ Elasticsearch 概念就把 Elasticsearch 当成 MySQL 一样的数据库。Index 索引 &#x3D;&gt; MySQL 里的表（table）建表、增删改查（查询需要花费的学习时间最多）用客户端去调用 Elasticsearch（3种）语法：SQL、代码的方法（4种语法）ES 相比于 MySQL，能够自动帮我们做分词，能够非常高效、灵活的查询内容。 总结什么是elasticsearch？ 一个开源的分布式搜索引擎，可以用来实现搜索、日志统计、分析、系统监控等功能 什么是elastic stack（ELK）？ 是以elasticsearch为核心的技术栈，包括beats、Logstash、kibana、elasticsearch 什么是Lucene？ 是Apache的开源搜索引擎类库，提供了搜索引擎的核心API 倒排索引倒排索引的概念是基于MySQL这样的正向索引而言的。那么什么是正向索引呢？例如给下表（tb_goods）中的id创建索引：如果是根据id查询，那么直接走索引，查询速度非常快。理解为数据的目录，可以快速帮你找到对应的内容（怎么根据页码找到文章） 类似英语字典 知道这个字母可以直接定位具体地点 但如果是基于title做模糊查询，只能是逐行扫描数据，流程如下：1）用户搜索数据，条件是title符合”%手机%”2）逐行获取数据，比如id为1的数据3）判断数据中的title是否符合用户搜索条件4）如果符合则放入结果集，不符合则丢弃。回到步骤1逐行扫描，也就是全表扫描，随着数据量增加，其查询效率也会越来越低。当数据量达到数百万时，就是一场灾难。 倒排索引（ES使用）：怎么根据内容找到文章 （利用的是切词） 倒排索引倒排索引中有两个非常重要的概念： 文档（Document）：用来搜索的数据，其中的每一条数据就是一个文档。例如一个网页、一个商品信息 词条（Term）：对文档数据或用户搜索数据，利用某种算法分词，得到的具备含义的词语就是词条。例如：我是中国人，就可以分为：我、是、中国人、中国、国人这样的几个词条 将数据进行切词放在词条中 文档记录这个词语在那个数据中有出现 创建倒排索引是对正向索引的一种特殊处理，流程如下： 将每一个文档的数据利用算法分词，得到一个个词条 创建表，每行数据包括词条、词条所在文档id、位置等信息 因为词条唯一性，可以给词条创建索引，例如hash表结构索引 倒排索引的搜索流程如下（以搜索”华为手机”为例）：1）用户输入条件”华为手机”进行搜索。2）对用户输入内容分词，得到词条：华为、手机。3）拿着词条在倒排索引中查找，可以得到包含词条的文档id：1、2、3。4）拿着文档id到正向索引中查找具体文档。ES倒排索引例子 用户输入一段文字 “ES真的有意思” 首先他会切词 将用户输入的文字转为切分为 “ES” “真的” “有意思” 或者切的更细 根据这些词去倒排索引表中寻找 找到对应的文章id 倒排索引表 假设ES中有这些文章 文章A：你好，我是 rapper 文章B：鱼皮你好，我是coder 把这些文章内容切词 切词： 文章A：你好，我是，rapper 文章B：鱼皮，你好，我是，coder 构建倒排索引表： 词 内容 id 你好 文章A，B 我是 文章A，B rapper 文章A 鱼皮 文章B coder 文章B 用户搜：“鱼皮rapper”ES 先见搜索内容切词：鱼皮，rapper去倒排索引表找对应的文章：文章A，B 正向和倒排那么为什么一个叫做正向索引，一个叫做倒排索引呢？ 正向索引是最传统的，根据id索引的方式。但根据词条查询时，必须先逐条获取每个文档，然后判断文档中是否包含所需要的词条，是根据文档找词条的过程。 (说白了 就是拿着内容一条条对比数据库) 而倒排索引则相反，是先找到用户要搜索的词条，根据词条得到保护词条的文档的id，然后根据id获取文档。是根据词条找文档的过程。 （说白了 先对数据进行分词 记录文档信息(文档id) 查找的时候根据内容获取文档id 然后再去根据id获取） 是不是恰好反过来了？那么两者方式的优缺点是什么呢？正向索引： 优点： 可以给多个字段创建索引 根据索引字段搜索、排序速度非常快 缺点： 根据非索引字段，或者索引字段中的部分词条查找时，只能全表扫描。 倒排索引： 优点： 根据词条搜索、模糊搜索时，速度非常快 缺点： 只能给词条创建索引，而不是字段 无法根据字段做排序 elasticsearchelasticsearch是面向文档（Document）存储的，可以是数据库中的一条商品数据，一个订单信息。文档数据会被序列化为json格式后存储在elasticsearch中：而Json文档中往往包含很多的字段（Field），类似于数据库中的列。 索引和映射索引（Index），就是相同类型的文档的集合。 可以把他当成数据库的表 同一个索引他们存储的字段是相同的例如： 所有用户文档，就可以组织在一起，称为用户的索引； 所有商品的文档，可以组织在一起，称为商品的索引； 所有订单的文档，可以组织在一起，称为订单的索引； 因此，我们可以把索引当做是数据库中的表。数据库的表会有约束信息，用来定义表的结构、字段的名称、类型等信息。因此，索引库中就有映射（mapping），是索引中文档的字段约束信息，类似表的结构约束(字符串还是数据等等类型) 映射就是约束这些字段的类型。 mysql与elasticsearch我们统一的把mysql与elasticsearch的概念做一下对比： MySQL Elasticsearch 说明 Table Index 索引(index)，就是文档的集合，类似数据库的表(table) Row Document 文档（Document），就是一条条的数据，类似数据库中的行（Row），文档都是JSON格式 Column Field 字段（Field），就是JSON文档中的字段，类似数据库中的列（Column） Schema Mapping Mapping（映射）是索引中文档的约束，例如字段类型约束。类似数据库的表结构（Schema） SQL DSL DSL是elasticsearch提供的JSON风格的请求语句，用来操作elasticsearch，实现CRUD 是不是说，我们学习了elasticsearch就不再需要mysql了呢？并不是如此，两者各自有自己的擅长支出： Mysql：擅长事务类型操作，可以确保数据的安全和一致性 Elasticsearch：擅长海量数据的搜索、分析、计算 因此在企业中，往往是两者结合使用： 对安全性要求较高的写操作，使用mysql实现 对查询性能要求较高的搜索需求，使用elasticsearch实现 两者再基于某种方式，实现数据的同步，保证一致性 ES启动操作步骤 下载 elasticsearch 和 kibana（可视化） 的zip包，下载完成后解压出来 注意版本一致 elasticsearch：https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.17.9-windows-x86_64.zipkibana：https://artifacts.elastic.co/downloads/kibana/kibana-7.17.9-windows-x86_64.zip 启动 elasticsearch 在 bin 目录下运行.\\bin\\elasticsearch.bat访问http://localhost:9200/ 如果显示成功就说明 服务通了 启动 kibana.bat 在 bin 目录下运行.\\bin\\kibana.bat ES 的几种调用方式1）restful api 调用（http 请求） GET 请求：http://localhost:9200/ 直接浏览器调用成功了就是es启动了 curl 可以模拟发送请求：curl -X GET “localhost:9200&#x2F;?pretty”ES 的启动端口： 9200：给外部用户（给客户端调用）的端口 9300：给 ES 集群内部通信的（外部调用不了的） 2）kibana devtools开启kibana 访问http://localhost:5601/app/integrations/browse左侧导航栏最下面有devtools 打开就是可视化的命令行自由的对 ES 进行操作（本质也是 restful api）devtools 不建议生产环境使用**3）客户端调用 *****java 客户端、go 客户端https://www.elastic.co/guide/en/elasticsearch/client/java-api-client/7.17/_getting_started.html IK 分词器（相当于ES 插件&#x2F;扩展） 对中文友好 国内一般用这个中文友好：https://github.com/medcl/elasticsearch-analysis-ik下载地址：https://github.com/medcl/elasticsearch-analysis-ik/releases/tag/v7.17.7（注意版本一致） 下载步骤 在 elasticsearch-7.17.9 目录下新建 plugins 目录 在 plugins 目录下新建 ik 目录 将下载的 zip 包解压到 ik 目录下 将 elasticsearch-analysis-ik-7.17.7 目录中的所有内容移到 ik 目录下 重启 ES，发生错误：插件版本不匹配解决方案：https://github.com/medcl/elasticsearch-analysis-ik/issues/996 下载相近的版本，解压后修改plugin-descriptor.properties文件里面的elasticsearch.version就可以 测试执行 测试 ik_smart 分词器，结果：你好、我、是、鱼皮、我、不是、小、黑子、我、是、困、困、真爱、粉 12345POST _analyze&#123; &quot;analyzer&quot;: &quot;ik_smart&quot;, # 分词方法 &quot;text&quot;: &quot;你好我是鱼皮，我不是小黑子，我是困困真爱粉&quot; # 内容&#125; 测试 ik_max_word 分词器，结果：你好、我、是、鱼皮、我、不是、小黑、黑子、我、是、困、困、真爱、粉 12345POST _analyze&#123; &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;text&quot;: &quot;你好我是鱼皮，我不是小黑子，我是困困真爱粉&quot;&#125; ik_smart 和 ik_max_word 的区别？举例：“小黑子” ik_smart 是智能分词，尽量选择最像一个词的拆分方式，比如“小”、“黑子”ik_max_word 尽可能多地分词，可以包括组合词，比如 “小黑”、“黑子” 因为中文博大精深 不可能说每个都按照我们像的来分词 因此需要自己设定规则？思考：怎么样让 ik 按自己的想法分词？回答：自定义词典 分词器的作用是什么？ 创建倒排索引时对文档分词 用户搜索时，对输入的内容分词 IK分词器有几种模式？ ik_smart：智能切分，粗粒度 ik_max_word：最细切分，细粒度 IK分词器如何拓展词条？如何停用词条？ 利用config目录的IkAnalyzer.cfg.xml文件添加拓展词典和停用词典 在词典中添加拓展词条或者停用词条 索引库操作索引库就类似数据库表，mapping映射就类似表的结构。我们要向es中存储数据，必须先创建“库”和“表”。 mapping映射属性mapping是对索引库中文档的约束，常见的mapping属性包括： type：字段数据类型，常见的简单类型有： 字符串：text（可分词的文本）、keyword（精确值 不可再分，例如：品牌、国家、ip地址） 数值：long、integer、short、byte、double、float、 布尔：boolean 日期：date 对象：object index：是否创建索引，默认为true analyzer：使用哪种分词器 properties：该字段的子字段 例如下面的json文档： 123456789101112&#123; &quot;age&quot;: 21, &quot;weight&quot;: 52.1, &quot;isMarried&quot;: false, &quot;info&quot;: &quot;黑马程序员Java讲师&quot;, &quot;email&quot;: &quot;zy@itcast.cn&quot;, &quot;score&quot;: [99.1, 99.5, 98.9], &quot;name&quot;: &#123; &quot;firstName&quot;: &quot;云&quot;, &quot;lastName&quot;: &quot;赵&quot; &#125;&#125; 对应的每个字段映射（mapping）： age：类型为 integer；参与搜索，因此需要index为true；无需分词器 weight：类型为float；参与搜索，因此需要index为true；无需分词器 isMarried：类型为boolean；参与搜索，因此需要index为true；无需分词器 info：类型为字符串，需要分词，因此是text；参与搜索，因此需要index为true；分词器可以用ik_smart email：类型为字符串，但是不需要分词，因此是keyword；不参与搜索，因此需要index为false；无需分词器 score：虽然是数组，但是我们只看元素的类型，类型为float；参与搜索，因此需要index为true；无需分词器 name：类型为object，需要定义多个子属性 name.firstName；类型为字符串，但是不需要分词，因此是keyword；参与搜索，因此需要index为true；无需分词器 name.lastName；类型为字符串，但是不需要分词，因此是keyword；参与搜索，因此需要index为true；无需分词器 创建索引库和映射基本语法： 请求方式：PUT 请求路径：&#x2F;索引库名，可以自定义 请求参数：mapping映射 格式： 1234567891011121314151617181920212223PUT /索引库名称&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;字段名&quot;:&#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_smart&quot; &#125;, &quot;字段名2&quot;:&#123; &quot;type&quot;: &quot;keyword&quot;, &quot;index&quot;: &quot;false&quot; &#125;, &quot;字段名3&quot;:&#123; &quot;properties&quot;: &#123; &quot;子字段&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125;, // ...略 &#125; &#125;&#125; 示例：1234567891011121314151617181920212223PUT /heima&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;info&quot;:&#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_smart&quot; &#125;, &quot;email&quot;:&#123; &quot;type&quot;: &quot;keyword&quot;, &quot;index&quot;: &quot;falsae&quot; &#125;, &quot;name&quot;:&#123; &quot;properties&quot;: &#123; &quot;firstName&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125;, // ... 略 &#125; &#125;&#125; 查询索引库基本语法： 请求方式：GET 请求路径：&#x2F;索引库名 请求参数：无 格式：GET &#x2F;索引库名 示例： GET /luhumu_es 删除索引库语法： 请求方式：DELETE 请求路径：&#x2F;索引库名 请求参数：无 格式：DELETE &#x2F;索引库名 示例： DELETE /luhumu_es 修改索引库倒排索引结构虽然不复杂，但是一旦数据结构改变（比如改变了分词器），就需要重新创建倒排索引，这简直是灾难。因此索引库一旦创建，无法修改mapping。虽然无法修改mapping中已有的字段，但是却允许添加新的字段到mapping中，因为不会对倒排索引产生影响。语法说明： 12345678PUT /索引库名/_mapping&#123; &quot;properties&quot;: &#123; &quot;新字段名&quot;:&#123; &quot;type&quot;: &quot;integer&quot; &#125; &#125;&#125; 文档操作新增文档语法： 12345678910POST /索引库名/_doc/文档id&#123; &quot;字段1&quot;: &quot;值1&quot;, &quot;字段2&quot;: &quot;值2&quot;, &quot;字段3&quot;: &#123; &quot;子属性1&quot;: &quot;值3&quot;, &quot;子属性2&quot;: &quot;值4&quot; &#125;, // ...&#125; 示例： 123456789POST /heima/_doc/1&#123; &quot;info&quot;: &quot;黑马程序员Java讲师&quot;, &quot;email&quot;: &quot;zy@itcast.cn&quot;, &quot;name&quot;: &#123; &quot;firstName&quot;: &quot;云&quot;, &quot;lastName&quot;: &quot;赵&quot; &#125;&#125; 查询文档根据rest风格，新增是post，查询应该是get，不过查询一般都需要条件，这里我们把文档id带上。语法：GET /&#123;索引库名称&#125;/_doc/&#123;id&#125;通过kibana查看数据：GET /heima/_doc/1 删除文档删除使用DELETE请求，同样，需要根据id进行删除：语法：DELETE /&#123;索引库名&#125;/_doc/id值示例： 12# 根据id删除数据DELETE /heima/_doc/1 修改文档修改有两种方式： 全量修改：直接覆盖原来的文档 增量修改：修改文档中的部分字段 全量修改全量修改是覆盖原来的文档，其本质是： 根据指定的id删除文档 新增一个相同id的文档 注意：如果根据id删除时，id不存在，第二步的新增也会执行，也就从修改变成了新增操作了。语法： 123456PUT /&#123;索引库名&#125;/_doc/文档id&#123; &quot;字段1&quot;: &quot;值1&quot;, &quot;字段2&quot;: &quot;值2&quot;, // ... 略&#125; 示例： 123456789PUT /heima/_doc/1&#123; &quot;info&quot;: &quot;黑马程序员高级Java讲师&quot;, &quot;email&quot;: &quot;zy@itcast.cn&quot;, &quot;name&quot;: &#123; &quot;firstName&quot;: &quot;云&quot;, &quot;lastName&quot;: &quot;赵&quot; &#125;&#125; 增量修改增量修改是只修改指定id匹配的文档中的部分字段。语法： 123456POST /&#123;索引库名&#125;/_update/文档id&#123; &quot;doc&quot;: &#123; &quot;字段名&quot;: &quot;新的值&quot;, &#125;&#125; 示例： 123456POST /heima/_update/1&#123; &quot;doc&quot;: &#123; &quot;email&quot;: &quot;ZhaoYun@itcast.cn&quot; &#125;&#125; 总结文档操作有哪些？ 创建文档：POST &#x2F;{索引库名}&#x2F;_doc&#x2F;文档id { json文档 } 查询文档：GET &#x2F;{索引库名}&#x2F;_doc&#x2F;文档id 删除文档：DELETE &#x2F;{索引库名}&#x2F;_doc&#x2F;文档id 修改文档： 全量修改：PUT &#x2F;{索引库名}&#x2F;_doc&#x2F;文档id { json文档 } 增量修改：POST &#x2F;{索引库名}&#x2F;_update&#x2F;文档id { “doc”: {字段}} DSL查询文档elasticsearch的查询依然是基于JSON风格的DSL来实现的。 DSL查询分类Elasticsearch提供了基于JSON的DSL（Domain Specific Language）来定义查询。常见的查询类型包括： 查询所有：查询出所有数据，一般测试用。例如：match_all 全文检索（full text）查询：利用分词器对用户输入内容分词，然后去倒排索引库中匹配。例如： match_query multi_match_query 精确查询：根据精确词条值查找数据，一般是查找keyword、数值、日期、boolean等类型字段。例如： ids range term 地理（geo）查询：根据经纬度查询。例如： geo_distance geo_bounding_box 复合（compound）查询：复合查询可以将上述各种查询条件组合起来，合并查询条件。例如： bool function_score 查询的语法基本一致： 12345678GET /索引名/_search&#123; &quot;query&quot;: &#123; &quot;查询类型&quot;: &#123; &quot;查询条件&quot;: &quot;条件值&quot; &#125; &#125;&#125; 我们以查询所有为例，其中： 查询类型为match_all 没有查询条件12345678// 查询所有GET /indexName/_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123; &#125; &#125;&#125; 其它查询无非就是查询类型、查询条件的变化。另外这里查询出来的数据只会展示几条 作为展示 在DevTools工具中 全文检索查询全文检索查询的基本流程如下： 对用户搜索的内容做分词，得到词条 根据词条去倒排索引库中匹配，得到文档id 根据文档id找到文档，返回给用户 比较常用的场景包括： 商城的输入框搜索 百度输入框搜索 基本语法常见的全文检索查询包括： match查询：单字段查询 multi_match查询：多字段查询，任意一个字段符合条件就算符合查询条件 match查询语法如下： 建议这种方式 12345678GET /indexName/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;FIELD&quot;: &quot;TEXT&quot; # 字段名 字段值 &#125; &#125;&#125; mulit_match语法如下： 123456789GET /indexName/_search&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;TEXT&quot;, # 查询的字段 &quot;fields&quot;: [&quot;FIELD1&quot;, &quot; FIELD12&quot;] # 指定多个字段名进行检索 任意满足即可 &#125; &#125;&#125; 实例： 12345678910111213141516171819# match/luhumu_es/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;all&quot; : &quot;2&quot; &#125; &#125;&#125;# mulit_matchGET /luhumu_es/_search&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;2&quot;, &quot;fields&quot;: [&quot;age&quot;, &quot;name&quot;] &#125; &#125;&#125; 精准查询精确查询一般是查找keyword、数值、日期、boolean等类型字段。所以不会对搜索条件分词。常见的有： term：根据词条精确值查询 range：根据值的范围查询 term查询因为精确查询的字段搜是不分词的字段，因此查询的条件也必须是不分词的词条。查询时，用户输入的内容跟自动值完全匹配时才认为符合条件。如果用户输入的内容过多，反而搜索不到数据。语法说明： 1234567891011// term查询GET /indexName/_search&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;FIELD&quot;: &#123; &quot;value&quot;: &quot;VALUE&quot; &#125; &#125; &#125;&#125; 示例： 12345678910GET /luhumu_es/_search&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;age&quot;: &#123; &quot;value&quot;: &quot;12&quot; &#125; &#125; &#125;&#125; 当我搜索的是精确词条时，能正确查询出结果： 1234567891011121314151617181920212223242526272829&#123; &quot;took&quot; : 0, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ # 击中 也就是查询到的具体消息 &#123; &quot;_index&quot; : &quot;luhumu_es&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;age&quot; : &quot;12321&quot;, &quot;email&quot; : &quot;code&quot; &#125; &#125; ] &#125;&#125; 但是，当我搜索的内容不是词条，而是多个词语形成的短语时，反而搜索不到： 12345678910111213141516171819&#123; &quot;took&quot; : 0, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 0, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;&#125; range查询范围查询，一般应用在对数值类型做范围过滤的时候。比如做价格范围过滤。基本语法： 123456789101112// range查询GET /indexName/_search&#123; &quot;query&quot;: &#123; &quot;range&quot;: &#123; &quot;FIELD&quot;: &#123; &quot;gte&quot;: 10, // 这里的gte代表大于等于，gt则代表大于 &quot;lte&quot;: 20 // lte代表小于等于，lt则代表小于 &#125; &#125; &#125;&#125; 示例： 1234567891011GET /luhumu_es/_search&#123; &quot;query&quot;: &#123; &quot;range&quot;: &#123; &quot;age&quot;: &#123; &quot;gte&quot;: 10, &quot;lte&quot;: 99999 &#125; &#125; &#125;&#125; 总结精确查询常见的有哪些？ term查询：根据词条精确匹配，一般搜索keyword类型、数值类型、布尔类型、日期类型字段 range查询：根据数值范围查询，可以是数值、日期的范围 地理坐标查询所谓的地理坐标查询，其实就是根据经纬度查询，官方文档：https://www.elastic.co/guide/en/elasticsearch/reference/current/geo-queries.html常见的使用场景包括： 携程：搜索我附近的酒店 滴滴：搜索我附近的出租车 微信：搜索我附近的人 矩形范围查询矩形范围查询，也就是geo_bounding_box查询，查询坐标落在某个矩形范围的所有文档：查询时，需要指定矩形的左上、右下两个点的坐标，然后画出一个矩形，落在该矩形内的都是符合条件的点。语法如下： 123456789101112131415161718// geo_bounding_box查询GET /indexName/_search&#123; &quot;query&quot;: &#123; &quot;geo_bounding_box&quot;: &#123; &quot;FIELD&quot;: &#123; &quot;top_left&quot;: &#123; // 左上点 &quot;lat&quot;: 31.1, &quot;lon&quot;: 121.5 &#125;, &quot;bottom_right&quot;: &#123; // 右下点 &quot;lat&quot;: 30.9, &quot;lon&quot;: 121.7 &#125; &#125; &#125; &#125;&#125; 附近查询附近查询，也叫做距离查询（geo_distance）：查询到指定中心点小于某个距离值的所有文档。换句话来说，在地图上找一个点作为圆心，以指定距离为半径，画一个圆，落在圆内的坐标都算符合条件：语法说明： 12345678910// geo_distance 查询GET /indexName/_search&#123; &quot;query&quot;: &#123; &quot;geo_distance&quot;: &#123; &quot;distance&quot;: &quot;15km&quot;, // 半径 &quot;FIELD&quot;: &quot;31.21,121.5&quot; // 圆心 &#125; &#125;&#125; 复合查询复合（compound）查询：复合查询可以将其它简单查询组合起来，实现更复杂的搜索逻辑。常见的有两种： fuction score：算分函数查询，可以控制文档相关性算分，控制文档排名 bool query：布尔查询，利用逻辑关系组合多个其它的查询，实现复杂搜索 相关性算分(打分机制)当我们利用match查询时，文档结果会根据与搜索词条的关联度打分（_score），返回结果时按照分值降序排列。例如，我们搜索 “虹桥如家”，结果如下： 1234567891011121314151617181920[ &#123; &quot;_score&quot; : 17.850193, &quot;_source&quot; : &#123; &quot;name&quot; : &quot;虹桥如家酒店真不错&quot;, &#125; &#125;, &#123; &quot;_score&quot; : 12.259849, &quot;_source&quot; : &#123; &quot;name&quot; : &quot;外滩如家酒店真不错&quot;, &#125; &#125;, &#123; &quot;_score&quot; : 11.91091, &quot;_source&quot; : &#123; &quot;name&quot; : &quot;迪士尼如家酒店真不错&quot;, &#125; &#125;] 在elasticsearch中，早期使用的打分算法是TF-IDF算法，公式如下： TF-IDF算法有一各缺陷，就是词条频率越高，文档得分也会越高，单个词条对文档影响较大。而BM25则会让单个词条的算分有一个上限，曲线更加平滑 小结：elasticsearch会根据词条和文档的相关度做打分，算法由两种： TF-IDF算法 BM25算法，elasticsearch5.1版本后采用的算法 算分函数查询根据相关度打分是比较合理的需求，但合理的不一定是产品经理需要的。以百度为例，你搜索的结果中，并不是相关度越高排名越靠前，而是谁掏的钱多排名就越靠前。要想认为控制相关性算分，就需要利用elasticsearch中的function score 查询了。 语法说明function score 查询中包含四部分内容： 原始查询条件：query部分，基于这个条件搜索文档，并且基于BM25算法给文档打分，原始算分（query score) 过滤条件：filter部分，符合该条件的文档才会重新算分 算分函数：符合filter条件的文档要根据这个函数做运算，得到的函数算分（function score），有四种函数 weight：函数结果是常量 field_value_factor：以文档中的某个字段值作为函数结果 random_score：以随机数作为函数结果 script_score：自定义算分函数算法 运算模式：算分函数的结果、原始查询的相关性算分，两者之间的运算方式，包括： multiply：相乘 replace：用function score替换query score 其它，例如：sum、avg、max、min function score的运行流程如下： 1）根据原始条件查询搜索文档，并且计算相关性算分，称为原始算分（query score） 2）根据过滤条件，过滤文档 3）符合过滤条件的文档，基于算分函数运算，得到函数算分（function score） 4）将原始算分（query score）和函数算分（function score）基于运算模式做运算，得到最终结果，作为相关性算分。 因此，其中的关键点是： 过滤条件：决定哪些文档的算分被修改 算分函数：决定函数算分的算法 运算模式：决定最终算分结果 示例需求：给“如家”这个品牌的酒店排名靠前一些翻译一下这个需求，转换为之前说的四个要点： 原始条件：不确定，可以任意变化 过滤条件：brand &#x3D; “如家” 算分函数：可以简单粗暴，直接给固定的算分结果，weight 运算模式：比如求和 因此最终的DSL语句如下： 12345678910111213141516171819GET /hotel/_search&#123; &quot;query&quot;: &#123; &quot;function_score&quot;: &#123; &quot;query&quot;: &#123; .... &#125;, // 原始查询，可以是任意条件 &quot;functions&quot;: [ // 算分函数 &#123; &quot;filter&quot;: &#123; // 满足的条件，品牌必须是如家 &quot;term&quot;: &#123; &quot;brand&quot;: &quot;如家&quot; &#125; &#125;, &quot;weight&quot;: 2 // 算分权重为2 &#125; ], &quot;boost_mode&quot;: &quot;sum&quot; // 加权模式，求和 &#125; &#125;&#125; 小结function score query定义的三要素是什么？ 过滤条件：哪些文档要加分 算分函数：如何计算function score 加权方式：function score 与 query score如何运算 布尔查询布尔查询是一个或多个查询子句的组合，每一个子句就是一个子查询。子查询的组合方式有： must：必须匹配每个子查询，类似“与” should：选择性匹配子查询，类似“或” must_not：必须不匹配，不参与算分，类似“非” filter：必须匹配，不参与算分 比如在搜索酒店时，除了关键字搜索外，我们还可能根据品牌、价格、城市等字段做过滤：每一个不同的字段，其查询的条件、方式都不一样，必须是多个不同的查询，而要组合这些查询，就必须用bool查询了。需要注意的是，搜索时，参与打分的字段越多，查询的性能也越差。因此这种多条件查询时，建议这样做： 搜索框的关键字搜索，是全文检索查询，使用must查询，参与算分 其它过滤条件，采用filter查询。不参与算分 1）语法示例：1234567891011121314151617181920GET /hotel/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123;&quot;term&quot;: &#123;&quot;city&quot;: &quot;上海&quot; &#125;&#125; ], &quot;should&quot;: [ &#123;&quot;term&quot;: &#123;&quot;brand&quot;: &quot;皇冠假日&quot; &#125;&#125;, &#123;&quot;term&quot;: &#123;&quot;brand&quot;: &quot;华美达&quot; &#125;&#125; ], &quot;must_not&quot;: [ &#123; &quot;range&quot;: &#123; &quot;price&quot;: &#123; &quot;lte&quot;: 500 &#125; &#125;&#125; ], &quot;filter&quot;: [ &#123; &quot;range&quot;: &#123;&quot;score&quot;: &#123; &quot;gte&quot;: 45 &#125; &#125;&#125; ] &#125; &#125;&#125; 2）示例需求：搜索名字包含“如家”，价格不高于400，在坐标31.21,121.5周围10km范围内的酒店。分析： 名称搜索，属于全文检索查询，应该参与算分。放到must中 价格不高于400，用range查询，属于过滤条件，不参与算分。放到must_not中 周围10km范围内，用geo_distance查询，属于过滤条件，不参与算分。放到filter中 3）小结bool查询有几种逻辑关系？ must：必须匹配的条件，可以理解为“与” should：选择性匹配的条件，可以理解为“或” must_not：必须不匹配的条件，不参与打分 filter：必须匹配的条件，不参与打分 排序elasticsearch默认是根据相关度算分（_score）来排序，但是也支持自定义方式对搜索结果排序。可以排序字段类型有：keyword类型、数值类型、地理坐标类型、日期类型等。 普通字段排序keyword、数值、日期类型排序的语法基本一致。语法： 1234567891011GET /indexName/_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;sort&quot;: [ &#123; &quot;FIELD&quot;: &quot;desc&quot; // 排序字段、排序方式ASC、DESC &#125; ]&#125; 排序条件是一个数组，也就是可以写多个排序条件。按照声明的顺序，当第一个条件相等时，再按照第二个条件排序，以此类推示例：需求描述：酒店数据按照用户评价（score)降序排序，评价相同的按照价格(price)升序排序 地理坐标排序地理坐标排序略有不同。语法说明： 123456789101112131415GET /indexName/_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;sort&quot;: [ &#123; &quot;_geo_distance&quot; : &#123; &quot;FIELD&quot; : &quot;纬度，经度&quot;, // 文档中geo_point类型的字段名、目标坐标点 &quot;order&quot; : &quot;asc&quot;, // 排序方式 &quot;unit&quot; : &quot;km&quot; // 排序的距离单位 &#125; &#125; ]&#125; 这个查询的含义是： 指定一个坐标，作为目标点 计算每一个文档中，指定字段（必须是geo_point类型）的坐标 到目标点的距离是多少 根据距离排序 示例：需求描述：实现对酒店数据按照到你的位置坐标的距离升序排序提示：获取你的位置的经纬度的方式：https://lbs.amap.com/demo/jsapi-v2/example/map/click-to-get-lnglat/假设我的位置是：31.034661，121.612282，寻找我周围距离最近的酒店。 分页elasticsearch 默认情况下只返回top10的数据。而如果要查询更多数据就需要修改分页参数了。elasticsearch中通过修改from、size参数来控制要返回的分页结果： from：从第几个文档开始 size：总共查询几个文档 类似于mysql中的limit ?, ? 基本的分页分页的基本语法如下： 1234567891011GET /hotel/_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;from&quot;: 0, // 分页开始的位置，默认为0 &quot;size&quot;: 10, // 期望获取的文档总数 &quot;sort&quot;: [ &#123;&quot;price&quot;: &quot;asc&quot;&#125; ]&#125; 深度分页问题现在，我要查询990~1000的数据，查询逻辑要这么写： 1234567891011GET /hotel/_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;from&quot;: 990, // 分页开始的位置，默认为0 &quot;size&quot;: 10, // 期望获取的文档总数 &quot;sort&quot;: [ &#123;&quot;price&quot;: &quot;asc&quot;&#125; ]&#125; 这里是查询990开始的数据，也就是 第990第1000条 数据。不过，elasticsearch内部分页时，必须先查询 01000条，然后截取其中的990 ~ 1000的这10条： 查询TOP1000，如果es是单点模式，这并无太大影响。但是elasticsearch将来一定是集群，例如我集群有5个节点，我要查询TOP1000的数据，并不是每个节点查询200条就可以了。因为节点A的TOP200，在另一个节点可能排到10000名以外了。因此要想获取整个集群的TOP1000，必须先查询出每个节点的TOP1000，汇总结果后，重新排名，重新截取TOP1000。那如果我要查询9900~10000的数据呢？是不是要先查询TOP10000呢？那每个节点都要查询10000条？汇总到内存中？当查询分页深度较大时，汇总数据过多，对内存和CPU会产生非常大的压力，因此elasticsearch会禁止from+ size 超过10000的请求。针对深度分页，ES提供了两种解决方案，官方文档： search after：分页时需要排序，原理是从上一次的排序值开始，查询下一页数据。官方推荐使用的方式。 scroll：原理将排序后的文档id形成快照，保存在内存。官方已经不推荐使用。 2.2.3.小结分页查询的常见实现方案以及优缺点： from + size： 优点：支持随机翻页 缺点：深度分页问题，默认查询上限（from + size）是10000 场景：百度、京东、谷歌、淘宝这样的随机翻页搜索 after search： 优点：没有查询上限（单次查询的size不超过10000） 缺点：只能向后逐页查询，不支持随机翻页 场景：没有随机翻页需求的搜索，例如手机向下滚动翻页 scroll： 优点：没有查询上限（单次查询的size不超过10000） 缺点：会有额外内存消耗，并且搜索结果是非实时的 场景：海量数据的获取和迁移。从ES7.1开始不推荐，建议用 after search方案。 高亮高亮原理什么是高亮显示呢？我们在百度，京东搜索时，关键字会变成红色，比较醒目，这叫高亮显示高亮显示的实现分为两步： 1）给文档中的所有关键字都添加一个标签，例如标签 2）页面给标签编写CSS样式 实现高亮高亮的语法： 12345678910111213141516GET /hotel/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;FIELD&quot;: &quot;TEXT&quot; // 查询条件，高亮一定要使用全文检索查询 &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; // 指定要高亮的字段 &quot;FIELD&quot;: &#123; &quot;pre_tags&quot;: &quot;&lt;em&gt;&quot;, // 用来标记高亮字段的前置标签 &quot;post_tags&quot;: &quot;&lt;/em&gt;&quot; // 用来标记高亮字段的后置标签 &#125; &#125; &#125;&#125; 注意： 高亮是对关键字高亮，因此搜索条件必须带有关键字，而不能是范围这样的查询。 默认情况下，高亮的字段，必须与搜索指定的字段一致，否则无法高亮 如果要对非搜索字段高亮，则需要添加一个属性：required_field_match&#x3D;false 示例： 总结查询的DSL是一个大的JSON对象，包含下列属性： query：查询条件 from和size：分页条件 sort：排序条件 highlight：高亮条件 示例： RestAPIsql 123456789101112131415CREATE TABLE `tb_hotel` ( `id` bigint(20) NOT NULL COMMENT &#x27;酒店id&#x27;, `name` varchar(255) NOT NULL COMMENT &#x27;酒店名称；例：7天酒店&#x27;, `address` varchar(255) NOT NULL COMMENT &#x27;酒店地址；例：航头路&#x27;, `price` int(10) NOT NULL COMMENT &#x27;酒店价格；例：329&#x27;, `score` int(2) NOT NULL COMMENT &#x27;酒店评分；例：45，就是4.5分&#x27;, `brand` varchar(32) NOT NULL COMMENT &#x27;酒店品牌；例：如家&#x27;, `city` varchar(32) NOT NULL COMMENT &#x27;所在城市；例：上海&#x27;, `star_name` varchar(16) DEFAULT NULL COMMENT &#x27;酒店星级，从低到高分别是：1星到5星，1钻到5钻&#x27;, `business` varchar(255) DEFAULT NULL COMMENT &#x27;商圈；例：虹桥&#x27;, `latitude` varchar(32) NOT NULL COMMENT &#x27;纬度；例：31.2497&#x27;, `longitude` varchar(32) NOT NULL COMMENT &#x27;经度；例：120.3925&#x27;, `pic` varchar(255) DEFAULT NULL COMMENT &#x27;酒店图片；例:/img/1.jpg&#x27;, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; es mapping 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950PUT /hotel&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;id&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;name&quot;:&#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;copy_to&quot;: &quot;all&quot; # copy到all中 &#125;, &quot;address&quot;:&#123; &quot;type&quot;: &quot;keyword&quot;, &quot;index&quot;: false &#125;, &quot;price&quot;:&#123; &quot;type&quot;: &quot;integer&quot; &#125;, &quot;score&quot;:&#123; &quot;type&quot;: &quot;integer&quot; &#125;, &quot;brand&quot;:&#123; &quot;type&quot;: &quot;keyword&quot;, &quot;copy_to&quot;: &quot;all&quot; &#125;, &quot;city&quot;:&#123; &quot;type&quot;: &quot;keyword&quot;, &quot;copy_to&quot;: &quot;all&quot; &#125;, &quot;starName&quot;:&#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;business&quot;:&#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;location&quot;:&#123; &quot;type&quot;: &quot;geo_point&quot; &#125;, &quot;pic&quot;:&#123; &quot;type&quot;: &quot;keyword&quot;, &quot;index&quot;: false &#125;, &quot;all&quot;:&#123; # 将多个字copy为一个 这样就可以进行用户通过多个字段进行检索 &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; &#125; &#125; &#125;&#125; 几个特殊字段说明： location：地理坐标，里面包含精度、纬度 all：一个组合字段，其目的是将多字段的值 利用copy_to合并为一个字段 这样用户就可以只输入一个字符串同时搜索多个字段然后搜索 es根据这个组合的字段进行排查 他并不是真的copy 只是进行了索引copy，提供给用户搜索 （我们都多个字段参与搜索 用户搜索可能根据多个字段搜索 查询条件不是一个值而是多个 而如果根据多个就需要去检索多个 搜索效率就低了 单个效率更高 可是我就希望用户能根据多个字段搜索到内容 因此我们把这些字段copy为一个 用户可以搜索多个 但是我们后台只用这个一个字段检索） Java 操作 ES3 种： ES 官方的 Java API 不建议 https://www.elastic.co/guide/en/elasticsearch/client/java-api-client/7.17/introduction.html快速开始：https://www.elastic.co/guide/en/elasticsearch/client/java-api-client/7.17/connecting.html ES 以前的官方 Java API，HighLeavelRestClient（已废弃，不建议用） Spring Data Elasticsearch(推荐) 底层是用的RestClientspring-data 系列：spring 提供的操作数据库的框架 一套spring-data-redis：操作 redis 的一套方法spring-data-mongodb：操作 mongodb的一套方法spring-data-elasticsearch：操作 elasticsearch 的一套方法官方文档：https://docs.spring.io/spring-data/elasticsearch/docs/4.4.10/reference/html/自定义方法：用户可以指定接口的方法名称，框架帮你自动生成查询 使用java进行es增删改查步骤 引入 jar 包12345&lt;!-- elasticsearch--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-elasticsearch&lt;/artifactId&gt;&lt;/dependency&gt; 使用 Elasticsearch进行增删改查 方式1：ElasticsearchRepository&lt;PostEsDTO, Long&gt;，默认提供了简单的增删改查，多用于可预期的、相对没那么复杂的查询、自定义查询，返回结果相对简单直接。 123public interface PostEsDao extends ElasticsearchRepository&lt;PostEsDTO, Long&gt; &#123;&#125; 1234567@Data@NoArgsConstructor@Document(indexName = &quot;hotel&quot;) // 这里必须对应指定 es的索引名称public class HotelDoc &#123; // ...&#125; 测试： 1234567891011121314151617181920212223@Resource public HotelEsMapper hotelEsDao; @Test public void selectbyId() &#123; Optional&lt;HotelDoc&gt; hotel = hotelEsDao.findById(36934L); System.out.println(hotel); &#125; @Test public void findAll() &#123; Iterable&lt;HotelDoc&gt; hotel = hotelEsDao.findAll(); hotel.forEach(System.out::println); &#125;@Test void testSelect() &#123; System.out.println(hotelEsDao.count()); Page&lt;HotelDoc&gt; PostPage = hotelEsDao.findAll( PageRequest.of(0, 5)); //分页查询 List&lt;HotelDoc&gt; postList = PostPage.getContent(); System.out.println(postList); Optional&lt;HotelDoc&gt; byId = hotelEsDao.findById(1L); // 根据 Id 查询 System.out.println(byId); &#125; 或者可以在 可视化窗口 GET post&#x2F;_doc&#x2F;id值ES 中，_开头的字段表示系统默认字段，比如 _id，如果系统不指定，会自动生成。但是不会在 _source 字段中补充 id 的值，所以建议大家手动指定。 第二种方式：Spring 默认给我们提供的操作 es 的客户端对象：ElasticsearchRestTemplate，也提供了简单的增删改查，它的增删改查更灵活，适用于更复杂的操作，返回结果更完整，但需要自己解析。 对于复杂的查询，建议用第二种方式。三个步骤： 取参数 把参数组合为 ES 支持的搜索条件 从返回值中取结果 查询语法 不用记 忘了跑一边就好查询 DSL：参考文档：https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-filter-context.htmlhttps://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl-bool-query.html 123456789101112131415GET post/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; // 组合条件 也就是有多个条件 &quot;must&quot;: [ // 必须都满足 查询出来的数据 会显示打分数据 &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;鱼皮&quot; &#125;&#125;, // match 模糊查询 &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;知识星球&quot; &#125;&#125; ], &quot;filter&quot;: [ // 只是过滤条件 不会影响打分 &#123; &quot;term&quot;: &#123; &quot;status&quot;: &quot;published&quot; &#125;&#125;, // term 精确查询 &#123; &quot;range&quot;: &#123; &quot;publish_date&quot;: &#123; &quot;gte&quot;: &quot;2015-01-01&quot; &#125;&#125;&#125; // range 范围查询 ] &#125; &#125;&#125; wildcard 模糊查询regexp 正则匹配查询 查询结果中，score 代表匹配方法建议先测试 DSL、再翻译成 Java 当忘了就参考这一段 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677GET post/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; // 组合条件 &quot;must_not&quot;: [ // 必须不满足 与他对应的must 就是其中的必须都满足 &#123; &quot;match&quot;: &#123; &quot;notId&quot;: &quot;&quot; // 写在must_not中 也就是必须不满足 &#125; &#125; ], &quot;should&quot;: [ // 只满足一部分就可以 &#123; &quot;match&quot;: &#123; // 模糊查询 &quot;title&quot;: &quot;&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;desc&quot;: &quot;&quot; &#125; &#125; ], &quot;filter&quot;: [ // 过滤 &#123; // term 就是必须的意思 &quot;term&quot;: &#123; // isDelete必须是0 &quot;isDelete&quot;: 0 &#125; &#125;, &#123; &quot;term&quot;: &#123; // id必须是1 &quot;id&quot;: 1 &#125; &#125;, &#123; &quot;term&quot;: &#123; &quot;tags&quot;: &quot;java&quot; // tags必须有java &#125; &#125;, &#123; &quot;term&quot;: &#123; &quot;tags&quot;: &quot;框架&quot;//tags必须有框架 &#125; &#125; ], &quot;minimum_should_match&quot;: 0 // 这是和should配合 指定满足的那一部分是多少 这里是0 should中满不满足都无所谓 如果是1 就必须满足一个 &#125; &#125;, &quot;from&quot;: 0, //分⻚ 从0开始 &quot;size&quot;: 5, //分⻚ 五条 &quot;_source&quot;: [ // 要查的字段 &quot;name&quot;, &quot;_createtime&quot;, &quot;desc&quot;, &quot;reviewStatus&quot;, &quot;priority&quot;, &quot;tags&quot; ], &quot;sort&quot;: [//排序 &#123; &quot;priority&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125;, &#123; &quot;_score&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125;, &#123; &quot;publishTime&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125; ]&#125; 将以上翻译为 Java： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697public Page&lt;Post&gt; searchFromEs(PostQueryRequest postQueryRequest) &#123; Long id = postQueryRequest.getId(); Long notId = postQueryRequest.getNotId(); String searchText = postQueryRequest.getSearchText(); String title = postQueryRequest.getTitle(); String content = postQueryRequest.getContent(); List&lt;String&gt; tagList = postQueryRequest.getTags(); List&lt;String&gt; orTagList = postQueryRequest.getOrTags(); Long userId = postQueryRequest.getUserId(); // es 起始页为 0 long current = postQueryRequest.getCurrent() - 1; long pageSize = postQueryRequest.getPageSize(); String sortField = postQueryRequest.getSortField(); String sortOrder = postQueryRequest.getSortOrder(); // BoolQueryBuilder bool构造器 也就是对应以上的 查询条件中的bool 组合条件 BoolQueryBuilder boolQueryBuilder = QueryBuilders.boolQuery(); // 过滤 boolQueryBuilder.filter(QueryBuilders.termQuery(&quot;isDelete&quot;, 0)); if (id != null) &#123; boolQueryBuilder.filter(QueryBuilders.termQuery(&quot;id&quot;, id)); &#125; if (notId != null) &#123; boolQueryBuilder.mustNot(QueryBuilders.termQuery(&quot;id&quot;, notId)); &#125; if (userId != null) &#123; boolQueryBuilder.filter(QueryBuilders.termQuery(&quot;userId&quot;, userId)); &#125; // 必须包含所有标签 if (CollectionUtils.isNotEmpty(tagList)) &#123; for (String tag : tagList) &#123; boolQueryBuilder.filter(QueryBuilders.termQuery(&quot;tags&quot;, tag)); &#125; &#125; // 包含任何一个标签即可 if (CollectionUtils.isNotEmpty(orTagList)) &#123; BoolQueryBuilder orTagBoolQueryBuilder = QueryBuilders.boolQuery(); for (String tag : orTagList) &#123; orTagBoolQueryBuilder.should(QueryBuilders.termQuery(&quot;tags&quot;, tag)); // 精确查询 必须一模一样的 &#125; orTagBoolQueryBuilder.minimumShouldMatch(1); // 满足一个就可以 boolQueryBuilder.filter(orTagBoolQueryBuilder); &#125; // 按关键词检索 if (StringUtils.isNotBlank(searchText)) &#123; boolQueryBuilder.should(QueryBuilders.matchQuery(&quot;title&quot;, searchText));// matchQuery模糊查询 boolQueryBuilder.should(QueryBuilders.matchQuery(&quot;description&quot;, searchText));// matchQuery模糊查询 boolQueryBuilder.should(QueryBuilders.matchQuery(&quot;content&quot;, searchText));// matchQuery模糊查询 boolQueryBuilder.minimumShouldMatch(1);// 满足一个就可以 &#125; // 按标题检索 if (StringUtils.isNotBlank(title)) &#123; boolQueryBuilder.should(QueryBuilders.matchQuery(&quot;title&quot;, title)); // matchQuery模糊查询 boolQueryBuilder.minimumShouldMatch(1); &#125; // 按内容检索 if (StringUtils.isNotBlank(content)) &#123; boolQueryBuilder.should(QueryBuilders.matchQuery(&quot;content&quot;, content)); boolQueryBuilder.minimumShouldMatch(1); &#125; // 排序 SortBuilder&lt;?&gt; sortBuilder = SortBuilders.scoreSort(); if (StringUtils.isNotBlank(sortField)) &#123; sortBuilder = SortBuilders.fieldSort(sortField); sortBuilder.order(CommonConstant.SORT_ORDER_ASC.equals(sortOrder) ? SortOrder.ASC : SortOrder.DESC); &#125; // 分页 PageRequest pageRequest = PageRequest.of((int) current, (int) pageSize); // 构造查询 将以上的条件包装到NativeSearchQuery NativeSearchQuery searchQuery = new NativeSearchQueryBuilder().withQuery(boolQueryBuilder) .withPageable(pageRequest).withSorts(sortBuilder).build(); // elasticsearchRestTemplate查询 search SearchHits&lt;PostEsDTO&gt; searchHits = elasticsearchRestTemplate.search(searchQuery, PostEsDTO.class); Page&lt;Post&gt; page = new Page&lt;&gt;(); page.setTotal(searchHits.getTotalHits()); // 查询到了多少条 List&lt;Post&gt; resourceList = new ArrayList&lt;&gt;(); // 查出结果后，从 db 获取最新动态数据（比如点赞数） if (searchHits.hasSearchHits()) &#123; List&lt;SearchHit&lt;PostEsDTO&gt;&gt; searchHitList = searchHits.getSearchHits(); List&lt;Long&gt; postIdList = searchHitList.stream().map(searchHit -&gt; searchHit.getContent().getId()) .collect(Collectors.toList()); List&lt;Post&gt; postList = baseMapper.selectBatchIds(postIdList); if (postList != null) &#123; Map&lt;Long, List&lt;Post&gt;&gt; idPostMap = postList.stream().collect(Collectors.groupingBy(Post::getId)); postIdList.forEach(postId -&gt; &#123; if (idPostMap.containsKey(postId)) &#123; resourceList.add(idPostMap.get(postId).get(0)); &#125; else &#123; // 从 es 清空 db 已物理删除的数据 String delete = elasticsearchRestTemplate.delete(String.valueOf(postId), PostEsDTO.class); log.info(&quot;delete post &#123;&#125;&quot;, delete); &#125; &#125;); &#125; &#125; page.setRecords(resourceList); return page; &#125; 先模糊筛选静态数据，查出数据后，再根据查到的内容 id 去数据库查找到动态数据。 es案例 黑马旅游搜索 分页 查询请求的信息如下： 请求方式：POST 请求路径：&#x2F;hotel&#x2F;list 请求参数：JSON对象，包含4个字段： key：搜索关键字 page：页码 size：每页大小 sortBy：排序，目前暂不实现 返回值：分页查询，需要返回分页结果PageResult，包含两个属性： total：总条数 List：当前页的数据 因此，我们实现业务的流程如下： 步骤一：定义实体类，接收请求参数的JSON对象 步骤二：编写controller，接收页面的请求 步骤三：编写业务实现，利用ElasticsearchRestTemplate以及构造器实现搜索、分页1234567@Datapublic class RequestParams &#123; private String key; private Integer page; private Integer size; private String sortBy;&#125; 1234567891011121314151617181920212223242526272829303132333435@Servicepublic class HotelService extends ServiceImpl&lt;HotelMapper, Hotel&gt; implements IHotelService &#123; @Resource public ElasticsearchRestTemplate restTemplate; @Override public PageResult search(RequestParams params) &#123; // 查询 BoolQueryBuilder boolQuery = QueryBuilders.boolQuery(); // 这个key就是搜索的内容 if (params.getKey() == null || &quot;&quot;.equals(params.getKey())) &#123; // 如果说等于空 那就默认查询所有 boolQuery.must(QueryBuilders.matchAllQuery()); &#125;else &#123; // 如果有的话 那就把这个key作为 既定条件 boolQuery.must(QueryBuilders.matchQuery(&quot;all&quot;, params.getKey())); &#125; // 分页 // 默认是从0开始 int page = params.getPage() - 1; int size = params.getSize(); PageRequest pageRequest = PageRequest.of(page, size); // 进行查询请求 // 构建查询 NativeSearchQuery searchQueryBuilder = new NativeSearchQueryBuilder().withQuery(boolQuery) .withPageable(pageRequest).build(); SearchHits&lt;HotelDoc&gt; searchHits = restTemplate.search(searchQueryBuilder, HotelDoc.class); List&lt;HotelDoc&gt; hotels = new ArrayList&lt;&gt;(); // 获取结果集 加入hotels数组中 进行返回 searchHits.stream().forEach(hit -&gt; &#123; HotelDoc hotelDoc = hit.getContent(); hotels.add(hotelDoc); &#125;); return new PageResult(searchHits.getTotalHits(), hotels); &#125;&#125; 过滤在页面搜索框下面，会有一些过滤项： brand city等等包含的过滤条件有： brand：品牌值 city：城市 minPrice~maxPrice：价格范围 starName：星级 我们需要做两件事情： 修改请求参数的对象RequestParams，接收上述参数 修改业务逻辑，在搜索条件之外，添加一些过滤条件 修改在cn.itcast.hotel.pojo包下的实体类RequestParams： 12345678910111213@Datapublic class RequestParams &#123; private String key; private Integer page; private Integer size; private String sortBy; // 下面是新增的过滤条件参数 private String city; private String brand; private String starName; private Integer minPrice; private Integer maxPrice;&#125; 在之前的业务中，只有match查询，根据关键字搜索，现在要添加条件过滤，包括： 品牌过滤：是keyword类型，用term查询 星级过滤：是keyword类型，用term查询 价格过滤：是数值类型，用range查询 城市过滤：是keyword类型，用term查询 多个查询条件组合，肯定是boolean查询来组合： 关键字搜索放到must中，参与算分 其它过滤条件放到filter中，不参与算分123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051@Servicepublic class HotelService extends ServiceImpl&lt;HotelMapper, Hotel&gt; implements IHotelService &#123; @Resource public ElasticsearchRestTemplate restTemplate; @Override public PageResult search(RequestParams params) &#123; // 查询 BoolQueryBuilder boolQuery = QueryBuilders.boolQuery(); if (params.getKey() == null || &quot;&quot;.equals(params.getKey())) &#123; // 如果说等于空 那就默认查询所有 boolQuery.must(QueryBuilders.matchAllQuery()); &#125;else &#123; // 如果有的话 那就把这个东key作为 既定条件 boolQuery.must(QueryBuilders.matchQuery(&quot;all&quot;, params.getKey())); &#125; // 过滤查询 if (params.getBrand() != null &amp;&amp; !&quot;&quot;.equals(params.getBrand())) &#123; boolQuery.filter(QueryBuilders.termQuery(&quot;brand&quot;, params.getBrand())); &#125; if (params.getCity() != null &amp;&amp; !&quot;&quot;.equals(params.getCity())) &#123; boolQuery.filter(QueryBuilders.termQuery(&quot;city&quot;, params.getCity())); &#125; if (params.getStarName() != null &amp;&amp; !&quot;&quot;.equals(params.getStarName())) &#123; boolQuery.filter(QueryBuilders.termQuery(&quot;starName&quot;, params.getStarName())); &#125; if (params.getMinPrice() != null &amp;&amp; params.getMaxPrice() != null) &#123; boolQuery.filter(QueryBuilders .rangeQuery(&quot;price&quot;) .gte(params.getMinPrice()) // 大于 .lte(params.getMaxPrice()) // 小于 ); &#125; // 分页 // 默认是从0开始 int page = params.getPage() - 1; int size = params.getSize(); PageRequest pageRequest = PageRequest.of(page, size); // 进行查询请求 // 构建查询 NativeSearchQuery searchQueryBuilder = new NativeSearchQueryBuilder().withQuery(boolQuery) .withPageable(pageRequest).build(); SearchHits&lt;HotelDoc&gt; searchHits = restTemplate.search(searchQueryBuilder, HotelDoc.class); List&lt;HotelDoc&gt; hotels = new ArrayList&lt;&gt;(); // 获取结果集 加入hotels数组中 进行返回 searchHits.stream().forEach(hit -&gt; &#123; HotelDoc hotelDoc = hit.getContent(); hotels.add(hotelDoc); &#125;); return new PageResult(searchHits.getTotalHits(), hotels); &#125;&#125; 周边酒店 地理位置在酒店列表页的右侧，有一个小地图，点击地图的定位按钮，地图会找到你所在的位置：并且，在前端会发起查询请求，将你的坐标发送到服务端：我们要做的事情就是基于这个location坐标，然后按照距离对周围酒店排序。实现思路如下： 修改RequestParams参数，接收location字段 修改search方法业务逻辑，如果location有值，添加根据geo_distance排序的功能 我们以前学习过排序功能，包括两种： 普通字段排序 地理坐标排序 我们只讲了普通字段排序对应的java写法。地理坐标排序只学过DSL语法，如下： 123456789101112131415161718GET /indexName/_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;sort&quot;: [ &#123; &quot;price&quot;: &quot;asc&quot; &#125;, &#123; &quot;_geo_distance&quot; : &#123; &quot;FIELD&quot; : &quot;纬度，经度&quot;, &quot;order&quot; : &quot;asc&quot;, &quot;unit&quot; : &quot;km&quot; &#125; &#125; ]&#125; 实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758@Servicepublic class HotelService extends ServiceImpl&lt;HotelMapper, Hotel&gt; implements IHotelService &#123; @Resource public ElasticsearchRestTemplate restTemplate; @Override public PageResult search(RequestParams params) &#123; // 查询 BoolQueryBuilder boolQuery = QueryBuilders.boolQuery(); if (params.getKey() == null || &quot;&quot;.equals(params.getKey())) &#123; // 如果说等于空 那就默认查询所有 boolQuery.must(QueryBuilders.matchAllQuery()); &#125;else &#123; // 如果有的话 那就把这个东key作为 既定条件 boolQuery.must(QueryBuilders.matchQuery(&quot;all&quot;, params.getKey())); &#125; // 过滤查询 if (params.getBrand() != null &amp;&amp; !&quot;&quot;.equals(params.getBrand())) &#123; boolQuery.filter(QueryBuilders.termQuery(&quot;brand&quot;, params.getBrand())); &#125; if (params.getCity() != null &amp;&amp; !&quot;&quot;.equals(params.getCity())) &#123; boolQuery.filter(QueryBuilders.termQuery(&quot;city&quot;, params.getCity())); &#125; if (params.getStarName() != null &amp;&amp; !&quot;&quot;.equals(params.getStarName())) &#123; boolQuery.filter(QueryBuilders.termQuery(&quot;starName&quot;, params.getStarName())); &#125; if (params.getMinPrice() != null &amp;&amp; params.getMaxPrice() != null) &#123; boolQuery.filter(QueryBuilders .rangeQuery(&quot;price&quot;) .gte(params.getMinPrice()) // 大于 .lte(params.getMaxPrice()) // 小于 ); &#125; // 地理位置排序 SortBuilder location = null; if (params.getLocation() != null &amp;&amp; !&quot;&quot;.equals(params.getLocation())) &#123; location = SortBuilders.geoDistanceSort(&quot;location&quot;, new GeoPoint(params.getLocation())) .order(SortOrder.ASC) .unit(DistanceUnit.KILOMETERS); &#125; // 分页 // 默认是从0开始 int page = params.getPage() - 1; int size = params.getSize(); PageRequest pageRequest = PageRequest.of(page, size); // 进行查询请求 // 构建查询 NativeSearchQuery searchQueryBuilder = new NativeSearchQueryBuilder().withQuery(boolQuery) .withPageable(pageRequest).withSort(location).build(); SearchHits&lt;HotelDoc&gt; searchHits = restTemplate.search(searchQueryBuilder, HotelDoc.class); List&lt;HotelDoc&gt; hotels = new ArrayList&lt;&gt;(); // 获取结果集 加入hotels数组中 进行返回 searchHits.stream().forEach(hit -&gt; &#123; HotelDoc hotelDoc = hit.getContent(); hotels.add(hotelDoc); &#125;); return new PageResult(searchHits.getTotalHits(), hotels); &#125;&#125; 显示距离数值排序完成后，页面还要获取我附近每个酒店的具体距离值，这个值在响应结果中是独立的：因此，我们在结果解析阶段，除了解析source部分以外，还要得到sort部分，也就是排序的距离，然后放到响应结果中。我们要做两件事： 修改HotelDoc，添加排序距离字段，用于页面显示 修改HotelService类12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273@Servicepublic class HotelService extends ServiceImpl&lt;HotelMapper, Hotel&gt; implements IHotelService &#123; @Resource public ElasticsearchRestTemplate restTemplate; @Override public PageResult search(RequestParams params) &#123; // 查询 BoolQueryBuilder boolQuery = QueryBuilders.boolQuery(); if (params.getKey() == null || &quot;&quot;.equals(params.getKey())) &#123; // 如果说等于空 那就默认查询所有 boolQuery.must(QueryBuilders.matchAllQuery()); &#125;else &#123; // 如果有的话 那就把这个东key作为 既定条件 boolQuery.must(QueryBuilders.matchQuery(&quot;all&quot;, params.getKey())); &#125; // 过滤查询 if (params.getBrand() != null &amp;&amp; !&quot;&quot;.equals(params.getBrand())) &#123; boolQuery.filter(QueryBuilders.termQuery(&quot;brand&quot;, params.getBrand())); &#125; if (params.getCity() != null &amp;&amp; !&quot;&quot;.equals(params.getCity())) &#123; boolQuery.filter(QueryBuilders.termQuery(&quot;city&quot;, params.getCity())); &#125; if (params.getStarName() != null &amp;&amp; !&quot;&quot;.equals(params.getStarName())) &#123; boolQuery.filter(QueryBuilders.termQuery(&quot;starName&quot;, params.getStarName())); &#125; if (params.getMinPrice() != null &amp;&amp; params.getMaxPrice() != null) &#123; boolQuery.filter(QueryBuilders .rangeQuery(&quot;price&quot;) .gte(params.getMinPrice()) // 大于 .lte(params.getMaxPrice()) // 小于 ); &#125; // 地理位置排序 SortBuilder location = null; if (params.getLocation() != null &amp;&amp; !&quot;&quot;.equals(params.getLocation())) &#123; location = SortBuilders.geoDistanceSort(&quot;location&quot;, new GeoPoint(params.getLocation())) .order(SortOrder.ASC) .unit(DistanceUnit.KILOMETERS); &#125; // 分页 // 默认是从0开始 int page = params.getPage() - 1; int size = params.getSize(); PageRequest pageRequest = PageRequest.of(page, size); // 进行查询请求 // 构建查询 NativeSearchQuery searchQueryBuilder = null; if (location == null) &#123; searchQueryBuilder = new NativeSearchQueryBuilder().withQuery(boolQuery) .withPageable(pageRequest).build(); &#125;else &#123; searchQueryBuilder = new NativeSearchQueryBuilder().withQuery(boolQuery) .withPageable(pageRequest).withSort(location).build(); &#125; SearchHits&lt;HotelDoc&gt; searchHits = restTemplate.search(searchQueryBuilder, HotelDoc.class); List&lt;HotelDoc&gt; hotels = new ArrayList&lt;&gt;(); // 获取结果集 加入hotels数组中 进行返回 searchHits.stream().forEach(hit -&gt; &#123; HotelDoc hotelDoc = hit.getContent(); // es中有给出距离大小 因此只需要取出来获取第一个写入结果返回即可 if (hit.getSortValues().isEmpty()) &#123; hotels.add(hotelDoc); &#125;else &#123; hotelDoc.setDistance(hit.getSortValues().get(0)); hotels.add(hotelDoc); &#125; &#125;); return new PageResult(searchHits.getTotalHits(), hotels); &#125; &#125; 这里修复了bug（空指针的 这里的问题是如果没有传递地址位置 排序构造体就是空） 但是查询的时候最好直接就带上地理位置 酒店竞价排名需求：让指定的酒店在搜索结果中排名置顶 页面会给指定的酒店添加广告标记。那怎样才能让指定的酒店排名置顶呢？我们之前学习过的function_score查询可以影响算分，算分高了，自然排名也就高了。而function_score包含3个要素： 过滤条件：哪些文档要加分 算分函数：如何计算function score 加权方式：function score 与 query score如何运算 这里的需求是：让指定酒店排名靠前。因此我们需要给这些酒店添加一个标记，这样在过滤条件中就可以根据这个标记来判断，是否要提高算分。比如，我们给酒店添加一个字段：isAD，Boolean类型： true：是广告 false：不是广告 这样function_score包含3个要素就很好确定了： 过滤条件：判断isAD 是否为true 算分函数：我们可以用最简单暴力的weight，固定加权值 加权方式：可以用默认的相乘，大大提高算分 因此，业务的实现步骤包括： 给HotelDoc类添加isAD字段，Boolean类型 挑选几个你喜欢的酒店，给它的文档数据添加isAD字段，值为true 修改search方法，添加function score功能，给isAD值为true的酒店增加权重1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889@Servicepublic class HotelService extends ServiceImpl&lt;HotelMapper, Hotel&gt; implements IHotelService &#123; @Resource public ElasticsearchRestTemplate restTemplate; @Override public PageResult search(RequestParams params) &#123; // 查询 BoolQueryBuilder boolQuery = QueryBuilders.boolQuery(); if (params.getKey() == null || &quot;&quot;.equals(params.getKey())) &#123; // 如果说等于空 那就默认查询所有 boolQuery.must(QueryBuilders.matchAllQuery()); &#125;else &#123; // 如果有的话 那就把这个东key作为 既定条件 boolQuery.must(QueryBuilders.matchQuery(&quot;all&quot;, params.getKey())); &#125; // 过滤查询 if (params.getBrand() != null &amp;&amp; !&quot;&quot;.equals(params.getBrand())) &#123; boolQuery.filter(QueryBuilders.termQuery(&quot;brand&quot;, params.getBrand())); &#125; if (params.getCity() != null &amp;&amp; !&quot;&quot;.equals(params.getCity())) &#123; boolQuery.filter(QueryBuilders.termQuery(&quot;city&quot;, params.getCity())); &#125; if (params.getStarName() != null &amp;&amp; !&quot;&quot;.equals(params.getStarName())) &#123; boolQuery.filter(QueryBuilders.termQuery(&quot;starName&quot;, params.getStarName())); &#125; if (params.getMinPrice() != null &amp;&amp; params.getMaxPrice() != null) &#123; boolQuery.filter(QueryBuilders .rangeQuery(&quot;price&quot;) .gte(params.getMinPrice()) // 大于 .lte(params.getMaxPrice()) // 小于 ); &#125; // 地理位置排序 SortBuilder location = null; if (params.getLocation() != null &amp;&amp; !&quot;&quot;.equals(params.getLocation())) &#123; location = SortBuilders.geoDistanceSort(&quot;location&quot;, new GeoPoint(params.getLocation())) .order(SortOrder.ASC) .unit(DistanceUnit.KILOMETERS); &#125; // 2.算分控制 FunctionScoreQueryBuilder functionScoreQuery = QueryBuilders.functionScoreQuery( // 原始查询，相关性算分的查询 boolQuery, // function score的数组 new FunctionScoreQueryBuilder.FilterFunctionBuilder[]&#123; // 其中的一个function score 元素 new FunctionScoreQueryBuilder.FilterFunctionBuilder( // 过滤条件 QueryBuilders.termQuery(&quot;isAD&quot;, true), // 算分函数 权重最大 ScoreFunctionBuilders.weightFactorFunction(10) ) &#125;); // 分页 // 默认是从0开始 int page = params.getPage() - 1; int size = params.getSize(); PageRequest pageRequest = PageRequest.of(page, size); // 进行查询请求 // 构建查询 NativeSearchQuery searchQueryBuilder = null; if (location == null) &#123; searchQueryBuilder = new NativeSearchQueryBuilder().withQuery(boolQuery) .withPageable(pageRequest).withQuery(functionScoreQuery).build(); &#125;else &#123; searchQueryBuilder = new NativeSearchQueryBuilder().withQuery(boolQuery) .withPageable(pageRequest).withQuery(functionScoreQuery).withSort(location).build(); &#125; SearchHits&lt;HotelDoc&gt; searchHits = restTemplate.search(searchQueryBuilder, HotelDoc.class); List&lt;HotelDoc&gt; hotels = new ArrayList&lt;&gt;(); // 获取结果集 加入hotels数组中 进行返回 searchHits.stream().forEach(hit -&gt; &#123; HotelDoc hotelDoc = hit.getContent(); // es中有给出距离大小 因此只需要取出来获取第一个写入结果返回即可 if (hit.getSortValues().isEmpty()) &#123; hotels.add(hotelDoc); &#125;else &#123; hotelDoc.setDistance(hit.getSortValues().get(0)); hotels.add(hotelDoc); &#125; &#125;); return new PageResult(searchHits.getTotalHits(), hotels); &#125;&#125; 数据聚合聚合（aggregations）可以让我们极其方便的实现对数据的统计、分析、运算。例如： 什么品牌的手机最受欢迎？ 这些手机的平均价格、最高价格、最低价格？ 这些手机每月的销售情况如何？ 实现这些统计功能的比数据库的sql要方便的多，而且查询速度非常快，可以实现近实时搜索效果。 聚合的种类聚合常见的有三类： 桶（Bucket）聚合：用来对文档做分组 TermAggregation：按照文档字段值分组，例如按照品牌值分组、按照国家分组 Date Histogram：按照日期阶梯分组，例如一周为一组，或者一月为一组 度量（Metric）聚合：用以计算一些值，比如：最大值、最小值、平均值等 Avg：求平均值 Max：求最大值 Min：求最小值 Stats：同时求max、min、avg、sum等 管道（pipeline）聚合：其它聚合的结果为基础做聚合 注意：参加聚合的字段必须是keyword、日期、数值、布尔类型 DSL实现聚合现在，我们要统计所有数据中的酒店品牌有几种，其实就是按照品牌对数据分组。此时可以根据酒店品牌的名称做聚合，也就是Bucket聚合。 Bucket聚合语法语法如下： 123456789101112GET /hotel/_search&#123; &quot;size&quot;: 0, // 设置size为0，结果中不包含文档，只包含聚合结果 &quot;aggs&quot;: &#123; // 定义聚合 &quot;brandAgg&quot;: &#123; //给聚合起个名字 &quot;terms&quot;: &#123; // 聚合的类型，按照品牌值聚合，所以选择term &quot;field&quot;: &quot;brand&quot;, // 参与聚合的字段 &quot;size&quot;: 20 // 希望获取的聚合结果数量 &#125; &#125; &#125;&#125; 结果如图： 聚合结果排序默认情况下，Bucket聚合会统计Bucket内的文档数量，记为_count，并且按照_count降序排序。我们可以指定order属性，自定义聚合的排序方式： 123456789101112131415GET /hotel/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;brandAgg&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;brand&quot;, &quot;order&quot;: &#123; &quot;_count&quot;: &quot;asc&quot; // 按照_count升序排列 &#125;, &quot;size&quot;: 20 &#125; &#125; &#125;&#125; 限定聚合范围默认情况下，Bucket聚合是对索引库的所有文档做聚合，但真实场景下，用户会输入搜索条件，因此聚合必须是对搜索结果聚合。那么聚合必须添加限定条件。我们可以限定要聚合的文档范围，只要添加query条件即可： 12345678910111213141516171819GET /hotel/_search&#123; &quot;query&quot;: &#123; &quot;range&quot;: &#123; &quot;price&quot;: &#123; &quot;lte&quot;: 200 // 只对200元以下的文档聚合 &#125; &#125; &#125;, &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;brandAgg&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;brand&quot;, &quot;size&quot;: 20 &#125; &#125; &#125;&#125; 这次，聚合得到的品牌明显变少了： Metric聚合语法上节课，我们对酒店按照品牌分组，形成了一个个桶。现在我们需要对桶内的酒店做运算，获取每个品牌的用户评分的min、max、avg等值。这就要用到Metric聚合了，例如stat聚合：就可以获取min、max、avg等结果。语法如下： 12345678910111213141516171819GET /hotel/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;brandAgg&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;brand&quot;, &quot;size&quot;: 20 &#125;, &quot;aggs&quot;: &#123; // 是brands聚合的子聚合，也就是分组后对每组分别计算 &quot;score_stats&quot;: &#123; // 聚合名称 &quot;stats&quot;: &#123; // 聚合类型，这里stats可以计算min、max、avg等 &quot;field&quot;: &quot;score&quot; // 聚合字段，这里是score &#125; &#125; &#125; &#125; &#125;&#125; 这次的score_stats聚合是在brandAgg的聚合内部嵌套的子聚合。因为我们需要在每个桶分别计算。另外，我们还可以给聚合结果做个排序，例如按照每个桶的酒店平均分做排序： 小结aggs代表聚合，与query同级，此时query的作用是？ 限定聚合的的文档范围 聚合必须的三要素： 聚合名称 聚合类型 聚合字段 聚合可配置属性有： size：指定聚合结果数量 order：指定聚合结果排序方式 field：指定聚合字段 数据同步 定时任务，比如 1 分钟 1 次（鱼皮编程导航就用的是这种方法），找到 MySQL 中过去几分钟内（至少是定时周期的 2 倍）发生改变的数据，然后更新到 ES。 es会根据你传入的数据判断id 一样的就覆盖原来的而不是增加新的数据 优点：简单易懂、占用资源少、不用引入第三方中间件 缺点：有时间差 意味着无法实时同步 应用场景：数据短时间内不同步影响不大、或者数据几乎不发生修改 123456789101112131415161718192021222324252627282930313233343536373839404142434445// todo 取消注释开启任务@Component@Slf4jpublic class IncSyncPostToEs &#123; @Resource private PostMapper postMapper; @Resource private PostEsDao postEsDao; /** * 每分钟执行一次 */ @Scheduled(fixedRate = 60 * 1000) public void run() &#123; // 查询近 5 分钟内的数据 根据数据的更新时间 Date fiveMinutesAgoDate = new Date(new Date().getTime() - 5 * 60 * 1000L); // 当前时间减去5分钟 也就是查询过去五分钟 数据有没有变动 List&lt;Post&gt; postList = postMapper.listPostWithDelete(fiveMinutesAgoDate); /* &lt;select id=&quot;listPostWithDelete&quot; resultType=&quot;com.luhumu.search.model.entity.Post&quot;&gt; select * from post where updateTime &gt;= #&#123;minUpdateTime&#125; // 如果数据更新时间在此事件后 意味着变动了 &lt;/select&gt; */ if (CollectionUtils.isEmpty(postList)) &#123; log.info(&quot;no inc post&quot;); return; &#125; // 如果有数据更新了 只把更新的数据同步 而不是把所有的同步 List&lt;PostEsDTO&gt; postEsDTOList = postList.stream() .map(PostEsDTO::objToDto) .collect(Collectors.toList()); final int pageSize = 500; int total = postEsDTOList.size(); log.info(&quot;IncSyncPostToEs start, total &#123;&#125;&quot;, total); for (int i = 0; i &lt; total; i += pageSize) &#123; int end = Math.min(i + pageSize, total); log.info(&quot;sync from &#123;&#125; to &#123;&#125;&quot;, i, end); postEsDao.saveAll(postEsDTOList.subList(i, end)); &#125; log.info(&quot;IncSyncPostToEs end, total &#123;&#125;&quot;, total); &#125;&#125; 双写（危险的操作 有可能出现更新失败）：写数据的时候，必须也去写 ES；更新删除数据库同理。（事务：建议先保证 MySQL 写成功，如果 ES 写失败了，可以通过定时任务 + 日志 + 告警进行检测和修复（补偿）） 用 Logstash（性能有点差 不适合大规模） 数据同步管道（一般要配合 kafka 消息队列 + beats 采集器） 不怎么用但要会 订阅数据库流水的同步方式 Canal 这种完全同步 只要数据库数据变了es就改变 小总结：方式一：同步调用（双写） 优点：实现简单，粗暴 缺点：业务耦合度高 方式二：异步通知（mq 通知） 优点：低耦合，实现难度一般 缺点：依赖mq的可靠性 方式三：监听binlog 优点：完全解除服务间耦合 缺点：开启binlog增加数据库负担、实现复杂度高 关于mq实现： 当发送增删改操作时候 mq监听 进行异步更改 es集群&#x2F;&#x2F; todo Sentinel雪崩问题redis雪崩：在同一时段大量的缓存key同时失效或者Redis服务宕机（OMG），导致大量请求到达数据库，带来巨大压力。 分布式雪崩问题：微服务中，服务间调用关系错综复杂，一个微服务往往依赖于多个其它微服务。如图，如果服务提供者I发生了故障，当前的应用的部分业务因为依赖于服务I，因此也会被阻塞。此时，其它不依赖于服务I的业务似乎不受影响。但是，依赖服务I的业务请求被阻塞，用户不会得到响应，则tomcat的这个线程不会释放，于是越来越多的用户请求到来，越来越多的线程会阻塞服务器支持的线程和并发数有限，请求一直阻塞，会导致服务器资源耗尽，从而导致所有其它服务都不可用，那么当前服务也就不可用了。那么，依赖于当前服务的其它服务随着时间的推移，最终也都会变的不可用，形成级联失败，雪崩就发生了一个服务挂了 导致一连串服务都挂了 线程中不断堆积请求 ** 解决雪崩问题的常见方式有四种：方案一：超时处理：设定超时时间，请求超过一定时间没有响应就返回错误信息，不会无休止等待方案2：仓壁模式仓壁模式来源于船舱的设计：船舱都会被隔板分离为多个独立空间，当船体破损时，只会导致部分空间进入，将故障控制在一定范围内，避免整个船体都被淹没。于此类似，我们可以限定每个业务能使用的线程数，避免耗尽整个tomcat的资源，因此也叫线程隔离。方案3：断路器模式**断路器模式：由断路器统计业务执行的异常比例，如果超出阈值则会熔断该业务，拦截访问该业务的一切请求。断路器会统计访问某个服务的请求数量，异常比例超过设置的阈值以后 进行熔断当发现访问服务D的请求异常比例过高时，认为服务D有导致雪崩的风险，会拦截访问服务D的一切请求，形成熔断：方案四：流量控制流量控制：限制业务访问的QPS（每秒钟请求的数量），避免服务因流量的突增而故障。实现方式Sentinel 总结：什么是雪崩问题？ 微服务之间相互调用，因为调用链中的一个服务故障，引起整个链路都无法访问的情况。 可以认为：限流是对服务的保护，避免因瞬间高并发流量而导致服务故障，进而避免雪崩。是一种预防措施。超时处理、线程隔离、降级熔断是在部分服务故障时，将故障控制在一定范围，避免雪崩。是一种补救措施。 服务保护技术对比在SpringCloud当中支持多种服务保护技术： Netfix Hystrix Sentinel Resilience4J 早期比较流行的是Hystrix框架，但目前国内实用最广泛的还是阿里巴巴的Sentinel框架，这里我们做下对比： Sentinel Hystrix 隔离策略 信号量隔离 线程池隔离&#x2F;信号量隔离 熔断降级策略 基于慢调用比例或异常比例 基于失败比率 实时指标实现 滑动窗口 滑动窗口（基于 RxJava） 规则配置 支持多种数据源 支持多种数据源 扩展性 多个扩展点 插件的形式 基于注解的支持 支持 支持 限流 基于 QPS，支持基于调用关系的限流 有限的支持 流量整形 支持慢启动、匀速排队模式 不支持 系统自适应保护 支持 不支持 控制台 开箱即用，可配置规则、查看秒级监控、机器发现等 不完善 常见框架的适配 Servlet、Spring Cloud、Dubbo、gRPC 等 Servlet、Spring Cloud Netflix 初识SentinelSentinel是阿里巴巴开源的一款微服务流量控制组件。官网地址：https://sentinelguard.io/zh-cn/index.htmlSentinel 具有以下特征:•丰富的应用场景：Sentinel 承接了阿里巴巴近 10 年的双十一大促流量的核心场景，例如秒杀（即突发流量控制在系统容量可以承受的范围）、消息削峰填谷、集群流量控制、实时熔断下游不可用应用等。•完备的实时监控：Sentinel 同时提供实时的监控功能。您可以在控制台中看到接入应用的单台机器秒级数据，甚至 500 台以下规模的集群的汇总运行情况。•广泛的开源生态：Sentinel 提供开箱即用的与其它开源框架&#x2F;库的整合模块，例如与 Spring Cloud、Dubbo、gRPC 的整合。您只需要引入相应的依赖并进行简单的配置即可快速地接入 Sentinel。•完善的SPI扩展点：Sentinel 提供简单易用、完善的 SPI 扩展接口。您可以通过实现扩展接口来快速地定制逻辑。例如定制规则管理、适配动态数据源等。 安装Sentinel1）下载sentinel官方提供了UI控制台，方便我们对系统做限流设置。可以在GitHub下载。2）运行将jar包放到任意非中文目录，执行命令：java -jar sentinel-dashboard-1.8.1.jar如果要修改Sentinel的默认端口、账户、密码，可以通过下列配置： 配置项 默认值 说明 server.port 8080 服务端口 sentinel.dashboard.auth.username sentinel 默认用户名 sentinel.dashboard.auth.password sentinel 默认密码 例如，修改端口：java -Dserver.port=8090 -jar sentinel-dashboard-1.8.1.jar3）访问访问http://localhost:8080页面，就可以看到sentinel的控制台了：需要输入账号和密码，默认都是：sentinel登录后，发现一片空白，什么都没有：这是因为我们还没有与微服务整合。 微服务整合Sentinel我们在order-service中整合sentinel，并连接sentinel的控制台，步骤如下：1）引入sentinel依赖 12345&lt;!--sentinel--&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt;&lt;/dependency&gt; 2）配置控制台修改application.yaml文件，添加下面内容： 1234567server: port: 8088spring: cloud: sentinel: transport: dashboard: localhost:8080 3）访问order-service的任意端点打开浏览器，访问http://localhost:8088/order/101，这样才能触发sentinel的监控。然后再访问sentinel的控制台，查看效果： 流量控制雪崩问题虽然有四种方案，但是限流是避免服务因突发的流量而发生故障，是对微服务雪崩问题的预防。我们先学习这种模式。限流：控制qps（每秒的访问量） 来预防雪崩问题 簇点链路当请求进入微服务时，首先会访问DispatcherServlet，然后进入Controller、Service、Mapper，这样的一个调用链就叫做簇点链路。簇点链路中被监控的每一个接口就是一个资源。默认情况下sentinel会监控SpringMVC的每一个端点（Endpoint，也就是controller中的方法），因此SpringMVC的每一个端点（Endpoint）就是调用链路中的一个资源。例如，我们刚才访问的order-service中的OrderController中的端点：&#x2F;order&#x2F;{orderId}流控、熔断等都是针对簇点链路中的资源来设置的，因此我们可以点击对应资源后面的按钮来设置规则： 流控：流量控制 降级：降级熔断 热点：热点参数限流，是限流的一种 授权：请求的权限控制 快速入门2.1.1.示例点击资源&#x2F;order&#x2F;{orderId}后面的流控按钮，就可以弹出表单。表单中可以填写限流规则其含义是限制 &#x2F;order&#x2F;{orderId}这个资源的单机QPS为1，即每秒只允许1次请求，超出的请求会被拦截并报错。 流控模式在添加限流规则时，点击高级选项，可以选择三种流控模式： 直接：统计当前资源的请求，触发阈值时对当前资源直接限流，也是默认的模式 关联：统计与当前资源相关的另一个资源，触发阈值时，对当前资源限流 链路：统计从指定链路访问到本资源的请求，触发阈值时，对指定链路限流 快速入门测试的就是直接模式。 关联模式关联模式：统计与当前资源相关的另一个资源，触发阈值时，对当前资源限流配置规则：语法说明：当&#x2F;write资源访问量触发阈值时，就会对&#x2F;read资源限流，避免影响&#x2F;write资源。使用场景：比如用户支付时需要修改订单状态，同时用户要查询订单。查询和修改操作会争抢数据库锁，产生竞争。业务需求是优先支付和更新订单的业务，因此当修改订单业务触发阈值时，需要对查询订单业务限流。需求说明： 在OrderController新建两个端点：&#x2F;order&#x2F;query和&#x2F;order&#x2F;update，无需实现业务 配置流控规则，当&#x2F;order&#x2F; update资源被访问的QPS超过5时，对&#x2F;order&#x2F;query请求限流 1）定义&#x2F;order&#x2F;query端点，模拟订单查询 1234@GetMapping(&quot;/query&quot;)public String queryOrder() &#123; return &quot;查询订单成功&quot;;&#125; 2）定义&#x2F;order&#x2F;update端点，模拟订单更新 1234@GetMapping(&quot;/update&quot;)public String updateOrder() &#123; return &quot;更新订单成功&quot;;&#125; 重启服务，查看sentinel控制台的簇点链路：3）配置流控规则对哪个端点限流，就点击哪个端点后面的按钮。我们是对订单查询&#x2F;order&#x2F;query限流，因此点击它后面的按钮：在表单中填写流控规则：4）在Jmeter测试选择《流控模式-关联》：可以看到1000个用户，100秒，因此QPS为10，超过了我们设定的阈值：5查看http请求：请求的目标是&#x2F;order&#x2F;update，这样这个断点就会触发阈值。但限流的目标是&#x2F;order&#x2F;query，我们在浏览器访问，可以发现：确实被限流了。5）总结 链路模式链路模式：只针对从指定链路访问到本资源的请求做统计，判断是否超过阈值。配置示例：例如有两条请求链路： &#x2F;test1 –&gt; &#x2F;common &#x2F;test2 –&gt; &#x2F;common 如果只希望统计从&#x2F;test2进入到&#x2F;common的请求，则可以这样配置：实战案例需求：有查询订单和创建订单业务，两者都需要查询商品。针对从查询订单进入到查询商品的请求统计，并设置限流。步骤： 在OrderService中添加一个queryGoods方法，不用实现业务 在OrderController中，改造&#x2F;order&#x2F;query端点，调用OrderService中的queryGoods方法 在OrderController中添加一个&#x2F;order&#x2F;save的端点，调用OrderService的queryGoods方法 给queryGoods设置限流规则，从&#x2F;order&#x2F;query进入queryGoods的方法限制QPS必须小于2 实现： 1）添加查询商品方法在order-service服务中，给OrderService类添加一个queryGoods方法： 123public void queryGoods()&#123; System.err.println(&quot;查询商品&quot;);&#125; 2）查询订单时，查询商品在order-service的OrderController中，修改&#x2F;order&#x2F;query端点的业务逻辑： 12345678@GetMapping(&quot;/query&quot;)public String queryOrder() &#123; // 查询商品 orderService.queryGoods(); // 查询订单 System.out.println(&quot;查询订单&quot;); return &quot;查询订单成功&quot;;&#125; 3）新增订单，查询商品在order-service的OrderController中，修改&#x2F;order&#x2F;save端点，模拟新增订单： 12345678@GetMapping(&quot;/save&quot;)public String saveOrder() &#123; // 查询商品 orderService.queryGoods(); // 查询订单 System.err.println(&quot;新增订单&quot;); return &quot;新增订单成功&quot;;&#125; 4）给查询商品添加资源标记默认情况下，OrderService中的方法是不被Sentinel监控的，需要我们自己通过注解来标记要监控的方法。给OrderService的queryGoods方法添加@SentinelResource注解： 1234@SentinelResource(&quot;goods&quot;)public void queryGoods()&#123; System.err.println(&quot;查询商品&quot;);&#125; 链路模式中，是对不同来源的两个链路做监控。但是sentinel默认会给进入SpringMVC的所有请求设置同一个root资源，会导致链路模式失效。我们需要关闭这种对SpringMVC的资源聚合，修改order-service服务的application.yml文件： 1234spring: cloud: sentinel: web-context-unify: false # 关闭context整合 重启服务，访问&#x2F;order&#x2F;query和&#x2F;order&#x2F;save，可以查看到sentinel的簇点链路规则中，出现了新的资源： 5）添加流控规则点击goods资源后面的流控按钮，在弹出的表单中填写下面信息：只统计从&#x2F;order&#x2F;query进入&#x2F;goods的资源，QPS阈值为2，超出则被限流。 6）Jmeter测试选择《流控模式-链路》：可以看到这里200个用户，50秒内发完，QPS为4，超过了我们设定的阈值2一个http请求是访问&#x2F;order&#x2F;save：运行的结果：完全不受影响。另一个是访问&#x2F;order&#x2F;query：运行结果：每次只有2个通过。 2.2.3.总结流控模式有哪些？•直接：对当前资源限流•关联：高优先级资源触发阈值，对低优先级资源限流。•链路：阈值统计时，只统计从指定资源进入当前资源的请求，是对请求来源的限流 流控效果在流控的高级选项中，还有一个流控效果选项：流控效果是指请求达到流控阈值时应该采取的措施，包括三种： 快速失败：达到阈值后，新的请求会被立即拒绝并抛出FlowException异常。是默认的处理方式。 warm up：预热模式，对超出阈值的请求同样是拒绝并抛出异常。但这种模式阈值会动态变化，从一个较小值逐渐增加到最大阈值。 排队等待：让所有的请求按照先后次序排队执行，两个请求的间隔不能小于指定时长 2.3.1.warm up阈值一般是一个微服务能承担的最大QPS，但是一个服务刚刚启动时，一切资源尚未初始化（冷启动），如果直接将QPS跑到最大值，可能导致服务瞬间宕机。warm up也叫预热模式，是应对服务冷启动的一种方案。请求阈值初始值是 maxThreshold &#x2F; coldFactor，持续指定时长后，逐渐提高到maxThreshold值。而coldFactor的默认值是3.例如，我设置QPS的maxThreshold为10，预热时间为5秒，那么初始阈值就是 10 &#x2F; 3 ，也就是3，然后在5秒后逐渐增长到10.案例需求：给&#x2F;order&#x2F;{orderId}这个资源设置限流，最大QPS为10，利用warm up效果，预热时长为5秒 1）配置流控规则：2）Jmeter测试选择《流控效果，warm up》：QPS为10.刚刚启动时，大部分请求失败，成功的只有3个，说明QPS被限定在3：随着时间推移，成功比例越来越高：到Sentinel控制台查看实时监控：一段时间后： 2.3.2.排队等待当请求超过QPS阈值时，快速失败和warm up 会拒绝新的请求并抛出异常。而排队等待则是让所有请求进入一个队列中，然后按照阈值允许的时间间隔依次执行。后来的请求必须等待前面执行完成，如果请求预期的等待时间超出最大时长，则会被拒绝。工作原理例如：QPS &#x3D; 5，意味着每200ms处理一个队列中的请求；timeout &#x3D; 2000，意味着预期等待时长超过2000ms的请求会被拒绝并抛出异常。那什么叫做预期等待时长呢？比如现在一下子来了12 个请求，因为每200ms执行一个请求，那么： 第6个请求的预期等待时长 &#x3D; 200 * （6 - 1） &#x3D; 1000ms 第12个请求的预期等待时长 &#x3D; 200 * （12-1） &#x3D; 2200ms 现在，第1秒同时接收到10个请求，但第2秒只有1个请求，此时QPS的曲线这样的：如果使用队列模式做流控，所有进入的请求都要排队，以固定的200ms的间隔执行，QPS会变的很平滑：平滑的QPS曲线，对于服务器来说是更友好的。案例需求：给&#x2F;order&#x2F;{orderId}这个资源设置限流，最大QPS为10，利用排队的流控效果，超时时长设置为5s 1）添加流控规则2）Jmeter测试选择《流控效果，队列》：QPS为15，已经超过了我们设定的10。如果是之前的 快速失败、warmup模式，超出的请求应该会直接报错。但是我们看看队列模式的运行结果：全部都通过了。再去sentinel查看实时监控的QPS曲线：QPS非常平滑，一致保持在10，但是超出的请求没有被拒绝，而是放入队列。因此响应时间（等待时间）会越来越长。当队列满了以后，才会有部分请求失败： 2.3.3.总结流控效果有哪些？ 快速失败：QPS超过阈值时，拒绝新的请求 warm up： QPS超过阈值时，拒绝新的请求；QPS阈值是逐渐提升的，可以避免冷启动时高并发导致服务宕机。 排队等待：请求会进入队列，按照阈值允许的时间间隔依次执行请求；如果请求预期等待时长大于超时时间，直接拒绝 热点参数限流之前的限流是统计访问某个资源的所有请求，判断是否超过QPS阈值。而热点参数限流是分别统计参数值相同的请求，判断是否超过QPS阈值。 全局参数限流例如，一个根据id查询商品的接口：访问&#x2F;goods&#x2F;{id}的请求中，id参数值会有变化，热点参数限流会根据参数值分别统计QPS，统计结果：当id&#x3D;1的请求触发阈值被限流时，id值不为1的请求不受影响。配置示例：代表的含义是：对hot这个资源的0号参数（第一个参数）做统计，每1秒相同参数值的请求数不能超过5 热点参数限流刚才的配置中，对查询商品这个接口的所有商品一视同仁，QPS都限定为5.而在实际开发中，可能部分商品是热点商品，例如秒杀商品，我们希望这部分商品的QPS限制与其它商品不一样，高一些。那就需要配置热点参数限流的高级选项了：结合上一个配置，这里的含义是对0号的long类型参数限流，每1秒相同参数的QPS不能超过5，有两个例外：•如果参数值是100，则每1秒允许的QPS为10•如果参数值是101，则每1秒允许的QPS为15 案例案例需求：给&#x2F;order&#x2F;{orderId}这个资源添加热点参数限流，规则如下：•默认的热点参数规则是每1秒请求量不超过2•给102这个参数设置例外：每1秒请求量不超过4•给103这个参数设置例外：每1秒请求量不超过10注意事项：热点参数限流对默认的SpringMVC资源无效，需要利用@SentinelResource注解标记资源 1）标记资源给order-service中的OrderController中的&#x2F;order&#x2F;{orderId}资源添加注解： 2）热点参数限流规则访问该接口，可以看到我们标记的hot资源出现了：这里不要点击hot后面的按钮，页面有BUG点击左侧菜单中热点规则菜单点击新增，填写表单 隔离和降级限流是一种预防措施，虽然限流可以尽量避免因高并发而引起的服务故障，但服务还会因为其它原因而故障。而要将这些故障控制在一定范围，避免雪崩，就要靠线程隔离（舱壁模式）和熔断降级手段了。线程隔离之前讲到过：调用者在调用服务提供者时，给每个调用的请求分配独立线程池，出现故障时，最多消耗这个线程池内资源，避免把调用者的所有资源耗尽。熔断降级：是在调用方这边加入断路器，统计对服务提供者的调用，如果调用的失败比例过高，则熔断该业务，不允许访问该服务的提供者了。可以看到，不管是线程隔离还是熔断降级，都是对客户端（调用方）的保护。需要在调用方 发起远程调用时做线程隔离、或者服务熔断。而我们的微服务远程调用都是基于Feign来完成的，因此我们需要将Feign与Sentinel整合，在Feign里面实现线程隔离和服务熔断。 FeignClient整合SentinelSpringCloud中，微服务调用都是通过Feign来实现的，因此做客户端保护必须整合Feign和Sentinel。 3.1.1.修改配置，开启sentinel功能修改OrderService的application.yml文件，开启Feign的Sentinel功能： 123feign: sentinel: enabled: true # 开启feign对sentinel的支持 3.1.2.编写失败降级逻辑业务失败后，不能直接报错，而应该返回用户一个友好提示或者默认结果，这个就是失败降级逻辑。给FeignClient编写失败后的降级逻辑①方式一：FallbackClass，无法对远程调用的异常做处理②方式二：FallbackFactory，可以对远程调用的异常做处理，我们选择这种这里我们演示方式二的失败降级处理。步骤一：在feign-api项目中定义类，实现FallbackFactory：代码： 1234567891011121314151617181920package cn.itcast.feign.clients.fallback;import cn.itcast.feign.clients.UserClient;import cn.itcast.feign.pojo.User;import feign.hystrix.FallbackFactory;import lombok.extern.slf4j.Slf4j;@Slf4jpublic class UserClientFallbackFactory implements FallbackFactory&lt;UserClient&gt; &#123; @Override public UserClient create(Throwable throwable) &#123; return new UserClient() &#123; @Override public User findById(Long id) &#123; log.error(&quot;查询用户异常&quot;, throwable); return new User(); &#125; &#125;; &#125;&#125; 步骤二：在feign-api项目中的DefaultFeignConfiguration类中将UserClientFallbackFactory注册为一个Bean： 1234@Beanpublic UserClientFallbackFactory userClientFallbackFactory()&#123; return new UserClientFallbackFactory();&#125; 步骤三：在feign-api项目中的UserClient接口中使用UserClientFallbackFactory： 123456789101112import cn.itcast.feign.clients.fallback.UserClientFallbackFactory;import cn.itcast.feign.pojo.User;import org.springframework.cloud.openfeign.FeignClient;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PathVariable;@FeignClient(value = &quot;userservice&quot;, fallbackFactory = UserClientFallbackFactory.class)public interface UserClient &#123; @GetMapping(&quot;/user/&#123;id&#125;&quot;) User findById(@PathVariable(&quot;id&quot;) Long id);&#125; 重启后，访问一次订单查询业务，然后查看sentinel控制台，可以看到新的簇点链路： 3.1.3.总结Sentinel支持的雪崩解决方案： 线程隔离（仓壁模式） 降级熔断 Feign整合Sentinel的步骤： 在application.yml中配置：feign.sentienl.enable&#x3D;true 给FeignClient编写FallbackFactory并注册为Bean 将FallbackFactory配置到FeignClient 说白了 这里采用降级方案 就是在失败次数过多的时候进行熔断服务 让他不能被访问 直接返回错误 线程隔离的实现方式线程隔离有两种方式实现： 线程池隔离 信号量隔离（Sentinel默认采用） 如图：线程池隔离：给每个服务调用业务分配一个线程池，利用线程池本身实现隔离效果信号量隔离：不创建线程池，而是计数器模式，记录业务使用的线程数量，达到信号量上限时，禁止新的请求。 sentinel的线程隔离用法说明：在添加限流规则时，可以选择两种阈值类型： QPS：就是每秒的请求数，在快速入门中已经演示过 线程数：是该资源能使用用的tomcat线程数的最大值。也就是通过限制线程数量，实现线程隔离（舱壁模式）。 案例需求：给 order-service服务中的UserClient的查询用户接口设置流控规则，线程数不能超过 2。然后利用jemeter测试。 1）配置隔离规则选择feign接口后面的流控按钮：填写表单： 2）Jmeter测试选择《阈值类型-线程数&lt;2》：一次发生10个请求，有较大概率并发线程数超过2，而超出的请求会走之前定义的失败降级逻辑。查看运行结果：发现虽然结果都是通过了，不过部分请求得到的响应是降级返回的null信息。 总结熔断降级线程隔离的两种手段是？ 信号量隔离 线程池隔离 信号量隔离的特点是？ 基于计数器模式，简单，开销小 线程池隔离的特点是？ 基于线程池模式，有额外开销，但隔离控制更强 熔断降级是解决雪崩问题的重要手段。其思路是由断路器统计服务调用的异常比例、慢请求比例，如果超出阈值则会熔断该服务。即拦截访问该服务的一切请求；而当服务恢复时，断路器会放行访问该服务的请求。断路器控制熔断和放行是通过状态机来完成的：状态机包括三个状态： closed：关闭状态，断路器放行所有请求，并开始统计异常比例、慢请求比例。超过阈值则切换到open状态 open：打开状态，服务调用被熔断，访问被熔断服务的请求会被拒绝，快速失败，直接走降级逻辑。Open状态5秒后会进入half-open状态 half-open：半开状态，放行一次请求，根据执行结果来判断接下来的操作。 请求成功：则切换到closed状态 请求失败：则切换到open状态 断路器熔断策略有三种：慢调用、异常比例、异常数 &#x2F;&#x2F; todo 这里需要看看具体另一个文档中的具体操作 其实就是可视化界面的设置 &#x2F;&#x2F; todo 遗留了一点 心情有点沉不下来 即使看下去也是囫囵吞枣 开始做项目先","categories":[{"name":"后端","slug":"后端","permalink":"http://example.com/categories/%E5%90%8E%E7%AB%AF/"}],"tags":[]},{"title":"MQ","slug":"消息队列","date":"2023-07-31T09:28:26.350Z","updated":"2023-07-31T09:29:12.327Z","comments":true,"path":"2023/07/31/消息队列/","link":"","permalink":"http://example.com/2023/07/31/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/","excerpt":"","text":"RabbitMQ一切mq的入门 具体技术不是用它 但是一切mq的思想都是他RabbitMQ（rabbit—兔子 MQ—消息队列）是一个广泛使用的消息服务器，采用Erlang语言编写，是一种开源的实现 AMQP（高级消息队列协议）的消息中间件；RabbitMQ最初起源于金融系统，它的性能及稳定性都非常出色；AMQP协议（http://www.amqp.org），即 Advanced Message Queuing Protocol，高级消息队列协议，是应用层协议的一个开放标准，为面向消息的中间件设计；我们学的协议有哪些：（http、ftp）他和redis一样都是中间件相关网址官网：https://www.rabbitmq.comGithub：https://github.com/rabbitmq 消息中间件（MQ&#x3D;Message Queue）简单来说，消息中间件就是指保存数据的一个容器（服务器），可以用于两个系统之间的数据传递。消息中间件一般有三个主要角色：生产者、消费者、消息代理(消息队列、消息服务器)；生产者发送消息到消息服务器，然后消费者从消息代理（消息队列）中获取数据并进行处理； broker 在这里就是rabbitmq producer 生产者（发送消息） consumer 消费者（接收消息） 常用的消息中间件目前比较主流的几个消息中间件： RabbitMQ kafka（大数据领域） RocketMQ（阿里巴巴开源）献给Apache组织 pulsar（最近一两年流行起来的） MQ（Message Queue）的应用场景异步处理**下订单业务：下订单–》加积分–》发红包–》发手机短信 **下订单—向MQ 发消息–》积分系统，红包系统，手机短信系统接收消息以上业务不使用MQ 就是串行的、同步的处理（同步就会出现阻塞） 如果在这个业务中 出现一处阻塞 后面的务必需要等待 而且这些业务加起来的执行时间过长（参考redis中的 使用消息队列优化秒杀 其实就是 用户下单了是否成功直接返回结果 具体数据库业务存放在消息队列中 开一个异步线程来一个个处理） 同步是阻塞的（会造成等待），异步是非阻塞的（不会等待）；大流量高并发请求、批量数据传递，就可以采用异步处理，提升系统吞吐量； 使用MQ可以异步处理 系统解耦多个系统之间，不需要直接交互，通过消息进行业务流转；以前是 如果系统之间有依赖关系 就需要相互去调用 这样会使得耦合度很高 如果使用mq使得系统之间解耦另外 这些系统可能是java c go等等 也就是说 mq使得不同语言写出来的系统相互交互 流量削峰高负载请求&#x2F;任务的缓冲处理：假设双十一期间： 请求量很大 这个时候使用mq 将批量的请求放在mq中 在根据数据库能接受的数据量一个个执行 这样就不会使得请求量过大 DB压力过大了 日志处理主要是用kafka这个服务器来做；日志处理是指将消息队列用于在日志处理中，比如Kafka解决大量日志传输的问题；loger.info(…)ELK 日志处理解决方案：loger.error(…) –&gt;logstash收集消息–&gt; 发送消息的kafka –&gt; elastic search (es) –&gt;Kibana ELK日志处理平台 RabbitMQ运行环境搭建Erlang及RabbitMQ安装版本的选择下载时一定要注意版本兼容性版本兼容说明地址：https://www.rabbitmq.com/which-erlang.html使用RabbitMQ 先要下载 Erlang 就像是下载jdk一样我们使用3.10.11的mq版本 就需要使用限定范围内的 Erlang 具体下载操作erlang压缩包安装 创建目录 mkdir 文件名 安装包导入 解压前先安装必要命令yum -y install make gcc gcc-c++ kernel-devel m4 ncurses-devel openssl-devel 解压erlang tar -zxvf otp_src_25.1.1.tar.gz -C /opt/apps/rabbitMQ/mq/ 进入到解压完成的目录中 运行相应命令 cd otp_src_25.1.1 .&#x2F;configure 编译 make将源代码转为可编译文件 执行 make install使得文件可以在任何地方都能够使用 安装好了erlang后可以将解压的文件夹删除 rm -rf otp_src_25.1.1 安装rabbitmq 导入压缩包 创建目录 同上 解压 tar -xvf rabbitmq-server-generic-unix-3.10.11.tar.xz -C /opt/apps/rabbitMQ/mq/ 说明 -C 选项是指定解压目录，如果不指定会解压到当前目录 此时rabbitmq就安装好了； 启动及停止RabbitMQ切换到安装目录的sbin目录下: #启动 .&#x2F;rabbitmq-server -detached 说明：-detached 将表示在后台启动运行rabbitmq；不加该参数表示前台启动；rabbitmq的运行日志存放在安装目录的var目录下；现在的目录是：..&#x2F;rabbitmq_server-3.10.11&#x2F;var&#x2F;log&#x2F;rabbitmqps -ef|grep rabbit查看当前端口是否启动 停止mq状态: **./rabbitmqctl shutdown** 查看RabbitMQ的状态切换到安装目录的sbin目录： .&#x2F;rabbitmqctl -n rabbit status 说明：-n rabbit 是指定节点名称为rabbit，目前只有一个节点，节点名默认为rabbit此处-n rabbit 也可以省略 配置path环境变量使用启动命令在任何目录下都可以运行vim /etc/profile 修改配置文件 到最后一行 按o&#x2F;i键 进入输入状态 123RABBIT_HOME=/opt/apps/rabbitMQ/mq/rabbitmq_server-3.10.11PATH=$PATH:$RABBIT_HOME/sbinexport RABBIT_HOME PATH 输入以后 先按esc键 在按shift和： 输入wq 保存退出刷新环境变量 source /etc/profile在任何时候都可以使用rabbitmq-server -detached 命令启动 RabbitMQ管理命令.&#x2F;rabbitmqctl 是一个管理命令，可以管理rabbitmq的很多操作。.&#x2F;rabbitmqctl help可以查看一下有哪些操作查看具体子命令 可以使用 .&#x2F;rabbitmqctl help 子命令名称 用户管理用户管理包括增加用户，删除用户，查看用户列表，修改用户密码。这些操作都是通过rabbitmqctl管理命令来实现完成。查看帮助： rabbitmqctl add_user –help 相应的命令(1) 查看当前用户列表 rabbitmqctl list_users (2) 新增一个用户 语法：rabbitmqctl add_user Username Password示例: rabbitmqctl add_user admin 123456 设置用户角色 rabbitmqctl set_user_tags User Tag示例：rabbitmqctl set_user_tags admin administrator 说明：此处设置用户的角色为管理员角色 设置用户权限 *rabbitmqctl set_permissions -p &#x2F; admin “._” “._” “.” 说明：此操作是设置admin用户拥有操作虚拟主机&#x2F;下的所有权限查看用户权限 .&#x2F;rabbitmqctl list_permissions web管理后台Rabbitmq有一个web管理后台，这个管理后台是以插件的方式提供的，启动后台web管理功能，切换到sbin目录下执行： 启用管理后台查看rabbitmq 的插件列表 123456# 列出插件rabbitmq-plugins list#启用rabbitmq-plugins enable rabbitmq_management#禁用rabbitmq-plugins disable rabbitmq_management 防火墙操作 systemctl status firewalld –检查防火墙状态 systemctl stop firewalld–关闭防火墙，Linux重启之后会失效 systemctl disable firewalld –防火墙置为不可用，Linux重启后，防火墙服务不自动启动，依然是不可用 访问http://192.168.131.131:15672用户名&#x2F;密码为我们上面创建的admin&#x2F;123456 注意上面改成你的虚拟主机的ip地址 备注：如果使用默认用户guest、密码guest登录，会提示User can only log in via localhost说明guest用户只能从localhost本机登录，所以不要使用该用户。 通过web页面新建虚拟主机virtual 虚拟的虚拟主机就相当于sql中数据库的概念 消息来了要存放 存放在哪里？ 就是这个虚拟主机里面 RabbitMQ工作模型 ***这个图片 核心 broker 相当于mysql服务器，virtual host相当于数据库（可以有多个数据库）queue相当于表，消息相当于记录。 以上的比喻很精妙 生产者就是写入数据库（虚拟主机）的具体表（队列）的人 接收者就是从 表（队列）中获取数据的人生产者 并不是和服务器建立连接就发送 他是通过信道（TCP连接） 因为创建连接非常的耗时 因此创建一个信道 其中有很多小通道供连接 消息队列有三个核心要素： 消息生产者、消息队列、消息消费者；生产者（Producer）：发送消息的应用，将数据发送到某一台虚拟主机的交换机中 这个交换机把消息转到队列中；（java程序，也可能是别的语言写的程序）消费者（Consumer）：接收消息的应用；对于消费者来说 他不需要知道交换机的存在 只需要知道队列 和队列建立连接 tcp（java程序，也可能是别的语言写的程序）代理（Broker）：就是消息服务器，RabbitMQ Server就是Message Broker；连接（Connection）：连接RabbitMQ服务器的TCP长连接；信道（Channel）：连接中的一个虚拟通道，消息队列发送或者接收消息时，都是通过信道进行的；虚拟主机（Virtual host）：一个虚拟分组，在代码中就是一个字符串，当多个不同的用户使用同一个RabbitMQ服务时，可以划分出多个Virtual host，每个用户在自己的Virtual host创建exchange&#x2F;queue等；（分类比较清晰、相互隔离）交换机（Exchange）：交换机负责从生产者接收消息，并根据交换机类型分发到对应的消息队列中，起到一个路由的作用；路由键（Routing Key）：交换机根据路由键来决定消息分发到哪个队列，路由键是消息的目的地址；绑定（Binding）：绑定是队列和交换机的一个关联连接（关联关系）；队列（Queue）：存储消息的缓存；消息（Message）：由生产者通过RabbitMQ发送给消费者的信息；（消息可以任何数据，字符串、user对象，json串等等） RabbitMQ交换机类型 ***生产者把消息发送到交换机中 交换器去发送到队列Exchange（X） 可翻译成交换机&#x2F;交换器&#x2F;路由器 交换机类型 Fanout Exchange（扇形） Direct Exchange（直连） Topic Exchange（主题） Headers Exchange（头部） Fanout Exchangefanout 扇形的 散开的投递到所有绑定的队列，不需要路由键，不需要进行路由键的匹配，相当于广播、群发；扇形交换工作流程就是 交换机收到消息 投递给所有和此交换机绑定的队列不需要绑定key 和 执行key 代码实现 需要生产者 生产数据 需要一台交换机 和 两个队列来模式 消费者接收数据 创建springboot项目 导入依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;&lt;/dependency&gt; 配置连接信息 123456789spring: application: name: producer rabbitmq: port: 5672 # mq端口号 host: 192.168.70.132 username: admin password: 123456 virtual-host: luhumu # 虚拟主机名称 理解为数据库名字 不写的话默认是/ 编写rabbitmq 三部曲 12345678910111213141516171819202122232425262728293031323334/** * 配置类 在这里创建 转换机和 队列 以及他们的连接方式 */@Configurationpublic class RabbitFanoutConfig &#123; // rabbitmq 三部曲 // 定义交换机 @Bean public FanoutExchange fanoutExchange() &#123; // 创建一个交换机bean对象 生产者通过交换机转换对象 return new FanoutExchange(&quot;exchange.fanout&quot;); &#125; // 定义队列 // 这里我们需要两个队列来模拟 扇形交换 @Bean public Queue queueA() &#123; // 创建一个队列 指定他的名字 队列相当于数据库的表 return new Queue(&quot;queue.fanout.a&quot;); &#125; @Bean public Queue queueB() &#123; // 创建一个队列 指定他的名字 队列相当于数据库的表 return new Queue(&quot;queue.fanout.b&quot;); &#125; // 绑定交换机和队列 一个交换机绑定了两个 因此写两个绑定 @Bean public Binding bindingA(FanoutExchange fanoutExchange, Queue queueA) &#123; return BindingBuilder.bind(queueA).to(fanoutExchange); &#125; @Bean public Binding bindingB(FanoutExchange fanoutExchange, Queue queueB) &#123; return BindingBuilder.bind(queueB).to(fanoutExchange); &#125;&#125; 发送执行 12345678910111213141516171819202122232425262728/** * 实现了CommandLineRunner 其中有run方法 springboot一开始执行就会运行这里的run */// 开启注解 开启单次执行@Component@Slf4jpublic class RabbitFanoutOnce implements CommandLineRunner &#123; @Resource private RabbitTemplate rabbitTemplate; @Override public void run(String... args) throws Exception &#123; // 生产者 发送消息到队列 sendMsg(); &#125; /** * 发送消息 借助message对象 */ public void sendMsg() &#123; // 具体消息内容 String msg = &quot;hello rabbitmq&quot;; // 转换为 消息对象 Message message = new Message(msg.getBytes()); // 将消息放松到该交换机 我们写了交换机的绑定 因此会转发到每一个队列 // convertAndSend(&quot;转换机的名字&quot;, &quot;路由key&quot;, 发送的消息) rabbitTemplate.convertAndSend(&quot;exchange.fanout&quot;, &quot;&quot;, message); log.info(&quot;发送时间:&#123;&#125;&quot;, new Date()); &#125;&#125; 消费者 12345678910111213141516@Component@Slf4jpublic class ReceiveFanoutMessage &#123; /** * @RabbitListener 可以接收多个队列信息 queues属性代表接收的队列名 自动运行 * 他是个监听器 会一直监听队列 有信息就会直接接收 非常快 * @param message 接收到的队列信息会放在message中 */ @RabbitListener(queues = &#123;&quot;queue.fanout.a&quot;,&quot;queue.fanout.b&quot;&#125;) public void receiveFanoutMessage(Message message) &#123; // 我们传的就是字节 取也要传字节 byte[] body = message.getBody(); String msg = new String(body); log.info(&quot;信息:&#123;&#125;&quot;, msg); &#125;&#125; 注意消费者 很快接收到 队列中的信息被接受以后 就没了 Direct Exchangedirect 直接的根据路由键精确匹配（一模一样）进行路由消息队列；直连交换 以上指定了路由key 如果交换机按照error的key转换 两个队列都会接收到 而按照info匹配只会找到一个队列 直连交换就是按照指定的路由key 来给信息 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * 配置类 在这里创建 转换机和 队列 以及他们的连接方式 */@Configuration@Data // 一定要写 不然配置文件信息无法注入@ConfigurationProperties(prefix = &quot;my&quot;)public class RabbitDirectConfig &#123; // 和配置属性中的名字一样 就能够通过set方法注入 private String exchangeName; private String queueAName; private String queueBName; // rabbitmq 三部曲 // 定义交换机 @Bean public DirectExchange fanoutExchange() &#123; // 创建一个交换机bean对象 生产者通过交换机转换对象 return ExchangeBuilder.directExchange(exchangeName).build(); &#125; // 定义队列 // 这里我们需要两个队列来模拟 扇形交换 @Bean public Queue queueA() &#123; // 创建一个队列 指定他的名字 队列相当于数据库的表 return QueueBuilder.durable(queueAName).build(); &#125; @Bean public Queue queueB() &#123; // 创建一个队列 指定他的名字 队列相当于数据库的表 return QueueBuilder.durable(queueBName).build(); &#125; // 绑定 @Bean public Binding bindingA(DirectExchange directExchange, Queue queueA) &#123; // 因为是直连交换 因此需要执行key 这个key就是这个交换机和queue的连接类型 return BindingBuilder.bind(queueA).to(directExchange).with(&quot;error&quot;); &#125; // 指定交换机和队列的直连关系 @Bean public Binding bindingB1(DirectExchange directExchange, Queue queueB) &#123; return BindingBuilder.bind(queueB).to(directExchange).with(&quot;info&quot;); &#125; @Bean public Binding bindingB2(DirectExchange directExchange, Queue queueB) &#123; return BindingBuilder.bind(queueB).to(directExchange).with(&quot;error&quot;); &#125; @Bean public Binding bindingB3(DirectExchange directExchange, Queue queueB) &#123; return BindingBuilder.bind(queueB).to(directExchange).with(&quot;warning&quot;); &#125;&#125; 123456789101112131415161718192021222324252627/** * 实现了CommandLineRunner 其中有run方法 springboot一开始执行就会运行这里的run */// 开启注解 开启单次执行@Component@Slf4jpublic class RabbitDirectOnce implements CommandLineRunner &#123; @Resource private RabbitTemplate rabbitTemplate; @Override public void run(String... args) throws Exception &#123; // 生产者 发送消息到队列 sendMsg(); &#125; /** * 发送消息 借助message对象 */ public void sendMsg() &#123; // 使用 建造者模式创建消息 建造者模式：多个简单对象转换为负责对象 Message message = MessageBuilder.withBody(&quot;hello direct&quot;.getBytes()).build(); // convertAndSend(&quot;转换机的名字&quot;, &quot;路由key&quot;, 发送的消息) // 此时我们使用的是 直连模式 需要指定这个路由发送key 指定消息到绑定key一致的队列 rabbitTemplate.convertAndSend(&quot;exchange.direct&quot;, &quot;warning&quot;, message); log.info(&quot;发送时间:&#123;&#125;&quot;, new Date()); // 发送到了指定的队列 消费者 监听的就会可以从对应队列中获取 &#125;&#125; Topic Exchange通配符匹配，相当于模糊匹配；#匹配多个单词，用来表示任意数量（零个或多个）单词匹配一个单词（必须有一个，而且只有一个），用.隔开的为一个单词：beijing.# &#x3D;&#x3D; beijing.queue.abc, beijing.queue.xyz.xxxbeijing. &#x3D;&#x3D; beijing.queue, beijing.xyz发送时指定的路由键：lazy.orange.rabbit很明显路由key和队列的两个绑定key都一样 因此两个队列都能收到 Q2虽然有两个绑定key 但是输入只会进入一条 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * 配置类 在这里创建 转换机和 队列 以及他们的连接方式 */@Configuration@Data // 一定要写 不然配置文件信息无法注入@ConfigurationProperties(prefix = &quot;my&quot;)public class RabbiTopicConfig &#123; // 和配置属性中的名字一样 就能够通过set方法注入 private String exchangeName; private String queueAName; private String queueBName; // rabbitmq 三部曲 // 定义交换机 @Bean public TopicExchange fanoutExchange() &#123; // 创建一个交换机bean对象 生产者通过交换机转换对象 return ExchangeBuilder.topicExchange(exchangeName).build(); &#125; // 定义队列 @Bean public Queue queueA() &#123; // 创建一个队列 指定他的名字 队列相当于数据库的表 return QueueBuilder.durable(queueAName).build(); &#125; @Bean public Queue queueB() &#123; // 创建一个队列 指定他的名字 队列相当于数据库的表 return QueueBuilder.durable(queueBName).build(); &#125; // 绑定 在这里需要绑定 三个交换机 @Bean public Binding bindingA(TopicExchange topicExchange, Queue queueA) &#123; // 因为是直连交换 因此需要执行key 这个key就是这个交换机和queue的连接类型 return BindingBuilder.bind(queueA).to(topicExchange).with(&quot;*.orange.*&quot;); &#125; // 因为此队列有两种不同的绑定key 因此建立两个连接 @Bean public Binding bindingB1(TopicExchange topicExchange, Queue queueB) &#123; return BindingBuilder.bind(queueB).to(topicExchange).with(&quot;*.*.rabbit&quot;); &#125; @Bean public Binding bindingB2(TopicExchange topicExchange, Queue queueB) &#123; return BindingBuilder.bind(queueB).to(topicExchange).with(&quot;lazy.#&quot;); &#125;&#125; 123456789101112131415161718192021222324252627/** * 实现了CommandLineRunner 其中有run方法 springboot一开始执行就会运行这里的run */// 开启注解 开启单次执行@Component@Slf4jpublic class RabbitTopicOnce implements CommandLineRunner &#123; @Resource private RabbitTemplate rabbitTemplate; @Override public void run(String... args) throws Exception &#123; // 生产者 发送消息到队列 sendMsg(); &#125; /** * 发送消息 借助message对象 */ public void sendMsg() &#123; // 使用 建造者模式创建消息 建造者模式：多个简单对象转换为负责对象 Message message = MessageBuilder.withBody(&quot;hello topic&quot;.getBytes()).build(); // convertAndSend(&quot;交换机的名字&quot;, &quot;路由key&quot;, 发送的消息) // 此时我们使用的是 直连模式 需要指定这个路由发送key 指定消息到绑定key一致的队列 rabbitTemplate.convertAndSend(&quot;exchange.topic&quot;, &quot;x.orange.x&quot;, message); log.info(&quot;发送时间:&#123;&#125;&quot;, new Date()); // 发送到了指定的队列 消费者 监听的就会可以从对应队列中获取 &#125;&#125; Headers Exchange （用的比较少 基本不用）基于消息内容中的headers属性进行匹配；绑定参考代码： 12345Map&lt;String, Object&gt; headerValues = new HashMap&lt;&gt;();headerValues.put(&quot;type&quot;, &quot;m&quot;);headerValues.put(&quot;status&quot;, 1);return BindingBuilder._bind_(queueA).to(headersExchange).whereAll(headerValues).match(); 发送参考代码 1234567MessageProperties messageProperties = new MessageProperties(); messageProperties.setHeader(&quot;type&quot;, &quot;m&quot;); messageProperties.setHeader(&quot;status&quot;, 1); Message message = new Message(msg.getBytes(), messageProperties);// void convertAndSend(String exchange, String routingKey, Object message) throws AmqpException; amqpTemplate.convertAndSend(RabbitConfig._EXCHANGE_, null, message); 学习它的目的是：发消息时可以指定消息属性（MessageProperties） RabbitMQ过期消息过期消息也叫TTL消息，TTL：Time To Live消息的过期时间有两种设置方式：（过期消息） 设置单条消息的过期时间12345678 // 消息属性类 在这里配置过期时间 MessageProperties messageProperties = new MessageProperties(); // 单位: 毫秒messageProperties.setExpiration(&quot;30000&quot;);Message message = MessageBuilder.withBody(&quot;hello topic&quot;.getBytes()).andProperties(messageProperties).build();// convertAndSend(&quot;交换机的名字&quot;, &quot;路由key&quot;, 发送的消息)rabbitTemplate.convertAndSend(&quot;exchange.topic&quot;, &quot;x.orange.x&quot;, message);log.info(&quot;发送时间:&#123;&#125;&quot;, new Date()); 通过队列属性设置消息过期时间123456789@Beanpublic Queue directQueue() &#123; // 使用map来存储队列的过期时间 Map&lt;String, Object&gt; arguments = new HashMap&lt;&gt;(); // 加入这个队列的过期时间参数固定写法 队列,存活时间 arguments.put(&quot;x-message-ttl&quot;, 10000); return new Queue(_DIRECT_QUEUE_, true, false, false, arguments);&#125; 队列的过期时间决定了在没有任何消费者的情况下，队列中的消息可以存活多久；注意事项：如果消息和对列都设置过期时间，则消息的TTL以两者之间较小的那个数值为准。 RabbitMQ死信队列***也有叫 死信交换机、死信邮箱等说法；DLX: Dead-Letter-Exchange 死信交换器，死信邮箱在原本队列中的数据过期了（消息变为死信） 就会进入死信交换机 在到死信队列中 他的作用就是 在一定时间段以后对消息进行处理延迟队列就是在这个基础上做的 队列过期代码写法：具体写法 在正常的消息队列中指定过期消息 死信交换机 路由key 过期以后携带路由key发送到指定的交换机 交换机根据路由key发送到指定key的队列 12345my: exchangeNormalName: exchange.normal.a # 正常交换机 queueNormalName: queue.normal.a # 正常队列 exchangeDlxName: exchange.dlx.a # 死信交换机 queueDlxName: queue.dlx.a # 死信队列 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/** * 死信交换机 */@Configuration@Data // 一定要写 不然配置文件信息无法注入@ConfigurationProperties(prefix = &quot;my&quot;)public class RabbiDlxConfig &#123; // 和配置属性中的名字一样 就能够通过set方法注入 private String exchangeNormalName; private String queueNormalName; private String exchangeDlxName; private String queueDlxName; // rabbitmq 三部曲 // 定义 正常交换机 @Bean public DirectExchange normalExchange() &#123; // 创建一个交换机bean对象 生产者通过交换机转换对象 return ExchangeBuilder.directExchange(exchangeNormalName).build(); &#125; ---------------------------------------------- // 定义正常队列 @Bean public Queue normalQueue() &#123; // 创建一个队列 指定他的名字 队列相当于数据库的表 // 重要： 正常队列过期以后 消息给死信交换机 *** HashMap&lt;String, Object&gt; arguments = new HashMap&lt;&gt;(); arguments.put(&quot;x-message-ttl&quot;, 20000); // 设置过期时间 arguments.put(&quot;x-dead-letter-exchange&quot;, exchangeDlxName); // 设置指定的死信交换器 arguments.put(&quot;x-dead-letter-routing-key&quot;, &quot;error&quot;); // 设置死信的路由key 要和死信队列和死信交换机的绑定key一样 // return QueueBuilder.durable(queueNormalName).withArguments(arguments).build(); return QueueBuilder.durable(queueNormalName) .expires(20000).deadLetterExchange(exchangeDlxName).deadLetterRoutingKey(&quot;error&quot;) .build(); &#125;----------------------------------------------------------- // 绑定 @Bean public Binding bindingNormal(DirectExchange normalExchange, Queue normalQueue) &#123; // 因为是直连交换 因此需要执行key 这个key就是这个交换机和queue的连接类型 return BindingBuilder.bind(normalQueue).to(normalExchange).with(&quot;order&quot;); &#125; // 创建死信 他的创建写法一样 // 定义死信交换机 @Bean public DirectExchange dxlExchange() &#123; // 创建一个交换机bean对象 生产者通过交换机转换对象 return ExchangeBuilder.directExchange(exchangeDlxName).build(); &#125; // 定义 死信队列 @Bean public Queue dlxQueue() &#123; // 创建一个队列 指定他的名字 队列相当于数据库的表 return QueueBuilder.durable(queueDlxName).build(); &#125; // 值得一说的是 这里的依赖注入 参数直接就是以上的bean对象 类型名字一样 因此可以直接注入 spring容器 自动取出对应对象 注入 @Bean public Binding bindingDlx(DirectExchange dxlExchange, Queue dlxQueue) &#123; return BindingBuilder.bind(dlxQueue).to(dxlExchange).with(&quot;error&quot;); &#125;&#125; 12345678910111213141516171819202122// 开启注解 开启单次执行@Component@Slf4jpublic class RabbitDlxOnce implements CommandLineRunner &#123; @Resource private RabbitTemplate rabbitTemplate; @Override public void run(String... args) throws Exception &#123; // 生产者 发送消息到队列 sendMsg(); &#125; /** * 发送消息 借助message对象 */ public void sendMsg() &#123; Message message = MessageBuilder.withBody(&quot;hello topic&quot;.getBytes()).build(); // convertAndSend(&quot;交换机的名字&quot;, &quot;路由key&quot;, 发送的消息) // 生产者向此交换机发送消息 指定了路由key 交换机会根据路由key发送到所有绑定key一致的队列 rabbitTemplate.convertAndSend(&quot;exchange.normal.a&quot;, &quot;order&quot;, message); log.info(&quot;发送时间:&#123;&#125;&quot;, new Date()); &#125;&#125; 最主要代码： 为正常队列指定一个私信交换器 1234567891011121314 // 定义正常队列 @Bean public Queue normalQueue() &#123; // 创建一个队列 指定他的名字 队列相当于数据库的表 // 重要： 正常队列过期以后 消息给死信交换机 *** HashMap&lt;String, Object&gt; arguments = new HashMap&lt;&gt;(); arguments.put(&quot;x-message-ttl&quot;, 20000); // 设置过期时间 arguments.put(&quot;x-dead-letter-exchange&quot;, exchangeDlxName); // 设置指定的死信交换器 arguments.put(&quot;x-dead-letter-routing-key&quot;, &quot;error&quot;); // 设置死信的路由key 要和死信队列和死信交换机的绑定key一样 // return QueueBuilder.durable(queueNormalName).withArguments(arguments).build(); return QueueBuilder.durable(queueNormalName) .expires(20000).deadLetterExchange(exchangeDlxName).deadLetterRoutingKey(&quot;error&quot;) .build(); &#125; 消息过期123456//设置此条消息的过期时间为30秒MessageProperties messageProperties=new MessageProperties();// 单位: 毫秒 messageProperties.setExpiration(&quot;30000&quot;);Message message = MessageBuilder.withBody(&quot;hello topic&quot;.getBytes()).andProperties(messageProperties).build(); 将原本的队列过期转为消息过期 队列达到最大长度（先入队的消息会被发送到DLX）1234Map&lt;String, Object&gt; arguments = new HashMap&lt;String, Object&gt;();//设置队列的最大长度 ，对头的消息会被挤出变成死信arguments.put(&quot;x-max-length&quot;, 5); 12345678return QueueBuilder.durable(queueNormalName) .expires(20000).deadLetterExchange(exchangeDlxName).deadLetterRoutingKey(&quot;error&quot;) .maxLength(5) .build();// 直接使用 maxLength方法 以下是源码方式 指定队列的最大长度public QueueBuilder maxLength(int count) &#123; return this.withArgument(&quot;x-max-length&quot;, count); &#125; 原本队列中有5条数据 此时加入一条 就会超过最大队列长度 那么先进先出 所以队头的会到死信队列中 消费者拒绝消息不进行重新投递以上的正常队列 没有设置消费者 它的作用就是数据过期发送到dlx死信交换机此时修改从正常的队列接收消息，但是对消息不进行确认，并且不对消息进行重新投递，此时消息就进入死信队列。（用户收到了消息 但是手动拒绝 并且让消息不重新进入队列，那么他就会进入死信队列，注意这里我们有两个选择 一个是让消息重新入队，一个是进入死信队列） 这里是可以 批量处理的一般消费者直接接收队列数据 就会告诉队列接收到了 这个时候队列就会删除这个数据 但是这里有一个问题如果直接接收了消息 就通知队列删除 接着就需要把消息放到数据库 但是如果数据库宕机了 消息没插入进去 就会出现丢失数据的问题解决：使用消费者的手动确认 等我数据库插入成功了 我在通知队列删除 而不是一接收直接删除 手动确认： 更改配置文件 转为手动确认 123456789101112spring: application: name: producer rabbitmq: port: 5672 # mq??? host: 192.168.70.132 username: admin password: 123456 virtual-host: luhumu listener: simple: acknowledge-mode: manual # 开启消费者的手动确认 修改消费者接收代码 12345678910111213141516171819202122232425262728293031323334353637383940@Component@Slf4jpublic class ReceiveAutoMessage &#123; /** * @RabbitListener 可以接收多个队列信息 queues属性代表接收的队列名 自动运行 * 他是个监听器 会一直监听队列 有信息就会直接接收 非常快 * @param message 接收到的队列信息会放在message中 * @param channel 信道 */ @RabbitListener(queues = &#123;&quot;queue.normal.4&quot;&#125;) public void receiveMessage(Message message, Channel channel) &#123; // 获取消息属性 MessageProperties messageProperties = message.getMessageProperties(); // 相当于这条消息的唯一id long deliveryTag = messageProperties.getDeliveryTag(); String msg = null; try &#123; // 我们传的就是字节 取也要传字节 byte[] body = message.getBody(); msg = new String(body); log.info(&quot;接收到了消息:&#123;&#125;&quot;, msg); // basicAck() 消费者手动确认， // params: 1----消息的唯一id, false----说明只确认当前消息 如果是true是批量确认 也就是会把这条消息前面的全部确认 channel.basicAck(deliveryTag, false); &#125; catch (Exception e) &#123; // 发生异常 告诉服务器 不要删除队列信息 队列中的消息会重新入队 // basicNack() 消费者手动不确认消息 // params： 1---消息唯一id， 2-false----说明只确认当前消息重新入队 true---都确认 // 3--true 是否重新入队 --- false就是不重新入队 也就是进入死信队列 try &#123; channel.basicNack(deliveryTag, false, true); &#125; catch (IOException ex) &#123; throw new RuntimeException(ex); &#125; throw new RuntimeException(e); &#125; log.info(&quot;信息:&#123;&#125;&quot;, msg); &#125;&#125; 此时就转为了手动确认 如果我们需要消费者不接受（数据库宕机了） 将信息转到死信队列只需要修改参数 1234567891011// 发生异常 告诉服务器 不要删除队列信息 队列中的消息会重新入队// basicNack() 消费者手动不确认消息// params： 1---消息唯一id， 2-false----说明只确认当前消息重新入队 3--true 是否重新入队 --- false就是不重新入队 也就是进入死信队列try &#123; // 重新入队 // channel.basicNack(deliveryTag, false, true); // 进入死信队列 不对消息进行重新投递（不重复入队） channel.basicNack(deliveryTag, false, false);&#125; catch (IOException ex) &#123; throw new RuntimeException(ex);&#125; 注意：如果没有配置死信队列选择了 false 这条消息就会被删除 消费者拒绝消息开启手动确认模式，并拒绝消息，不重新投递，则进入死信队列和以上的区别是 上面可以选择批量处理 这里只能处理单条消息 1234567891011121314151617181920212223242526 /** * 监听正常的那个队列的名字，不是监听那个死信队列 * 我们从正常的队列接收消息，但是对消息不进行确认，并且不对消息进行重新投递，此时消息就进入死信队列 * * channel 消息信道（是连接下的一个消息信道，一个连接下有多个消息信息，发消息/接消息都是通过信道完成的） */ @RabbitListener(queues = &#123;RabbitConfig.QUEUE&#125;) public void process(Message message, Channel channel) &#123; System.out.println(&quot;接收到的消息：&quot; + message); //对消息不确认, ack单词是 确认 的意思 // void basicNack(long deliveryTag, boolean multiple, boolean requeue) // deliveryTag：消息的一个数字标签 唯一id // multiple：翻译成中文是多个的意思，如果是true表示对小于deliveryTag标签下的消息都进行Nack不确认，false表示只对当前deliveryTag标签的消息Nack // requeue：如果是true表示消息被Nack（not ack）后，重新发送到队列，如果是false，消息被Nack后，不会重新发送到队列 进入死信队列 try &#123; System.out.println(&quot;deliveryTag = &quot; + message.getMessageProperties().getDeliveryTag()); //要开启rabbitm消息消费的手动确认模式，然后才这么写代码； // 这里和上面的区别就是没有批量处理的选项 // 1--- 数字标签 2--- false死信队列 true重新入队 channel.basicReject(message.getMessageProperties().getDeliveryTag(), false); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 注意：如果没有配置死信队列选择了 false 这条消息就会被删除 RabbitMQ延迟队列延迟消息使用的场景有很多，比如： 在订单系统中，一个用户下单之后通常有30分钟的时间进行支付，如果30分钟之内没有支付成功，那么这个订单将进行异常处理，这时候就可以使用延迟队列来处理这些订单了。用户希望通过手机远程遥控家里的智能设备在指定的时间进行工作，这时候就可以将用户指令发送到延迟队列，当指令设定的时间到了再将指令推送到只能设备。 延迟队列存储的对象是对应的延迟消息，所谓的延迟消息是指当消息被发送以后，并不想让消费者立刻拿到消息，而是等待特定时间后，消费者才能拿到这个消息进行消费。 以上场景中 就是 消息发送了 30分钟未处理 这样消费者才能获取这条消息取消订单 定时任务方式 pass每隔3秒扫描一次数据库，查询过期的订单然后进行处理；优点：简单，容易实现；缺点：1、存在延迟（延迟时间不准确），如果你每隔1分钟扫一次，那么就有可能延迟1分钟；2、性能较差，每次扫描数据库，如果订单量很大 被动取消当用户查询订单的时候，判断订单是否超时，超时了就取消（交易关闭）；优点：对服务器而言，压力小；缺点：1、用户不查询订单，将永远处于待支付状态，会对数据统计等功能造成影响；2、用户打开订单页面，有可能比较慢，因为要处理大量订单，用户体验少稍差； JDK延迟队列（单体应用，不能分布式下） passDelayedQueue无界阻塞队列，该队列只有在延迟期满的时候才能从中获取元素优点：实现简单，任务延迟低；缺点：服务重启、宕机，数据丢失；只适合单机版，不适合集群；订单量大，可能内存不足而发生异常； oom—out of memory 内存溢出 采用消息中间件（rabbitmq） ***RabbitMQ本身不支持延迟队列，可以使用TTL结合DLX的方式来实现消息的延迟投递，即把DLX跟某个队列绑定，到了指定时间，消息过期后，就会从DLX路由到这个队列，消费者可以从这个队列取走消息。以此达到延迟队列使用一个交换机（Topic exchange方式） 首先进入正常队列 当ttl到期以后 通过交换机转到死信队列关键就是上图中 交换机一致 通过ttl和dlx实现 1234567891011121314151617181920212223242526272829303132333435363738394041@Configuration@Data // 一定要写 不然配置文件信息无法注入@ConfigurationProperties(prefix = &quot;my&quot;)public class RabbiTtlDlxConfig &#123; // 和配置属性中的名字一样 就能够通过set方法注入 private String exchangeNormalName; private String queueNormalName; private String queueDlxName; // rabbitmq 三部曲 // 定义 正常交换机 @Bean public DirectExchange exchange() &#123; // 创建一个交换机bean对象 生产者通过交换机转换对象 return ExchangeBuilder.directExchange(exchangeNormalName).build(); &#125; // 定义正常队列 @Bean public Queue normalQueue() &#123; // 这里很关键 这个队列的交换机和死信队列的交换机是一样的 return QueueBuilder.durable(queueNormalName) .expires(20000).deadLetterExchange(exchangeNormalName).deadLetterRoutingKey(&quot;error&quot;) .build(); &#125; // 绑定 @Bean public Binding bindingNormal(DirectExchange exchange, Queue normalQueue) &#123; // 因为是直连交换 因此需要执行key 这个key就是这个交换机和queue的连接类型 return BindingBuilder.bind(normalQueue).to(exchange).with(&quot;pay&quot;); &#125; // 定义 死信队列 @Bean public Queue dlxQueue() &#123; // 创建一个队列 指定他的名字 队列相当于数据库的表 return QueueBuilder.durable(queueDlxName).build(); &#125; // 值得一说的是 这里的依赖注入 参数直接就是以上的bean对象 类型名字一样 因此可以直接注入 spring容器 自动取出对应对象 注入 @Bean public Binding bindingDlx(DirectExchange exchange, Queue dlxQueue) &#123; return BindingBuilder.bind(dlxQueue).to(exchange).with(&quot;error&quot;); &#125;&#125; 问题？ 如果先发送的消息，消息延迟时间长，会影响后面的 延迟时间段的消息的消费；解决：不同延迟时间的消息要发到不同的队列上，同一个队列的消息，它的延迟时间应该一样因为是队列 先进先出 如果对头的过期时间比较长 后面的短 对头没出去 后面到期了也出不去解决方法： 将不同过期消息的消息放在不同的队列 比如5s过期的消息放在5s过期的队列中 10s的放在10s过期的队列中 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455@Configuration@Data // 一定要写 不然配置文件信息无法注入@ConfigurationProperties(prefix = &quot;my&quot;)public class RabbiTtlDlxConfig &#123; // 和配置属性中的名字一样 就能够通过set方法注入 private String exchangeNormalName; private String queueNormalName; private String queueDlxName; // rabbitmq 三部曲 // 定义 正常交换机 @Bean public DirectExchange exchange() &#123; // 创建一个交换机bean对象 生产者通过交换机转换对象 return ExchangeBuilder.directExchange(exchangeNormalName).build(); &#125; // 定义正常队列 @Bean public Queue normalQueue() &#123; // 这里很关键 这个队列的交换机和死信队列的交换机是一样的 return QueueBuilder.durable(queueNormalName) .expires(20000).deadLetterExchange(exchangeNormalName).deadLetterRoutingKey(&quot;error&quot;) .build(); &#125; // 绑定 @Bean public Binding bindingNormal(DirectExchange exchange, Queue normalQueue) &#123; // 因为是直连交换 因此需要执行key 这个key就是这个交换机和queue的连接类型 return BindingBuilder.bind(normalQueue).to(exchange).with(&quot;pay&quot;); &#125; // 定义正常队列 @Bean public Queue normal2Queue() &#123; // 这里很关键 这个队列的交换机和死信队列的交换机是一样的 return QueueBuilder.durable(queueNormalName) .expires(20000).deadLetterExchange(exchangeNormalName).deadLetterRoutingKey(&quot;error&quot;) .build(); &#125; // 绑定 @Bean public Binding binding2Normal(DirectExchange exchange, Queue normal2Queue) &#123; // 因为是直连交换 因此需要执行key 这个key就是这个交换机和queue的连接类型 return BindingBuilder.bind(normal2Queue).to(exchange).with(&quot;pay&quot;); &#125; // 定义 死信队列 @Bean public Queue dlxQueue() &#123; // 创建一个队列 指定他的名字 队列相当于数据库的表 return QueueBuilder.durable(queueDlxName).build(); &#125; // 值得一说的是 这里的依赖注入 参数直接就是以上的bean对象 类型名字一样 因此可以直接注入 spring容器 自动取出对应对象 注入 @Bean public Binding bindingDlx(DirectExchange exchange, Queue dlxQueue) &#123; return BindingBuilder.bind(dlxQueue).to(exchange).with(&quot;error&quot;); &#125;&#125; 多一个 队列 主要是生产者需要将按照过期时间将消息发送到不同队列 使用rabbitmq-delayed-message-exchange 延迟插件 一段时间以来，人们一直在寻找用RabbitMQ实现延迟消息的传递方法，到目前为止，公认的解决方案是混合使用TTL和DLX。而rabbitmq_delayed_message_exchange插件就是基于此来实现的，RabbitMQ延迟消息插件新增了一种新的交换器类型，消息通过这种交换器路由就可以实现延迟发送。 下载对应版本的插件包 http://www.rabbitmq.com/community-plugins.html 将文件放在plugins中 unzip rabbitmq_delayed_message_exchange-3.10.2.ez 如果unzip 没有安装，先安装一下 yum install unzip -y ./rabbitmq-plugins enable rabbitmq_delayed_message_exchange 开启插件； 消息发送后不会直接投递到队列，而是存储到 Mnesia（嵌入式数据库），检查 x-delay 时间（消息头部）；延迟插件在 RabbitMQ 3.5.7 及以上的版本才支持，依赖 Erlang&#x2F;OPT 18.0 及以上运行环境；Mnesia 是一个小型数据库，不适合于大量延迟消息的实现解决了消息过期时间不一致出现的问题。 1234567891011121314151617181920212223242526@Component@Slf4jpublic class RabbitConfig &#123; public static final String _EXCHANGE _= &quot;exchange:plugin&quot;; public static final String _QUEUE _= &quot;queue.plugin&quot;; public static final String _KEY _= &quot;plugin&quot;; /** * 创建自定义交换机 使用插件 没办法 只能自定义 */@Bean public CustomExchange customExchange() &#123; Map&lt;String, Object&gt; arguments = new HashMap&lt;&gt;(); arguments.put(&quot;x-delayed-type&quot;, &quot;direct&quot;); // 自定义交换机的类型是直连 // CustomExchange(String name, String type, boolean durable, boolean autoDelete, Map&lt;String, Object&gt; arguments) return new CustomExchange(_EXCHANGE_, &quot;x-delayed-message&quot;, true, false, arguments); &#125; @Bean public Queue queue() &#123; return QueueBuilder._durable_(_QUEUE_).build(); &#125; @Bean public Binding binding(CustomExchange customExchange, Queue queue) &#123; return BindingBuilder._bind_(queue).to(customExchange).with(_KEY_).noargs(); &#125;&#125; 1234567MessageProperties messageProperties=new MessageProperties();messageProperties.setHeader(&quot;x-delay&quot;,16000);String msg = &quot;hello world&quot;;Message message=new Message(msg.getBytes(),messageProperties);rabbitTemplate.convertAndSend(RabbitConfig._EXCHANGE_, &quot;plugin&quot;, message);_log_.info(&quot;发送完毕，发送时间为：&#123;&#125;&quot;,new Date()); RabbitMQ消息 可靠性Confirm模式 保证到交换机数据的安全消息的confirm确认机制，是指生产者投递消息后，到达了消息服务器Broker里面的exchange交换机，则会给生产者一个应答，生产者接收到应答，用来确定这条消息是否正常的发送到Broker的exchange中，这也是消息可靠性投递的重要保障；说白了：为了保证数据传递的可靠性 生产者发送消息给交换机以后 交换机要返回一个已接收的命令 不然就需要重新发送或者日志记录错误 1 配置文件application.yml 开启确认模式：spring.rabbitmq.publisher-confirm-type&#x3D;correlated2 写一个类实现implements RabbitTemplate.ConfirmCallback，判断成功和失败的ack结果，可以根据具体的结果，如果ack为false，对消息进行重新发送或记录日志等处理；3 设置rabbitTemplate的确认回调方法rabbitTemplate.setConfirmCallback(messageConfirmCallBack); 12345678910spring: application: name: producer rabbitmq: port: 5672 host: 192.168.70.132 username: admin password: 123456 virtual-host: luhumu publisher-confirm-type: correlated # 开启生产者的确认模式 设置关联模式 1234567891011121314151617181920212223242526/** * 实现这个接口 重写confirm方法 在交换机接收消息以后 会回调该方法 */@Componentpublic class MessageConfirmCallBack implements RabbitTemplate.ConfirmCallback &#123; /** * 交换机收到消息后，会回调该方法 * * @param correlationData 相关联的数据 * @param ack 有两个取值，true和false，true表示成功：消息正确地到达交换机，反之false就是消息没有正确地到达交换机 * @param cause 消息没有正确地到达交换机的原因是什么 */ @Override public void confirm(CorrelationData correlationData, boolean ack, String cause) &#123; System.out.println(&quot;correlationData = &quot; + correlationData); System.out.println(&quot;ack = &quot; + ack); System.out.println(&quot;cause = &quot; + cause); if (ack) &#123; //正常 &#125; else &#123; //不正常的，可能需要记日志或重新发送 &#125; &#125;&#125; 1234567891011121314151617181920212223242526@Servicepublic class MessageService &#123; @Resource private RabbitTemplate rabbitTemplate; @Resource private MessageConfirmCallBack messageConfirmCallBack; @PostConstruct //bean在初始化的时候，会调用一次该方法，只调用一次，起到初始化的作用 public void init() &#123; // 设置使用此对象来接收 返回的值 rabbitTemplate.setConfirmCallback(messageConfirmCallBack); &#125; /** * 发送消息 */ public void sendMessage() &#123; //关联数据对象 CorrelationData correlationData = new CorrelationData(); correlationData.setId(“O159899323”); //比如设置一个订单ID，到时候在confirm回调里面，你就可以知道是哪个订单没有发送到交换机上去 rabbitTemplate.convertAndSend(&quot;交换机名字发送到哪里？&quot;, “info”, “hello”, correlationData); System.out.println(“消息发送完毕…”); &#125;&#125; 写法2：同一个类直接使用生产者类 实现接口 发送 这样的好处就是少些一个类 123456789101112131415161718192021222324252627282930313233343536373839404142434445@Servicepublic class MessageService implements RabbitTemplate.ConfirmCallback&#123; @Resource private RabbitTemplate rabbitTemplate; @Resource private MessageConfirmCallBack messageConfirmCallBack; @PostConstruct //bean在初始化的时候，会调用一次该方法，只调用一次，起到初始化的作用 public void init() &#123; // 因为是在本类中所以使用this rabbitTemplate.setConfirmCallback(this); &#125; /** * 发送消息 */ public void sendMessage() &#123; //关联数据对象 CorrelationData correlationData = new CorrelationData(); correlationData.setId(“O159899323”); //比如设置一个订单ID，到时候在confirm回调里面，你就可以知道是哪个订单没有发送到交换机上去 rabbitTemplate.convertAndSend(&quot;交换机名字发送到哪里？&quot;, “info”, “hello”, correlationData); System.out.println(“消息发送完毕…”); &#125; /** * 交换机收到消息后，会回调该方法 * * @param correlationData 相关联的数据 * @param ack 有两个取值，true和false，true表示成功：消息正确地到达交换机，反之false就是消息没有正确地到达交换机 * @param cause 消息没有正确地到达交换机的原因是什么 */ @Override public void confirm(CorrelationData correlationData, boolean ack, String cause) &#123; System.out.println(&quot;correlationData = &quot; + correlationData); System.out.println(&quot;ack = &quot; + ack); System.out.println(&quot;cause = &quot; + cause); if (ack) &#123; //正常 &#125; else &#123; //不正常的，可能需要记日志或重新发送 &#125; &#125;&#125; 写法三：匿名内部类写法四：lambda Return模式 保证到队列数据的安全rabbitmq 整个消息投递的路径为：producer —&gt; exchange —&gt; queue —&gt; consumer 消息从 producer 到 exchange 则会返回一个 confirmCallback； 上面代码 消息从 exchange –&gt; queue 投递失败则会返回一个 returnCallback； 我们可以利用这两个callback控制消息的可靠性投递；开启 确认模式；使用rabbitTemplate.setConfirmCallback设置回调函数，当消息发送到exchange后回调confirm方法。在方法中判断ack，如果为true，则发送成功，如果为false，则发送失败，需要处理；注意配置文件中，开启 退回模式；spring.rabbitmq.publisher-returns: true 代码实现和以上没有差别 只是接口实现不同 使用rabbitTemplate.setReturnCallback设置退回函数，当消息从exchange路由到queue失败后，则会将消息退回给producer，并执行回调函数returnedMessage； 123456789101112131415161718192021222324252627282930313233343536373839// 实现接口重写方法 ReturnsCallback@Componentpublic class MessageReturnCallBack implements RabbitTemplate.ReturnsCallback &#123; /** * 当消息从交换机 没有正确地 到达队列，则会触发该方法 * 如果消息从交换机 正确地 到达队列了，那么就不会触发该方法 * * @param returned */ @Override public void returnedMessage(ReturnedMessage returned) &#123; System.out.println(“消息return模式：” + returned.getReplyText()); &#125;&#125;@Servicepublic class MessageService &#123; @Resource private RabbitTemplate rabbitTemplate; @Resource private MessageReturnCallBack messageReturnCallBack; @PostConstruct //bean在初始化的时候，会调用一次该方法，只调用一次，起到初始化的作用 public void init() &#123; rabbitTemplate.setReturnsCallback(messageReturnCallBack); &#125; /** * 发送消息 */ public void sendMessage() &#123; rabbitTemplate.convertAndSend(RabbitConfig.EXCHANGE, “info123”, “hello”); System.out.println(“消息发送完毕…”); &#125;&#125; 写法二：直接在发送消息类中实现写法三：匿名内部类写法四：lambda RabbitMQ交换机详细属性具体参数1、Name：交换机名称；就是一个字符串2、Type：交换机类型，direct, topic, fanout, headers四种3、Durability：持久化，声明交换机是否持久化，代表交换机在服务器重启后是否还存在；4、Auto delete：是否自动删除，曾经有队列绑定到该交换机，后来解绑了，那就会自动删除该交换机；5、Internal：内部使用的，如果是yes，客户端无法直接发消息到此交换机，它只能用于交换机与交换机的绑定。6、Arguments：只有一个取值alternate-exchange，表示备用交换机； 结论1：没发消息之前不会创建交换机和对列结论2：发消息后，如果交换机不存在，才开始创建交换机，如果队列不存在，则创建新的对列结论3：创建交换机或者队列完成后再重新创建，如果修改交换机或队列参数则会报错406错误（inequivalent arg ‘durable’ for exchange ‘exchange.durability’ in vhost ‘powernode’: received ‘false’ but current is ‘true’, class-id&#x3D;40, method-id&#x3D;10)）结论4：设置持久化为false ，重启rabbitmq-server，则交换机丢失，实验durable参数，先看下控制台，然后重启rabbitmq-server结论5：实验自动删除为 true ，从控制台上手动解绑，会发现自动删除 备用交换机当消息经过交换器准备路由给队列的时候，发现没有对应的队列可以投递信息，在rabbitmq中会默认丢弃消息，如果我们想要监测哪些消息被投递到没有对应的队列，我们可以用备用交换机来实现，可以接收备用交换机的消息，然后记录日志或发送报警信息。备用交换机示例如下：注意：备用交换机一般使用fanout交换机测试时：指定一个错误路由重点：普通交换机设置参数绑定到备用交换机 123456Map&lt;String, Object&gt; arguments = new HashMap&lt;&gt;();//指定当前正常的交换机的备用交换机是谁arguments.put(“alternate-exchange”, EXCHANGE_ALTERNATE);//DirectExchange(String name, boolean durable, boolean autoDelete, Map&lt;String, Object&gt; arguments)return new DirectExchange(EXCHANGE, true, false, arguments);//return ExchangeBuilder.directExchange(EXCHANGE).withArguments(args).build(); 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768@Configurationpublic class RabbitConfig &#123; //交换机的名字，就是一个字符串 public static final String EXCHANGE = “exchange”; //队列的名字，就是一个字符串 public static final String QUEUE = “queue”; //定义的一个路由键 public static final String INFO = “info”; //------------------------------------------- //交换机的名字，就是一个字符串 public static final String EXCHANGE_ALTERNATE = “exchange.alternate”; //队列的名字，就是一个字符串 public static final String QUEUE_ALTERNATE = “queue.alternate”; //定义的一个路由键 public static final String ALTERNATE = “alternate”; @Bean public DirectExchange directExchange() &#123; Map&lt;String, Object&gt; arguments = new HashMap&lt;&gt;(); arguments.put(“alternate-exchange”, EXCHANGE_ALTERNATE); //指定当前正常的交换机的备用交换机是谁 //DirectExchange(String name, boolean durable, boolean autoDelete, Map&lt;String, Object&gt; arguments) return new DirectExchange(EXCHANGE, true, false, arguments); &#125; /** * 声明一个队列 * * @return / @Bean public Queue queue() &#123; return QueueBuilder.durable(QUEUE).build(); &#125; /* * @Qualifier 限定bean的名字是 directExchange 的Bean * * @param directExchange * @return / @Bean public Binding binding(DirectExchange directExchange, Queue queue) &#123; return BindingBuilder.bind(queue).to(directExchange).with(INFO); &#125; //----------------------------------------- /* * 备用交换机需要用Fanout交换机； * * @return */ @Bean public FanoutExchange alternateExchange() &#123; //DirectExchange(String name, boolean durable, boolean autoDelete, Map&lt;String, Object&gt; arguments) return new FanoutExchange(EXCHANGE_ALTERNATE, true, false); &#125; @Bean public Queue alternateQueue() &#123; return QueueBuilder.durable(QUEUE_ALTERNATE).build(); &#125; @Bean public Binding alternateBnding(FanoutExchange alternateExchange, Queue alternateQueue) &#123; return BindingBuilder.bind(alternateQueue).to(alternateExchange); &#125;&#125; 12345678910111213141516@Servicepublic class MessageService &#123; @Resource private RabbitTemplate rabbitTemplate; /** * 发送消息 */public void sendMessage() &#123;//我们故意写错路由key,由于我们正常交换机设置了备用交换机，所以该消息就会进入备用交换机 //从而进入备用对列，我们可以写一个程序接收备用对列的消息，接收到后通知相关人员进行处理 //如果正常交换机没有设置备用交换机，则该消息会被抛弃。 rabbitTemplate.convertAndSend(RabbitConfig.EXCHANGE, “info1223”, “hello”); System.out.println(“消息发送完毕…”); &#125;&#125; RabbitMQ队列详细属性Type：队列类型Name：队列名称，就是一个字符串，随便一个字符串就可以；Durability：声明队列是否持久化，代表队列在服务器重启后是否还存在；Auto delete： 是否自动删除，如果为true，当没有消费者连接到这个队列的时候，队列会自动删除；Exclusive：exclusive属性的队列只对首次声明它的连接可见，并且在连接断开时自动删除；基本上不设置它，设置成false Arguments：队列的其他属性，例如指定DLX（死信交换机等）；1、x-expires：Number当Queue（队列）在指定的时间未被访问，则队列将被自动删除；2、x-message-ttl：Number发布的消息在队列中存在多长时间后被取消（单位毫秒）；3、x-overflow：String设置队列溢出行为，当达到队列的最大长度时，消息会发生什么，有效值为Drop Head或Reject Publish；4、x-max-length：Number队列所能容下消息的最大长度，当超出长度后，新消息将会覆盖最前面的消息，类似于Redis的LRU算法； 5、 x-single-active-consumer：默认为false激活单一的消费者，也就是该队列只能有一个消息者消费消息；6、x-max-length-bytes：Number限定队列的最大占用空间，当超出后也使用类似于Redis的LRU算法；7、x-dead-letter-exchange：String指定队列关联的死信交换机，有时候我们希望当队列的消息达到上限后溢出的消息不会被删除掉，而是走到另一个队列中保存起来；8.x-dead-letter-routing-key：String指定死信交换机的路由键，一般和6一起定义；9.x-max-priority：Number如果将一个队列加上优先级参数，那么该队列为优先级队列；（1）、给队列加上优先级参数使其成为优先级队列x-max-priority&#x3D;10【0-255取值范围】（2）、给消息加上优先级属性通过优先级特性，将一个队列实现插队消费； MessageProperties messageProperties&#x3D;new MessageProperties();messageProperties.setPriority(8); 10、x-queue-mode：String（理解下即可）队列类型x-queue-mode&#x3D;lazy懒队列，在磁盘上尽可能多地保留消息以减少RAM使用，如果未设置，则队列将保留内存缓存以尽可能快地传递消息；11、x-queue-master-locator：String（用的较少，不讲）在集群模式下设置队列分配到的主节点位置信息；每个queue都有一个master节点，所有对于queue的操作都是事先在master上完成，之后再slave上进行相同的操作；每个不同的queue可以坐落在不同的集群节点上，这些queue如果配置了镜像队列，那么会有1个master和多个slave。基本上所有的操作都落在master上，那么如果这些queues的master都落在个别的服务节点上，而其他的节点又很空闲，这样就无法做到负载均衡，那么势必会影响性能；关于master queue host 的分配有几种策略，可以在queue声明的时候使用x-queue-master-locator参数，或者在policy上设置queue-master-locator，或者直接在rabbitmq的配置文件中定义queue_master_locator，有三种可供选择的策略：（1）min-masters：选择master queue数最少的那个服务节点host；（2）client-local：选择与client相连接的那个服务节点host；（3）random：随机分配； 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677@Configurationpublic class RabbitConfig &#123; public static final String EXCHANGE = “exchange”; public static final String QUEUE = “queue”; public static final String KEY = “info”; QueueBuilder builder; @Bean public DirectExchange directExchange() &#123; return ExchangeBuilder.directExchange(EXCHANGE).build(); &#125; @Bean public Queue queue() &#123; Map&lt;String, Object&gt; arguments = new HashMap&lt;&gt;(); //arguments.put(“x-expires”, 5000); //arguments.put(“x-max-length”, 5); //arguments.put(“x-overflow”, “reject-publish”); arguments.put(“x-single-active-consumer”, false); //TODO ??? //arguments.put(“x-max-length-bytes”, 20); // 单位是字节 //arguments.put(“x-max-priority”, 10); // 0-255 //表示把当前声明的这个队列设置成了优先级队列，那么该队列它允许消息插队 //将队列设置为延迟模式，在磁盘上保留尽可能多的消息，以减少RAM内存的使用，如果未设置，队列将保留内存缓存以尽可能快地传递消息； //有时候我们把这种队列叫：惰性队列 //arguments.put(“x-queue-mode”, “lazy”); //设置队列版本。默认为版本1。 //版本1有一个基于日志的索引，它嵌入了小消息。 //版本2有一个不同的索引，可以在许多场景中提高内存使用率和性能，并为以前嵌入的消息提供了按队列存储。 //arguments.put(“x-queue-version”, 2); // x-queue-master-locator：在集群模式下设置镜像队列的主节点信息。 //arguments.put(“x-queue-master-locator”, QueueBuilder.LeaderLocator.clientLocal.getValue()); //------------------------- //arguments.put(“x-expires”, 10000); //自动过期，10秒 //arguments.put(“x-message-ttl”, 10000); //自动过期，10秒，不会删除队列 //QueueBuilder 类里面有定义，设置队列溢出行为，当达到队列的最大长度时消息会发生什么，有效值是drop-head、reject-publish //arguments.put(“x-max-length”, 5); //arguments.put(“x-overflow”, QueueBuilder.Overflow.dropHead.getValue()); //表示队列是否是单一活动消费者，true时，注册的消费组内只有一个消费者消费消息，其他被忽略，false时消息循环分发给所有消费者(默认false) //arguments.put(“x-single-active-consumer”, true); // x-max-length-bytes，队列消息内容占用最大空间，受限于内存大小，超过该阈值则从队列头部开始删除消息； //arguments.put(“x-max-length-bytes”, 10); //参数是1到255之间的正整数，表示队列应该支持的最大优先级，数字越大代表优先级越高，没有设置priority优先级字段，那么priority字段值默认为0；如果优先级队列priority属性被设置为比x-max-priority大，那么priority的值被设置为x-max-priority的值。 //arguments.put(“x-max-priority”, 10); //将队列设置为延迟模式，在磁盘上保留尽可能多的消息，以减少RAM的使用;如果未设置，队列将保留内存缓存以尽可能快地传递消息； //arguments.put(“x-queue-mode”, “lazy”); arguments.put(“x-queue-version”, 2); // x-queue-master-locator：在集群模式下设置镜像队列的主节点信息。 arguments.put(“x-queue-master-locator”, QueueBuilder.LeaderLocator.clientLocal.getValue()); //--------------------------------------------- // Queue(String name, boolean durable, boolean exclusive, boolean autoDelete, @Nullable Map&lt;String, Object&gt; arguments) return new Queue(QUEUE, true, false, false, arguments); &#125; @Bean public Binding binding(DirectExchange directExchange, Queue queue) &#123; return BindingBuilder.bind(queue).to(directExchange).with(KEY); &#125;&#125; 实验durable 参数 重启rabbitmq-server，队列丢失实验autodelete参数：加入接收者，发现停掉服务，那么久没有消费者了，对列就会自动删除 消息可靠性投递消息的可靠性投递就是要保证消息投递过程中每一个环节都要成功，那么这肯定会牺牲一些性能，性能与可靠性是无法兼得的；如果业务实时一致性要求不是特别高的场景，可以牺牲一些可靠性来换取性能。① 代表消息从生产者发送到Exchange;② 代表消息从Exchange路由到Queue；③ 代表消息在Queue中存储；④ 代表消费者监听Queue并消费消息； 1、确保消息发送到RabbitMQ服务器的交换机上可能因为网络或者Broker的问题导致①失败，而此时应该让生产者知道消息是否正确发送到了Broker的exchange中；有两种解决方案：第一种是开启Confirm（确认）模式；（异步）第二种是开启Transaction（事务）模式；（性能低，实际项目中很少用） 2、确保消息路由到正确的队列可能因为路由关键字错误，或者队列不存在，或者队列名称错误导致②失败。使用return模式，可以实现消息无法路由的时候返回给生产者；当然在实际生产环境下，我们不会出现这种问题，我们都会进行严格测试才会上线（很少有这种问题）；另一种方式就是使用备份交换机（alternate-exchange），无法路由的消息会发送到这个备用交换机上； 3、确保消息在队列正确地存储可能因为系统宕机、重启、关闭等等情况导致存储在队列的消息丢失，即③出现问题；解决方案：（1）、队列持久化代码：QueueBuilder.durable(QUEUE).build();（2）、交换机持久化代码：ExchangeBuilder.directExchange(EXCHANGE).durable(true).build();（3）、消息持久化代码：默认持久化| MessageProperties messageProperties &#x3D; new MessageProperties();&#x2F;&#x2F;设置消息持久化，当然它默认就是持久化，所以可以不用设置，可以查看源码messageProperties.setDeliveryMode(MessageDeliveryMode.PERSISTENT);（4）、集群，镜像队列，高可用 （5）确保消息从队列正确地投递到消费者采用消息消费时的手动ack确认机制来保证；如果消费者收到消息后未来得及处理即发生异常，或者处理过程中发生异常，会导致④失败。为了保证消息从队列可靠地达到消费者，RabbitMQ提供了消息确认机制（message acknowledgement）；#开启手动ack消息消费确认spring.rabbitmq.listener.simple.acknowledge-mode&#x3D;manual消费者在订阅队列时，通过上面的配置，不自动确认，采用手动确认，RabbitMQ会等待消费者显式地回复确认信号后才从队列中删除消息；如果消息消费失败，也可以调用basicReject()或者basicNack()来拒绝当前消息而不是确认。如果requeue参数设置为true，可以把这条消息重新存入队列，以便发给下一个消费者（当然，只有一个消费者的时候，这种方式可能会出现无限循环重复消费的情况，可以投递到新的队列中，或者只打印异常日志）； 消息的幂等性消息消费时的幂等性（消息不被重复消费）同一个消息，第一次接收，正常处理业务，如果该消息第二次再接收，那就不能再处理业务，否则就处理重复了；幂等性是：对于一个资源，不管你请求一次还是请求多次，对该资源本身造成的影响应该是相同的，不能因为重复的请求而对该资源重复造成影响；以接口幂等性举例：接口幂等性是指：一个接口用同样的参数反复调用，不会造成业务错误，那么这个接口就是具有幂等性的；注册接口；发送短信验证码接口；比如同一个订单我支付两次，但是只会扣款一次，第二次支付不会扣款，这说明这个支付接口是具有幂等性的；如何避免消息的重复消费问题？（消息消费时的幂等性）全局唯一ID + Redis生产者在发送消息时，为每条消息设置一个全局唯一的messageId，消费者拿到消息后，使用setnx命令，将messageId作为key放到redis中：setnx(messageId, 1)，若返回1，说明之前没有消费过，正常消费；若返回0，说明这条消息之前已消费过，抛弃；具体代码参考以下代码；参考代码： 12345678//1、把消息的唯一ID写入redis boolean flag = stringRedisTemplate.opsForValue().setIfAbsent(“idempotent:” + orders.getId(), String.valueOf(orders.getId())); //如果redis中该key不存在，那么就设置，存在就不设置 if (flag) &#123; //key不存在返回true //相当于是第一次消费该消息 //TODO 处理业务 System.out.println(“正常处理业务…” + orders.getId()); &#125; 集群 &#x2F;&#x2F;todohttps://blog.csdn.net/f5465245/article/details/130422034?spm=1001.2014.3001.5502 RocketMQhttp://rocketmq.apache.org/ RocketMQ是阿里巴巴2016年MQ中间件，使用Java语言开发，RocketMQ 是一款开源的分布式消息系统（可以做集群 不仅仅只是单点式），基于高可用分布式集群技术，提供低延时的、高可靠的消息发布与订阅服务。同时，广泛应用于多个领域，包括异步通信解耦、企业解决方案、金融支付、电信、电子商务、快递物流、广告营销、社交、即时通信、移动应用、手游、视频、物联网、车联网等。具有以下特点： 能够保证严格的消息顺序提供丰富的消息拉取模式高效的订阅者水平扩展能力实时的消息订阅机制亿级消息堆积能力 为什么要使用MQ1，要做到系统解耦，当新的模块进来时，可以做到代码改动最小; 能够解耦2，设置流程缓冲池，可以让后端系统按自身吞吐能力进行消费，不被冲垮; 能够削峰，限流3，强弱依赖梳理能把非关键调用链路的操作异步化并提升整体系统的吞吐能力;能够异步 Mq的作用** 削峰限流（消息全部堆积在队列 消费者可以根据自己的性能进行处理） 异步 解耦合** 解耦合：使用消息队列可以将发送者和接收者分离开来。发送者只需要将消息推送到队列中，而不需要知道消息被哪些接收者使用。接收者只需要监听指定的队列就可以获取消息，而不需要知道消息来自哪个发送者。 异步处理：通过消息队列，应用程序可以异步地处理一些任务，用户无需等待其完成。例如，当用户上传文件时，应用程序可以将文件放入消息队列中进行后续处理，例如转换文件格式或将其保存到磁盘上。 缓解流量峰值：在高并发场景下，如果所有请求都直接对后台服务进行调用，可能会导致系统过载。通过使用消息队列，可以缓解高峰期的流量压力。例如，可以将许多请求转换为消息，并将这些消息推送到消息队列中，以便稍后处理。 数据传输：消息队列常用于数据传送，它们允许您将数据从一个地方传送到另一个地方。例如，您可以使用消息队列传输超出单个服务器容量的日志文件或其他数据项。 可靠性：消息队列提供可靠的消息传递机制，保证消息不会丢失或重复。如果某个服务暂时不可用，消息队列也可以保留消息，直到该服务重新可用时再次处理它们。 吞吐量: 一定时间内，接收和处理消息的速度 例如：10m&#x2F;s协议： 一种规范 大家都遵从这个协议就能够交互 http tcp&#x2F;ip ftp 主流mq activeMQ (出现的最早 java写的 性能一般) 基本不用 rabbitMQ （很顶 吞吐量也一般但是比上面的好） 看以上 RocketMQ （java写的 阿里出品 吞吐量高 功能最丰富 性能好） Kafka（吞吐量最大 功能单一 就专注于吞吐量了 因此大数据使用多） 定义中间件（缓存中间件 redis memcache 数据库中间件 mycat canal 消息中间件mq ）面向消息的中间件(message-oriented middleware) MOM能够很好的解决以上的问题。是指利用高效可靠的消息传递机制进行与平台无关（跨平台 什么地方都能运行）的数据交流，并基于数据通信来进行分布式系统的集成。通过提供消息传递和消息排队模型在分布式环境下提供应用解耦，弹性伸缩，冗余存储，流量削峰，异步通信，数据同步等 大致流程发送者把消息发给消息服务器[MQ]，消息服务器把消息存放在若干队列&#x2F;主题中，在合适的时候，消息服务器会把消息转发给接受者。在这个过程中，发送和接受是异步的,也就是发送无需等待，发送者和接受者的生命周期也没有必然关系在发布pub&#x2F;订阅sub模式下，也可以完成一对多的通信，可以让一个消息有多个接受者[微信订阅号就是这样的] 对于rabbitmq： 这里面没有交换机的概念 很明显 这里就直接是 生产者-&gt; 队列-&gt; 消费者 RocketMQ重要概念【重点】基础流程Producer：消息的发送者，生产者；举例：发件人Consumer：消息接收者，消费者；举例：收件人Broker：暂存和传输消息的通道；举例：快递 也就是中转站NameServer：管理Broker；举例：各个快递公司的管理机构 相当于broker的注册中心，保留了broker的信息 **** 实际上他是一个路由注册中心 broker会定时的把信息例如ip地址上传到这里 这样生产者和消费者获取到同一个broker 就能进行交互Queue：队列，消息存放的位置，一个Broker中可以有多个队列 **Topic：主题，消息的分类 区分不同消息 用户只需要订阅主题就能之获得这一部分的消息 他只是一个概念 不是真实存在的**ProducerGroup：生产者组ConsumerGroup：消费者组，多个消费者组可以同时消费一个主题的消息消息发送的流程是，Producer询问NameServer，NameServer分配一个broker 然后Consumer也要询问NameServer，得到一个具体的broker，然后消费消息 （单机模式中）流程 生产者和消费者都向这个nameserve（路由注册中心） 获取broker（通道&#x2F;这里成为主题）的信息 这个主题里有队列（默认4个） 这样就能进行交互 （以上的图很好解释了概念）broker中有 不同主题 主题包含很多队列注意 消息给每一个组 组内如何划分需要看消费者组的配置 以上是单机模式集群下首先生产者需要去nameserver（路由注册中心）中获取broker（通道 中转站） 将数据写入这个通道的队列中 消费者同样取nameserver中获取 在集群状态下 数据会同步 因此 消费者可以在主从的集群中获取 rocketmq的核心存储消息队列最重要的一环就是削减峰值（削峰填谷） 简单来说就是请求太多 系统压力大 因此先把请求发送到消息队列中 系统在平缓的从队列中获取消息处理 这就需要保证消息存储的可靠性了 不能系统处理到一半 消息队列G了 那就G了 所以 broker中的消息应该存在那里？当然是持久化到磁盘 redis mysql当然也可以 但是都不安全 并且增加成本这里插一句题外话：有时候炫技固然很酷，但是真正在工作中切记不要过度设计、不要复杂化系统，一切先追求最简单的实现方式，这样更高效。那具体怎么存储呢？磁盘中的存储方式是怎么样的？类似一行一行存储 每条消息的大小不固定 不同topic的消息存在同一个文件 （之所以不同的topic存储在同一个文件 涉及到机械硬盘的结构的特征） 简单阐述以下机器硬盘的存储结构：如要访问硬盘的数据，就需要将磁头放到对应数据存储的扇区上所以我们查找磁盘的数据就要让磁头来回移动到我们数据所在的地方获取数据这就有一个问题 如果两个数据相隔较远 来回的取就很耗费时间所以在物理上 如果存储在硬盘上的数据在同一个磁道且相邻的扇区，那么根据硬盘的机械运行轨迹，读&#x2F;写的顺序就非常快（省略寻道时间，顺应旋转的方向），甚至可以媲美内存的读写速度！这就是常被用到的顺序写和顺序读 回归正题：所有的消息不论是哪个 topic 都写入同一个文件，并且都是顺序追加写入，那么对应到硬盘上就是顺序写！（基本上一个文件的内容，如果空间足够，那么都是连续的） 顺序写 读取的速度就非常快如果将不同的 topic 存入不同的文件，我们无法保证这些文件在物理上的位置是连续的 且如果 topic 很多，那么对应的文件就会很多，那每次写入不同的文件，可能都需要寻不同的道，写不同的扇区，这样速度就很慢，这就是随机写所以为了保证写入时候的性能， RocketMQ 设计所有 topic 的消息都写到一个文件里。这也是面试常常会问到 RocketMQ 为什么性能好的原因之一：顺序写。一定要牢记顺序写，这个在很多地方都有应用，比如 MySQL 的 redo log。 总结：RocketMQ 选择将消息写入到文件中，依赖机械硬盘（当然也可以是ssd）来保证消息存储的可靠性，并且根据机械硬盘的特性，把不同 topic 的消息都追加写入到一个叫 commitlog 的文件中，这样的顺序写性能很好。 nameserver启动broker前 需要先启动 nameserver待 Broker 启动后，Broker 需要将自己的一些信息上报到 NameSrv 上，并且每过 30s 也会上报自身信息至 NameSrv 。NameSrv 每 10s 会扫描它记录的 Broker 列表，看看这些 Broker 是否还活着。具体判断存活的方式是看 120s 内该 Broker 是否有上报自身消息至 NameSrv ，如果超过 120s 都没有，那么 NameSrv 会移除这个 Broker 相关的信息，表示这个 Broker 下线了。 Broker 会定时的把自己关于 Topic 的信息上报至 NameSrv，NameSrv 会维护记录这些信息，并且将无用的 Broker 剔除 RocketMQ安装下载地址：https://rocketmq.apache.org/dowloading/releases/ 安装rocketmq 导入到linux中 如果没有存放目录文件创建一个 创建目录 mkdir 文件名 解压 bin文件 unzip rocketmq-all-4.9.2-bin-release.zip 如果没有此压缩 下载一个 yum install unzip mv rocketmq-4.9.2/ ../mq 将解压的文件移动到指定目录 添加配置文件 vim &#x2F;etc&#x2F;profile 到最后一行加入 export NAMESRV_ADDR&#x3D;阿里云公网IP:9876 # 如果是线上就是用线上ip 本机就是用本机ip export NAMESRV_ADDR=localhost:9876 进入bin目录下，修改runserver.sh文件,将71行和76行的Xms和Xmx等改小一点 进入文档 输入： 然后:set nu 显示行数 按i进入编辑模式 之所以修改是因为他默认的参数太大 我们虚拟机承受不住 进入bin目录下，修改runbroker.sh文件,修改67行 进入conf目录下，修改broker.conf文件 加入三个参数 12345678910111213namesrvAddr=localhost:9876 # 指定你的namesrv 地址 注册中心autoCreateTopicEnable=true # 自动创建主题brokerIP1=阿里云公网IP # broker地址 这里我们指定自己的虚拟机地址namesrvAddr：nameSrv地址 可以写localhost因为nameSrv和broker在一个服务器autoCreateTopicEnable：自动创建主题，不然需要手动创建出来brokerIP1：broker也需要一个公网ip，如果不指定，那么是阿里云的内网地址，我们再本地无法连接使用# 自己设置的参数namesrvAddr=localhost:9876autoCreateTopicEnable=truebrokerIP1=192.168.70.132 **启动 **首先在安装目录下创建一个logs文件夹，用于存放日志mkdir logs 后台启动 同时启动两个命令 1234nohup sh bin/mqnamesrv &gt; ./logs/namesrv.log &amp;nohup sh bin/mqbroker -c conf/broker.conf &gt; ./logs/broker.log &amp; 我们的jdk没有写配置文件而是软连接 &#x2F;usr&#x2F;bin&#x2F;java 这里我们需要下载yum install java-1.8.0-openjdk-devel.x86_64 才能使用 jps查看 使用jps查看是否 启动成功 jsp -l 安装RocketMQ控制台的安装RocketMQ-Console 可视化工具 下载文件 https://github.com/apache/rocketmq-dashboard/archive/refs/tags/rocketmq-dashboard-1.0.0.zip 根目录执行 **mvn clean package -Dmaven.test.skip=true**** **他会执行此项目 生成jar包 传入 linux 这里我放在了 和以上rocket解压目录同级下 执行命令 nohup java -jar ./rocketmq-dashboard-1.0.0.jar --server.port=8001 rocketmq.config.namesrvAddr=127.0.0.1:9876 &gt; ./rocketmq-4.9.2/logs/dashboard.log &amp; 访问http://192.168.70.132:8001/ 因为我们namesrvAddr设置的就是这个地址 这里安装搞了很久 干 按照以上的配置走就是了 注意别打错参数 发送消息的模式 ***RocketMQ提供了多种发送消息的模式 例如：同步消息，异步消息，顺序消息，延迟消息，事物消息 消息发送和监听流程 消息生产者 producer 创建一个消息生产者 指定nameserver的地址 获取broker 通道 启动程序 创建消息对象 指定主题 tag等等 发送消息 关闭 消息消费者 consumer 创建消息消费者 指定消费者组名 指定 nameserver地址 创建监听订阅topic tag等等 处理消息 启动消费者 代码实现消费者 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.rocketmq&lt;/groupId&gt; &lt;artifactId&gt;rocketmq-client&lt;/artifactId&gt; &lt;version&gt;4.9.2&lt;/version&gt;&lt;/dependency&gt; 1234567891011121314151617181920212223/** * 生产者发送消息 * 1. 创建生产者对象 并且指定组名 * 2. 设置nameserver地址 * 3. 启动 * 4. 创建消息 * 5. 发送消息 * 6. 关闭 */ @Test void testProducer() throws Exception &#123; // 创建生产者组 DefaultMQProducer producer = new DefaultMQProducer(&quot;test-group&quot;); // 设置nameserver地址 访问这个地址获取broker信息 producer.setNamesrvAddr(&quot;192.168.70.132:9876&quot;); // 启动 producer.start(); // 设置消息 params 1----主题名称 2---- 具体消息 byte类型 Message topicTest = new Message(&quot;testTopic&quot;, &quot;Hello RecketMQ&quot;.getBytes()); producer.send(topicTest); // 关闭 producer.shutdown(); &#125; 一个主题默认有四个队列 具体放在哪一个队列 他有一个内置算法 向nameserver（路由注册中心）申请以后 生产者获取到了broker的ip信息 和他建立连接 向这个信道中的队列发送信息 监听者 1234567891011121314151617181920212223242526272829303132/** * 消费者 * 1. 创建消费者组 * 2. 设置nameserver地址 * 3. 订阅主题 来消费 * 4. 设置监听器 * 5. 返回结果 * 6. 启动 */ @Test void consumer() throws Exception&#123; // 创建消费者组 注意这里有两个 pull push 他们的底层都是pull模式 push就是有消息就推送给你 pull是消费者自己拉取 DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(&quot;consumer-group&quot;); // 设置nameserver地址 consumer.setNamesrvAddr(&quot;192.168.70.132:9876&quot;); // 订阅一个主题 来消费其中消息 params 1---- 主题组名称 2---表示订阅所有消息 consumer.subscribe(&quot;testTopic&quot;, &quot;*&quot;); // 设置监听器(异步的 一直监听) // lambda写法 list是消息列表 consumer.registerMessageListener((MessageListenerConcurrently) (list, consumeConcurrentlyContext) -&gt; &#123; // 实际上这里执行的就是业务 System.out.println(&quot;消费消费信息，内容是&quot; + new String(list.get(0).getBody())); // 消费上下文 System.out.println(consumeConcurrentlyContext); // 必须返回 有两种结果 返回成功---接收消息 队列中消息出队 返回失败 --- 拒绝消息 消息再次返回队伍 等待下一次被消费 return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; &#125;); consumer.start(); // 防止当前主进程执行完 监听还没监听完就停止了 因此阻塞一下 // 一直读，读不到就阻塞 System.in.read(); &#125; 生产者组 消费者组 可以存在多个生产者 向主题中的队列发送消息 那么这些生产者 组成为生产者组 注意：生产者组可以向任意主题发送消息 多个消费者也组成消费者组 **消费者组中的不同消费者 需要订阅同一个主题 不然会出现问题（订阅关系一致， 不能出现一个消费者组其中的消费者订阅不同的主题 ） ** 可以存在多个组存在 生产者发送消息到broker 有两个不同的消费者组都订阅了这个消息的主题 他们都会拿到这条信息 一个组消费信息 不影响其他组也消费以上图较为合理消息给到了不同的消费者组 消费者组的消息分配生产者按照轮询模式进行发送 （也就是按照队列顺序第一次给0队列 第二次给1队列 …）假设 主题队列中有消息进入 消费者组拿到了消息 那么这一条消息 要怎么分配呢？ 以组为单位分配具体组内分配方法 广播 （也就是每一个消费者都有一份） 负载均衡算法 具体看下面 如果多个消费者组都订阅了这个topic（主题） 那么每一组分配一个 生产者按照轮询模式进行发送 （也就是按照队列顺序第一次给0队列 第二次给1队列 …）一个主题内 默认有四个队列 那么消费者组中的不同消费者具体获取哪一个队列的消息？ 如何分配？ 广播 （每一个消费者 都去主题里获取一份消息） 负载均衡模式 这个负载均衡 消息到了消费者组 到底哪一个消费者接收？ 是如何处理的 负载均衡模式***消费者组中的消费者和主题中的队列绑定 或者说 组内消费者负责主题中的指定队列 另一个消费者消费其他的队列 主题默认有四个队列 假设是 0，1，2，3 现在消费者组中有一个消费者 那么就是 一个消费者监听四个队列消息 **如果消费者组中有两个（c1, c2） 那就是 c1绑定0，1 c2绑定2，3 队列0，1的消息给组内的c1消费者 ** 如果是三个消费者（c1,c2,c3） 那么其中一个绑定两个 另外一人绑定一个 **四个 一人一个 ** 五个 消费者组的数量大于了队列数量 这个时候多余的消费者 就会永远空闲 获取不到消息。因此 队列&gt;&#x3D;消费者 这里的负载均衡模式 采用的是轮询加加权随机的两种方式结合 首先向nameserver获取broker信息 获取所有队列，根据算法 来计算出每个消费者获取哪些消息队列的消息 注意：消费者组中的消费者消费信息 队列中的每个消息对应每个消费者有不同的指针（点） 先把队列中的信息给消费者 消费者返回一个收到的信息 再将点位移动到下一个 等此消费者下次来就可以消费指针在的位置位置点位来记录 如何实现消费者组订阅了主题 就能获取其中消息呢？ ***这里就需要引入消息位置（offset）的概念，这个概念你可以类比理解为数组的下标。我们的述求是消息可以被多个消费者消费，那么只需维护每个消费者已经消费到的位置，每当消费者消费一条消息，消费位置就+1，然后消费者根据记录的消息位置去消费对应的数据即可。就跟遍历数组一样，下标+1来访问后面的数据。这样就能满足不同消费者消费同一条消息，且不影响他们之间的消费进度的需求。就算加入了新的消费者 也只是只是加一个这个消费者的消费位置以上是单个消费者消费不同主题 如果是消费者组中的不同消费者 如何处理主题中的消息？在 RocketMQ 中叫队列（这个跟数据结构上的队列在概念上要区分下），在Kafka中叫分区 一个主题有默认有四个队列所以说 生产者发送到topic的消息是分散到不同队列中的此时我们就要有不同记录点位的方式我们消费点位的记录维度就变成了 Topic-消费组-队列，比如现在一共有两个主题，分别是 Topic-LOL、Topic-DOTA，每个主题都有两个队列（分区）。上图中 两个消费者组中有两个成员 两个消费者组都订阅了两个主题 而对于队列的划分是 每个消费者 对应主题中的一个队列 如果队列增加了 而消费者组的成员没有增加 那就一个消费者消费两个至此，我们就清晰了企业级消息队列实现的发布-订阅模式的核心原理：即 Topic 下分队列（分区），然后维护每个消费组在每个 Topic 下每个队列的消息位置，以消息位置（offset）来控制消息消费的进度。消息位置的灵活性不仅仅能区分不同消费组或者说消费者们的消费进度，还能实现重复消费或者跳过部分消息不消费的功能比如鱼皮-1已经消费到 Topic-LOL-队列1-20，即第 20 条消息，但是鱼皮1一不小心把之前关于 Topic-LOL的消费得到的结果数据弄丢了，如果按照队列模式那就找不到消息了，因为消息已经出队了没了。 重置消费点位即可而在发布-订阅模式中，我们仅需把这个消息位置变更成 Topic-LOL-队列1-0，这样又可以让鱼皮-1 重新消费，只需要简单地改一条数据就能实现这样功能。 关于代理者位点 和 差值代理者位点：随着当前队列的消息数量增大 如果队列消息1000 他就指向1000 当队列有消息进入 这个指针就向前消费者位点：随着消费者消费的数量增大 如果消费者了100 返回了成功的消息 此时消费者位点增加差值： 差值就是还没有被消费的消息 代理者位点减去消费者位点 如果这个差值过大 可能就需要看看是不是哪里出问题了 因为太多消息没被消费（难道是服务挂了？） 消费模式分为两种 push pull （官方已经不推荐使用了） 注意：任何的中间件都是pull模式 push的底层也是pull模式 MQ的消费模式可以大致分为两种，一种是推Push，一种是拉Pull。Push是服务端【MQ】主动推送消息给客户端，优点是及时性较好，但如果客户端没有做好流控，一旦服务端推送大量消息到客户端时，就会导致客户端消息堆积甚至崩溃。Pull是客户端需要主动到服务端取数据，优点是客户端可以依据自己的消费能力进行消费，但拉取的频率也需要用户自己控制，拉取频繁容易造成服务端和客户端的压力，拉取间隔长又容易造成消费不及时。Push模式也是基于pull模式的，只能客户端内部封装了api，一般场景下，上游消息生产量小或者均速的时候，选择push模式。在特殊场景下，例如电商大促，抢优惠券等场景可以选择pull模式 推模式适用在消费者不多、消息量不大、及时性要求高的场景 众口难调，每个消费者自己根据自己情况去拉取比较合适。 所谓的push 是采用了pull来实现：**长轮询: **消费者发送拉取请求到 Broker 时，如果此时有消息，那么 Broker 直接响应返回消息，如果没消息就 hold 住这个请求，比如等 15s，在 15s 内如果有消息过来就立马响应这个请求返回消息。 RocketMQ 本质上只实现了拉模式，pullConsumer 就是去拉消息很好理解，至于还有个 pushConsumer 实际上是伪推模式，底层的实现还是基于长链接的长轮询去拉取消息pushConsumer 的实现是背后有个线程会一直从 Broker 拉取消息，如果当前有过多的消息未被消费，那就过一会儿再执行，一旦有消息返回就回调用户定义的 MessageListener 来消费消息。 发送同步消息*** 用的多上面的代码就是发送同步消息，发送过后会有一个返回值，也就是mq服务器接收到消息后返回的一个确认，这种方式非常安全，但是性能上并没有这么高，而且在mq集群中，也是要等到所有的从机都复制了消息以后才会返回，所以针对重要的消息可以选择这种方式 生产者发送消息到mq主机 需要等待mq主机的确认收到消息 才开始下一步 是 一问一答的形式必须等到mq主机发送收到消息的确认 没有收到之前什么事都不做 也就是我们发送消息以后 他会返回一个ok 发送异步消息*** 用的多 异步消息通常用在对响应时间敏感的业务场景，即发送端不能容忍长时间地等待Broker的响应。发送完以后会有一个异步消息通知说白了** 发送消息以后 就可以去执行下面的代码了 等mq主机接收了 他会开启一个异步线程通知生产者是否接收到了** 12345678910111213141516171819202122232425@Test public void testAsyncProducer() throws Exception &#123; // 创建默认的生产者 DefaultMQProducer producer = new DefaultMQProducer(&quot;test-group&quot;); // 设置nameServer地址 producer.setNamesrvAddr(&quot;192.168.70.132:9876&quot;); // 启动实例 producer.start(); Message msg = new Message(&quot;TopicTest&quot;, (&quot;异步消息&quot;).getBytes()); // 开启异步消息 执行完以后不等待这里的返回结果 直接向下走 等到送完了 这个mq主机 broker会异步执行这个回调函数 producer.send(msg, new SendCallback() &#123; @Override public void onSuccess(SendResult sendResult) &#123; System.out.println(&quot;发送成功&quot;); &#125; @Override public void onException(Throwable e) &#123; System.out.println(&quot;发送失败&quot;); &#125; &#125;); System.out.println(&quot;看看谁先执行&quot;); // 挂起jvm 因为回调是异步的不然测试不出来 System.in.read(); // 关闭实例 producer.shutdown(); &#125; 消费者代码没有变化 发送单向消息 * 一般多这种方式主要用在不关心发送结果的场景，这种方式吞吐量很大，但是存在消息丢失的风险，例如日志信息的发送 单向的 就是我只管发 消息怎么样（丢了，收到了）与我无关，whatever 例子： 生产模式下日志是十分庞大的 b站高峰下日志达到了PB（MB GB TB PB） 这个时候存sql不合适 存在ES(搜索引擎 nosql)中 这个时候就要mq发送消息 log服务器的消费者消费消息写入ES关于以上中的日志 就可以生产者发送就可以使用单向消息 丢几条日志 怎么样 无伤大雅 123456789101112131415@Test void testoneWayProducer() throws Exception &#123; // 创建生产者组 DefaultMQProducer producer = new DefaultMQProducer(&quot;test-group&quot;); // 设置nameserver地址 访问这个地址获取broker信息 producer.setNamesrvAddr(&quot;192.168.70.132:9876&quot;); // 启动 producer.start(); // 设置消息 params 1----主题名称 2---- 具体消息 byte类型 Message topicTest = new Message(&quot;testTopic&quot;, &quot;Hello RecketMQ&quot;.getBytes()); // 发单次消息 producer.sendOneway(topicTest); // 关闭 producer.shutdown(); &#125; 发送延迟消息***消息放入mq后，过一段时间，才会被监听到，然后消费比如下订单业务，提交了一个订单就可以发送一个延时消息，30min后去检查这个订单的状态，如果还是未付款就取消订单释放库存。类似rabbitmq中的延迟队列 当消息被发送以后，并不想让消费者立刻拿到消息，而是等待特定时间后，消费者才能拿到这个消息进行消费。 生产者代码 123456789101112131415// 注意这是有误差的 第一次的时候可能会很慢void testDelayProducer() throws Exception &#123; // 创建生产者组 DefaultMQProducer producer = new DefaultMQProducer(&quot;test-group&quot;); // 设置nameserver地址 访问这个地址获取broker信息 producer.setNamesrvAddr(&quot;192.168.70.132:9876&quot;); // 启动 producer.start(); // 设置消息 params 1----主题名称 2---- 具体消息 byte类型 Message topicTest = new Message(&quot;testTopic&quot;, &quot;Hello RecketMQ&quot;.getBytes()); // 生产者设置延迟等级 注意是等级 有18个级别 1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h topicTest.setDelayTimeLevel(3); producer.send(topicTest); producer.shutdown(); &#125; 消费者消费者和上面一样这里注意的是RocketMQ不支持任意时间的延时只支持以下几个固定的延时等级，等级1就对应1s，以此类推，最高支持2h延迟private String messageDelayLevel &#x3D; “1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h”; 实际上延迟消息一开始不放在正常的 Topic 中， RocketMQ 专门搞了个 Topic 叫 SCHEDULE_TOPIC_XXXX ，将所有延迟消息都放在这个 Topic 下。然后有个定时任务来扫描遍历消息的延迟时间到了没，如果到了，那么再把延迟消息发往它本身的 Topic 队列中。这样就保证了延迟消息到时间之前，消费者不会消费到这个消息（因为消费者根本就没有订阅 SCHEDULE_TOPIC_XXXX ），然后一到时间，消息就被投递到原来的 Topic 上，这样消费者就能消费到了。 发送顺序消息 * 用的不多消息有序指的是可以按照消息的发送顺序来消费(FIFO—先进先出)。RocketMQ可以严格的保证消息有序，可以分为：分区有序或者全局有序。 分区有序： 保证多个队列的发送消费消息是有序的（FIFO） 全局有序：发送消息 消费消息只有一个队列 rocketMq的broker的机制，导致了rocketMq会有这个问题因为一个broker中对应了四个queue 顺序消费的原理解析，在默认的情况下消息发送会采取Round Robin轮询方式把消息发送到不同的queue(分区队列)；而消费消息的时候从多个queue上拉取消息，这种情况发送和消费是不能保证顺序。但是如果控制发送的顺序消息只依次发送到同一个queue中，消费的时候只从这个queue上依次拉取，则就保证了顺序。当发送和消费参与的queue只有一个，则是全局有序；如果多个queue参与，则为分区有序，即相对每个queue，消息都是有序的。 代码示例 分区有序 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647private List&lt;Order&gt; msgOrders = Arrays.asList( new Order(&quot;o1&quot;, 1, &quot;下单&quot;), new Order(&quot;o1&quot;, 1, &quot;短信&quot;), new Order(&quot;o1&quot;, 1, &quot;物流&quot;), new Order(&quot;o2&quot;, 2, &quot;下单&quot;), new Order(&quot;o2&quot;, 2, &quot;短信&quot;), new Order(&quot;o2&quot;, 2, &quot;物流&quot;) ); @Test void testOrderProducer() throws Exception &#123; // 创建生产者组 DefaultMQProducer producer = new DefaultMQProducer(&quot;order-p-group&quot;); // 设置nameserver地址 访问这个地址获取broker信息 producer.setNamesrvAddr(&quot;192.168.70.132:9876&quot;); // 启动 producer.start(); // 发送顺序消息 发送时保证有序 msgOrders.forEach(order -&gt; &#123; Message message1 = new Message(&quot;orderbyTopic&quot;, order.toString().getBytes()); try &#123; producer.send(message1, new MessageQueueSelector() &#123; /** * * @param list 这里代表消息队列 有三个 * @param message 消息 * @param o 我们下面写的order.getOrderId()会传给这里 * @return */ @Override public MessageQueue select(List&lt;MessageQueue&gt; list, Message message, Object o) &#123; // 在这里选择队列 将消息按照顺序发送 // 这里的o是我们下面的订单id 获取它的hashcode int hashCode = o.toString().hashCode(); // 这里很精妙 所有订单号一致的都是同一份hashcode 他们对list集合取模 一定比list.size()的值小 // 取模和取余 周期性函数 无论是什么取模 最终结果一定小于 这个模 // 这样 也使得分散的发送到不同队列中 int i = hashCode % list.size(); return list.get(i); &#125; &#125;, order.getOrderId()); &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125;); producer.shutdown(); &#125; 123456789101112131415161718192021222324@Testvoid orderConsumer() throws Exception&#123;// 创建消费者组 注意这里有两个 pull push 他们的底层都是pull模式 push就是有消息就推送给你 pull是消费者自己拉取DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(&quot;order-c-group&quot;);// 设置nameserver地址consumer.setNamesrvAddr(&quot;192.168.70.132:9876&quot;);// 订阅一个主题 来消费其中消息 params 1---- 主题组名称 2---表示订阅所有消息consumer.subscribe(&quot;orderbyTopic&quot;, &quot;*&quot;);// 设置监听器(异步的 一直监听)// MessageListenerConcurrently 并发的 多线程的// MessageListenerOrderly 顺序 单线程的consumer.registerMessageListener((MessageListenerOrderly) (list, consumeConcurrentlyContext) -&gt; &#123; // 实际上这里执行的就是业务 System.out.println(&quot;消费消费信息，内容是&quot; + new String(list.get(0).getBody())); // 消费上下文 System.out.println(consumeConcurrentlyContext); // 必须返回 有两种结果 返回成功---接收消息 队列中消息出队 返回失败 --- 拒绝消息 消息再次返回队伍 等待下一次被消费 return ConsumeOrderlyStatus.SUCCESS;&#125;);consumer.start();// 防止当前主进程执行完 监听还没监听完就停止了 因此阻塞一下// 一直读，读不到就阻塞System.in.read();&#125; 发送批量消息Rocketmq可以一次性发送一组消息，那么这一组消息会被当做一个消息消费说白了 发送的消息是一个数组 这一组消息被发送到一个队列中 假设数组有三个消息 那么队列中就有三个消息 他是将消息打包发送 12345678910111213141516@Test void testListProducer() throws Exception &#123; // 创建生产者组 DefaultMQProducer producer = new DefaultMQProducer(&quot;test-group&quot;); // 设置nameserver地址 访问这个地址获取broker信息 producer.setNamesrvAddr(&quot;192.168.70.132:9876&quot;); // 启动 producer.start(); // 批量消息 List&lt;Message&gt; messages = Arrays.asList( new Message(&quot;batchTopic&quot;, &quot;一组消息的A&quot;.getBytes()), new Message(&quot;batchTopic&quot;, &quot;一组消息的A&quot;.getBytes()), new Message(&quot;batchTopic&quot;, &quot;一组消息的A&quot;.getBytes())); producer.send(messages); producer.shutdown(); &#125; 发送带标签的消息，消息过滤 tagRocketmq提供消息过滤功能，通过tag或者key进行区分我们往一个主题里面发送消息的时候，根据业务逻辑，可能需要区分，比如带有tagA标签的被A消费，带有tagB标签的被B消费，还有在事务监听的类里面，只要是事务消息都要走同一个监听，我们也需要通过过滤才区别对待 说白了 主题内也要细分多个不同标签 同一个消费组的只能订阅同一份主题 但是组内的消费者需要根据不同的业务进行划分 这就需要tagtag相当于二级分类比如有人对足球主题感兴趣，但是只喜欢看梅西相关的消息，而有些人喜欢看 C 罗的消息。那么发送消息是往足球主题发，但是也顺带上了 tag 信息来标记这个消息是梅西的还是C罗的。这样订阅了足球主题的消费组可以根据 tag 选择它们想要的消息，过滤它们不想要的消息。比如喜欢梅西，那么订阅的时候标记自己只要 Topic-football 且 tag 是梅西的消息。 代码实现 123456789101112131415@Test void testTagProducer() throws Exception &#123; // 创建生产者组 DefaultMQProducer producer = new DefaultMQProducer(&quot;order-p-group&quot;); // 设置nameserver地址 访问这个地址获取broker信息 producer.setNamesrvAddr(&quot;192.168.70.132:9876&quot;); // 启动 producer.start(); // 创建消息 并且指定tag标签 可以指定多个tag Message message = new Message(&quot;football&quot;, &quot;c&quot;, &quot;c罗&quot;.toString().getBytes()); Message message2 = new Message(&quot;football&quot;, &quot;mx&quot;, &quot;梅西&quot;.toString().getBytes()); producer.send(message); producer.send(message2); producer.shutdown(); &#125; 12345678910111213141516171819202122@Test void tagConsumer() throws Exception&#123; // 创建消费者组 注意这里有两个 pull push 他们的底层都是pull模式 push就是有消息就推送给你 pull是消费者自己拉取 DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(&quot;tag-c-group&quot;); // 设置nameserver地址 consumer.setNamesrvAddr(&quot;192.168.70.132:9876&quot;); // 订阅一个主题 来消费其中消息 params 1---- 主题组名称 2--- 这里此时变化不是*所有 写tag名称 就会获取到该主题下的tag标签的消息 // 注意tag可以用 || 来选择多个 表示c或者mx都接收 consumer.subscribe(&quot;football&quot;, &quot;c || mx&quot;); // 设置监听器(异步的 一直监听) consumer.registerMessageListener(new MessageListenerConcurrently() &#123; @Override public ConsumeConcurrentlyStatus consumeMessage(List&lt;MessageExt&gt; list, ConsumeConcurrentlyContext consumeConcurrentlyContext) &#123; System.out.println(list.get(0).getBody()); return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; &#125; &#125;); consumer.start(); // 防止当前主进程执行完 监听还没监听完就停止了 因此阻塞一下 // 一直读，读不到就阻塞 System.in.read(); &#125; 总结：不同的业务应该使用不同的Topic如果是相同的业务里面有不同表的表现形式，那么我们要使用tag进行区分可以从以下几个方面进行判断： 消息类型是否一致：如普通消息、事务消息、定时（延时）消息、顺序消息，不同的消息类型使用不同的 Topic，无法通过 Tag 进行区分。 业务是否相关联：没有直接关联的消息，如淘宝交易消息，京东物流消息使用不同的 Topic 进行区分；而同样是天猫交易消息，电器类订单、女装类订单、化妆品类订单的消息可以用 Tag 进行区分。 消息优先级是否一致：如同样是物流消息，盒马必须小时内送达，天猫超市 24 小时内送达，淘宝物流则相对会慢一些，不同优先级的消息用不同的 Topic 进行区分。 消息量级是否相当：有些业务消息虽然量小但是实时性要求高，如果跟某些万亿量级的消息使用同一个 Topic，则有可能会因为过长的等待时间而“饿死”，此时需要将不同量级的消息进行拆分，使用不同的 Topic。 总的来说，针对消息分类，您可以选择创建多个 Topic，或者在同一个 Topic 下创建多个 Tag。但通常情况下，不同的 Topic 之间的消息没有必然的联系，而 Tag 则用来区分同一个 Topic 下相互关联的消息，例如全集和子集的关系、流程先后的关系。 还是那句话 tag相当于二级分类 如果需要的话就用 订阅关系：一个消费者组订阅一个 Topic 的某一个 Tag，这种记录被称为订阅关系。订阅关系一致：同一个消费者组下所有消费者实例所订阅的Topic、Tag必须完全一致。如果订阅关系（消费者组名-Topic-Tag）不一致，会导致消费消息紊乱，甚至消息丢失。官网的描述 订阅关系一致 https://rocketmq.apache.org/zh/docs/4.x/bestPractice/07subscribe RocketMQ中消息的Key***在rocketmq中的消息，默认会有一个messageId当做消息的唯一标识，我们也可以给消息携带一个key，用作唯一标识或者业务标识（一般都是自己设置一个key来控制 业务参数 自身确保唯一），包括在控制面板查询的时候也可以使用messageId或者key来进行查询 代码实现 1234567891011121314@Test void testKeyProducer() throws Exception &#123; // 创建生产者组 DefaultMQProducer producer = new DefaultMQProducer(&quot;order-key-group&quot;); // 设置nameserver地址 访问这个地址获取broker信息 producer.setNamesrvAddr(&quot;192.168.70.132:9876&quot;); // 启动 producer.start(); String key = UUID.randomUUID().toString(); // 创建消息 并且指定key 注意这个key一般一定要指定 这是业务上的确认唯一 Message message = new Message(&quot;football&quot;, &quot;c&quot;, key, &quot;c罗&quot;.toString().getBytes()); producer.send(message); producer.shutdown(); &#125; 1234567891011121314151617181920212223@Test void keyConsumer() throws Exception&#123; // 创建消费者组 注意这里有两个 pull push 他们的底层都是pull模式 push就是有消息就推送给你 pull是消费者自己拉取 DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(&quot;key-c-group&quot;); // 设置nameserver地址 consumer.setNamesrvAddr(&quot;192.168.70.132:9876&quot;); // 订阅一个主题 来消费其中消息 params 1---- 主题组名称 2--- 这里此时变化不是*所有 写tag名称 就会获取到该主题下的tag标签的消息 consumer.subscribe(&quot;football&quot;, &quot;c || mx&quot;); // 设置监听器(异步的 一直监听) consumer.registerMessageListener(new MessageListenerConcurrently() &#123; @Override public ConsumeConcurrentlyStatus consumeMessage(List&lt;MessageExt&gt; list, ConsumeConcurrentlyContext consumeConcurrentlyContext) &#123; MessageExt messageExt = list.get(0); // 这个MessageExt包含了消息的基本所有信息 key msgId等等 System.out.println(messageExt.getKeys()); return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; &#125; &#125;); consumer.start(); // 防止当前主进程执行完 监听还没监听完就停止了 因此阻塞一下 // 一直读，读不到就阻塞 System.in.read(); &#125; 消息重复消费问题（去重）***BROADCASTING(广播)模式下，所有注册的消费者都会消费，而这些消费者通常是集群部署的一个个微服务，这样就会多台机器重复消费，当然这个是根据需要来选择。CLUSTERING（负载均衡）模式下，如果一个topic被多个consumerGroup消费，也会重复消费。即使是在CLUSTERING模式下，同一个consumerGroup下，一个队列只会分配给一个消费者，看起来好像是不会重复消费。但是，有个特殊情况：一个消费者新上线后，同组的所有消费者要重新负载均衡（反之一个消费者掉线后，也一样）。一个队列所对应的新的消费者要获取之前消费的offset（偏移量，也就是消息消费的点位），此时之前的消费者可能已经消费了一条消息，但是并没有把offset提交给broker，那么新的消费者可能会重新消费一次。虽然orderly模式是前一个消费者先解锁，后一个消费者加锁再消费的模式，比起concurrently要严格了，但是加锁的线程和提交offset的线程不是同一个，所以还是会出现极端情况下的重复消费。还有在发送批量消息的时候，会被当做一条消息进行处理，那么如果批量消息中有一条业务处理成功，其他失败了，还是会被重新消费一次。那么如果在CLUSTERING（负载均衡）模式下，并且在同一个消费者组中，不希望一条消息被重复消费，改怎么办呢？我们可以想到去重操作，找到消息唯一的标识，可以是msgId也可以是你自定义的唯一的key，这样就可以去重了消息重复的两个原因 生产者多次投递 消费者方因为扩容时会重试 首先 p-&gt;mq 是基于tcp的可靠连接 三次握手 四次挥手因此 生产者多次投递 这种几率很小 而消费者方扩容 这是因为 a队列给c1投递消息 c1还没有发送给a队列确认收到的消息的时候 扩容消息负载均衡模式 此时a队列又给了c2 这个时候就出现了问题 重复投递的问题 解决：使用唯一key来确认 这条消息是否有确认过 如果已经处理过该消息 那就只是签收而不执行业务 唯一标识 存储唯一标识（redis mysql等等） 说白了 具体解决建立一个表（去重表) 这里指定一个唯一的索引 当消息进来 先向表中加入一条新数据 如果插入成功 代表可以执行 插入不成功 代表时重复的消息 就不执行 这里将插入数据库的操作提前 也保证了线程安全性 代码表中设置唯一索引 12345678CREATE TABLE `oder_oper_log` ( `id` bigint NOT NULL AUTO_INCREMENT, `type` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci NULL DEFAULT NULL, `order_sn` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci NULL DEFAULT NULL, `user` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci NULL DEFAULT NULL, PRIMARY KEY (`id`) USING BTREE, UNIQUE INDEX `order_sn_idx`(`order_sn`) USING BTREE) 使得存入表的key是唯一的 消费者 这里我们发两条一样key的消息 模拟消息重复 1234567891011121314151617@Test void testRepeatProducer() throws Exception &#123; // 创建生产者组 DefaultMQProducer producer = new DefaultMQProducer(&quot;repeat-key-group&quot;); // 设置nameserver地址 访问这个地址获取broker信息 producer.setNamesrvAddr(&quot;192.168.70.132:9876&quot;); // 启动 producer.start(); String key = UUID.randomUUID().toString(); // 创建消息 并且指定key 注意这个key一般一定要指定 这是业务上的确认唯一 Message message = new Message(&quot;football&quot;, null, key, &quot;库存-1&quot;.toString().getBytes()); Message messageRepeat = new Message(&quot;football&quot;, null, key, &quot;库存-1&quot;.toString().getBytes()); producer.send(message); producer.send(messageRepeat); log.info(&quot;发送成功&quot;); producer.shutdown(); &#125; 消费者 这里使用jdbc只是为了看到异常的发生 另外这里可以使用redis的setnx 但是他会有数据丢失的风险 因此实现的方法很多的 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152@Test void repeatConsumer() throws Exception&#123; DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(&quot;repeat-c-group&quot;); consumer.setNamesrvAddr(&quot;192.168.70.132:9876&quot;); consumer.subscribe(&quot;football&quot;, &quot;*&quot;); // 设置监听器(异步的 一直监听) consumer.registerMessageListener(new MessageListenerConcurrently() &#123; @Override public ConsumeConcurrentlyStatus consumeMessage(List&lt;MessageExt&gt; list, ConsumeConcurrentlyContext consumeConcurrentlyContext) &#123; // 获取key MessageExt messageExt = list.get(0); String keys = messageExt.getKeys(); // 先插入数据库 Connection conn = null; PreparedStatement ps = null; try &#123; conn = DriverManager.getConnection(&quot;jdbc:mysql://localhost:3306/order&quot;,&quot;root&quot;, &quot;123456&quot;); &#125; catch (SQLException e) &#123; throw new RuntimeException(e); &#125; // 新增 要么成功 要么报错 修改 要么成功 要么返回0 要么失败 String sql = &quot;insert into order_oper_log(`type`, `order_sn`, `user`) values (1, &quot;+ keys +&quot;, &#x27;123&#x27;)&quot;; try &#123; ps = conn.prepareStatement(sql); &#125; catch (SQLException e) &#123; throw new RuntimeException(e); &#125; int i = 0; try &#123; i = ps.executeUpdate(); &#125; catch (SQLException e) &#123; if (e instanceof SQLIntegrityConstraintViolationException) &#123; // 唯一索引冲突异常 // 说明消息来过了 log.error(&quot;消息重复&quot;); // 接收消息 不执行业务 return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; &#125; throw new RuntimeException(e); &#125; // 执行到这里 说明插入成功 // 注意要是这里执行业务逻辑的时候出现了错误 消息会重新投递 那就会发现数据库有key了 不能执行 // 因此 这里try-catch 出现异常将这个key的消息删掉 log.info(&quot;业务执行&quot;); return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; &#125; &#125;); consumer.start(); // 防止当前主进程执行完 监听还没监听完就停止了 因此阻塞一下 // 一直读，读不到就阻塞 System.in.read(); &#125; RocketMQ重试机制生产者发送失败 重试1234// 失败的情况重发3次 默认是2次producer.setRetryTimesWhenSendFailed(3);// 消息在1S内没有发送成功，就会重试producer.send(msg, 1000); 消费者重试如果我们的消费者消费消息失败 这个时候会重试 默认重试16次 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h 每次重试的间隔按照这个等级来 能否自定义重试次数？consumer.setMaxReconsumeTimes(2); 1234567891011121314151617181920@Test void retryConsumer() throws Exception&#123; DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(&quot;consumer-group&quot;); consumer.setNamesrvAddr(&quot;192.168.70.132:9876&quot;); consumer.subscribe(&quot;testTopic&quot;, &quot;*&quot;); // 在消息消费之前 监听器之前 设置重试次数 consumer.setMaxReconsumeTimes(2); consumer.registerMessageListener((MessageListenerConcurrently) (list, consumeConcurrentlyContext) -&gt; &#123; MessageExt messageExt = list.get(0); // 获取重试次数 如果重试一次这个参数就+1 第一次是0 重试第一次就是1 等30s重试第二次 // 我们设置了两次重试 因此就不会在重试了 // 注意这里的messageExt 存储了很多的消息的基本信息 System.out.println(messageExt.getReconsumeTimes()); return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; &#125;); consumer.start(); // 防止当前主进程执行完 监听还没监听完就停止了 因此阻塞一下 // 一直读，读不到就阻塞 System.in.read(); &#125; 如果重试16次（并发模式下） 顺序模式下（int最大值次）都是失败如何处理？ 死信消息 把这个消息放在死信主题中 %DLQ%+消费者组的名称(他也是是一个正常主题 只是说概念上是死信的 类似rabbitmq的死信队列) 当消息处理失败的时候 该如何正确的处理？ 在进入死信以后 我们写一个消费者监听死信队列 有消息通知人工处理 在业务中判断重试的次数 如果超过了这个次数 人工通知 这样避免写很多的死信监听器 (用的多) RocketMQ死信消息当消费重试到达阈值以后，消息不会被投递给消费者了，而是进入了死信队列当一条消息初次消费失败，RocketMQ会自动进行消息重试，达到最大重试次数后，若消费依然失败，则表明消费者在正常情况下无法正确地消费该消息。此时，该消息不会立刻被丢弃，而是将其发送到该消费者对应的特殊队列中，这类消息称为死信消息（Dead-Letter Message），存储死信消息的特殊队列称为死信队列（Dead-Letter Queue），死信队列是死信Topic下分区数唯一的单独队列。如果产生了死信消息，那对应的ConsumerGroup的死信Topic名称为%DLQ%ConsumerGroupName，死信队列的消息将不会再被消费。可以利用RocketMQ Admin工具或者RocketMQ Dashboard上查询到对应死信消息的信息。我们也可以去监听死信队列，然后进行自己的业务上的逻辑 SpringBoot 集成RocketMQ 依赖 配置文件 注入对象 基础用法生产者 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.rocketmq&lt;/groupId&gt; &lt;artifactId&gt;rocketmq-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.2.2&lt;/version&gt; &lt;/dependency&gt; 12345rocketmq: name-server: 192.168.70.132:9876 producer: group: boot-p-group # 生产者组名称 并不是说一个项目就只能是一个组 可以有很多个 # max-message-size: # 默认是4194304 4m 1024*1024*4 1234567@Resource private RocketMQTemplate rocketMQTemplate; @Test void testbootProducer() throws Exception &#123; // 发送同步消息 这是个重载方法 很多参数 在这里我们params1----主题名 2---消息内容 rocketMQTemplate.syncSend(&quot;bootTestTopic&quot;, &quot;springboot整合rocketmq&quot;); &#125; 消费者 123456&lt;dependency&gt; &lt;groupId&gt;org.apache.rocketmq&lt;/groupId&gt; &lt;artifactId&gt;rocketmq-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.2.2&lt;/version&gt;&lt;/dependency&gt; 123rocketmq: name-server: 192.168.70.132:9876 # 项目中可以有多个消费者组 但是一般项目开发中 一个boot项目就只对应一个消费者 12345678910111213public class ConsumerListener implements RocketMQListener&lt;MessageExt&gt; &#123; /** * 上面泛型指定了参数 所以这里的形参就是泛型的参数 一般约定是MessageExt 但是有可能有人写String * @param messageExt 消息的所有信息 *------------------------------------------- * 这个方法 没有报错就是签收 * 报错了就是拒收 会重试 */ @Override public void onMessage(MessageExt messageExt) &#123; System.out.println(Arrays.toString(messageExt.getBody())); &#125;&#125; 生产者123456789101112131415161718192021222324252627282930313233343536373839404142434445@Resource private RocketMQTemplate rocketMQTemplate; @Test void testbootProducer() throws Exception &#123; // 发送同步消息 这是个重载方法 很多参数 在这里我们params1----主题名 2---消息内容 rocketMQTemplate.syncSend(&quot;bootTestTopic&quot;, &quot;springboot整合rocketmq&quot;); // 发送异步消息 rocketMQTemplate.asyncSend(&quot;bootAsyncTopic&quot;, &quot;异步消息&quot;, new SendCallback() &#123; @Override public void onSuccess(SendResult sendResult) &#123; log.info(&quot;success&quot;); &#125; @Override public void onException(Throwable throwable) &#123; log.info(&quot;error&quot; + throwable.getMessage()); &#125; &#125;); // 单向消息 rocketMQTemplate.sendOneWay(&quot;bootOneWayTopic&quot;, &quot;单向消息&quot;); // 延迟消息 // 创建消息 org.springframework.messaging.Message&lt;String&gt; message = MessageBuilder.withPayload(&quot;延迟消息&quot;).build(); // params 1---主题名 2---消息 3---发送的最大时长 4---延迟等级 rocketMQTemplate.syncSend(&quot;bootMsTopic&quot;, message, 3000, 3); // 顺序 List&lt;Order&gt; orders = Arrays.asList( new Order(&quot;o1&quot;, 1, &quot;go&quot;), new Order(&quot;o1&quot;, 1, &quot;on&quot;), new Order(&quot;o1&quot;, 1, &quot;man&quot;), new Order(&quot;o2&quot;, 2, &quot;go&quot;), new Order(&quot;o2&quot;, 2, &quot;on&quot;) ); // 这里最大的区别是 发送到哪一个队列的均衡处理 不需要我们写了 // params 1---主题名 2---消息内容 3---hashcode值 可以直接写字符串 他会帮我们处理 分散的发送到不同队列中 // 分区有序 orders.forEach(order -&gt; rocketMQTemplate.syncSendOrderly(&quot;bootOrderlyTopic&quot;, JSON.toJSONString(order), order.getOrderId()) ); &#125; 消费者123456789101112131415161718192021222324/** * 消费者 * RocketMQMessageListener注解 消费的主题 消费者组名 * 实现接口 */@Component@RocketMQMessageListener(topic = &quot;bootTestTopic&quot;, consumerGroup = &quot;boot-test-consumer-group&quot;, consumeMode = ConsumeMode.ORDERLY, // 消费模式 并发还是顺序 多线程还是单线程 maxReconsumeTimes = 5 // 消费重试次数)public class ConsumerListener implements RocketMQListener&lt;MessageExt&gt; &#123; /** * 上面泛型指定了参数 所以这里的形参就是泛型的参数 一般约定是MessageExt 但是有可能有人写String * @param messageExt 消息的所有信息 *------------------------------------------- * 这个方法 没有报错就是签收 * 报错了就是拒收 会重试 */ @Override public void onMessage(MessageExt messageExt) &#123; System.out.println(Arrays.toString(messageExt.getBody())); &#125;&#125; tag生产者 12// 主题名这里使用 :tag名的方式 注入tag rocketMQTemplate.syncSend(&quot;bootTestTopic:tagA&quot;, &quot;tag&quot;); 消费者 123456789101112131415161718192021@Component@RocketMQMessageListener(topic = &quot;bootTestTopic&quot;, consumerGroup = &quot;boot-test-consumer-group&quot;, consumeMode = ConsumeMode.ORDERLY, // 消费模式 并发还是顺序 多线程还是单线程 maxReconsumeTimes = 5, // 消费重试次数 selectorType = SelectorType.TAG, // tag过滤方式 还有个sql方式 太抽象了 selectorExpression = &quot;tagA || tagB&quot; // tag类型)public class ConsumerListener implements RocketMQListener&lt;MessageExt&gt; &#123; /** * 上面泛型指定了参数 所以这里的形参就是泛型的参数 一般约定是MessageExt 但是有可能有人写String * @param messageExt 消息的所有信息 *------------------------------------------- * 这个方法 没有报错就是签收 * 报错了就是拒收 会重试 */ @Override public void onMessage(MessageExt messageExt) &#123; System.out.println(Arrays.toString(messageExt.getBody())); &#125;&#125; key生产者 123// 带key的消息 setHeader可以设置多个消息头的值 这里设置key的值 MessageBuilder.withPayload(&quot;我是一个带key的消息&quot;).setHeader(RocketMQHeaders.KEYS, &quot;keyofvalue&quot;).build(); rocketMQTemplate.syncSend(&quot;bootkeyTopic&quot;, message); 消费者 1String keys = messageExt.getKeys(); 消息消费两种模式Rocketmq消息消费的模式分为两种：负载均衡模式和广播模式负载均衡模式表示多个消费者交替消费同一个主题里面的消息广播模式表示每个每个消费者都消费一遍订阅的主题的消息 123456789101112131415161718192021222324252627 /** * messageModel 指定消息消费的模式 * CLUSTERING 为负载均衡模式 * BROADCASTING 为广播模式 */@Component@RocketMQMessageListener( topic = &quot;model&quot;, consumerGroup = &quot;powernode-group&quot;, messageModel = MessageModel.CLUSTERING // CLUSTERING负载均衡模式(默认) BROADCASTING 广播模式)public class ConsumerListener implements RocketMQListener&lt;MessageExt&gt; &#123; /** * 上面泛型指定了参数 所以这里的形参就是泛型的参数 一般约定是MessageExt 但是有可能有人写String * @param messageExt 消息的所有信息 *------------------------------------------- * 这个方法 没有报错就是签收 * 报错了就是拒收 会重试 */ @Override public void onMessage(MessageExt messageExt) &#123; // 获取key String keys = messageExt.getKeys(); System.out.println(Arrays.toString(messageExt.getBody())); &#125;&#125; CLUSTERING 负载均衡模式下 队列被消费者组的消费者分摊， 队列数量&gt;&#x3D; 消费者数量 不同消费者消费消息的消费点位 mq服务器会记录处理 以此来确定不同消费者组获取消息的独立性 BROADCASTING 广播模式下 消息会被每一个消费者都处理一次 mq服务器不会记录消费点位 报错了也不重试 也就是上图 不同消费者组中的消费者 对应不同主题的队列 mq服务器会记录他们的消费位置 如何解决消息堆积问题？ 面试题***消息堆积：很多消息没处理 都囤积在broker中 差值太大了一般认为单条队列消息差值&gt;&#x3D;10w时 算堆积问题 什么时候会产生问这个问题？ 生产太快了 生产方可以做业务限流 增加消费者数量 但是消费者数量 &lt;&#x3D; 队列数量 适当的设置最大的消费线程数量（根据业务是 io密集度（2n） 还是 cpu密集度(n+1)来确定）动态扩容队列数量,从而增加消费者数量123456@Component@RocketMQMessageListener(topic = &quot;bootTestTopic&quot;, consumerGroup = &quot;boot-test-consumer-group&quot;, consumeThreadNumber = 40 // 最大线程数量 )public class ConsumerListener implements RocketMQListener&lt;MessageExt&gt; &#123; 一般来说 一开始的时候运维就制定队列大小 队列多了 轮询方式 每一个都有一点 这样消费者也跟着加大 压力就小了 另外扩容的时候 不是直接就成功的 生产者会隔一段时间跟nameserver获取对应新地址 才会匹配上 消费者消费出现问题 (消费者G了 那消息肯定堆积啊)解决：排查消费者程序的问题 简单提一下 线程池 1234567// params 1--- 核心池的大小 2--- 最大线程数// 注意在java中一个线程就是一个对象ThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor( 10, // 核心线程大小 Runtime.getRuntime().availableProcessors() * 2, // 最大写多少个？根据做什么来判断 IO密集型（读写）一般写2n n是当前电脑的处理器数量 CPU密集型（运算） 一般写n+1); 因此解决这个问题： 加队列并且加消费者 如何确保消息不丢失？保证消息不丢失 那就是序列化到硬盘 同步刷盘&#x2F;落盘 生产者发同步消息 给 broker 先序列化到机械硬盘 然后返回给生产者一个完成的消息虽然是同步 但是性能不算慢 因为是顺序读写(参考最上面的核心存储) 异步刷盘 生产者 发送消息 到broker 这个时候 先把消息放在buffer中 缓冲区里 等到一定数量 再去存到磁盘这种是不安全的 万一断电了 消息就没了 自己持久化一份 生产者使用同步发送模式 ，收到mq的返回确认以后 顺便往自己的数据库里面写 msgId status(0) time 消费者消费以后 修改数据这条消息的状态 &#x3D; 1 写一个定时任务 间隔两天去查询数据 如果有status &#x3D; 0 and time &lt; day-2 将mq的刷盘机制设置为同步刷盘 使用集群模式 ，搞主备模式，将消息持久化在不同的硬件上 可以开启mq的trace机制，消息跟踪机制 mq提供了 消息从发送到接收的监控（消息追踪） 1.在broker.conf中开启消息追踪traceTopicEnable&#x3D;true2.重启broker即可3.生产者配置文件开启消息轨迹enable-msg-trace: true4. 消费者开启消息轨迹功能，可以给单独的某一个消费者开启enableMsgTrace &#x3D; true 在rocketmq的面板中可以查看消息轨迹默认会将消息轨迹的数据存在 RMQ_SYS_TRACE_TOPIC 主题里面 安全 开启acl的控制 在broker.conf中开启aclEnable&#x3D;true 配置账号密码 修改plain_acl.yml 修改控制面板的配置文件 放开52&#x2F;53行 把49行改为true 上传到服务器的jar包平级目录下即可","categories":[{"name":"中间件","slug":"中间件","permalink":"http://example.com/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[]},{"title":"Hello World","slug":"first","date":"2023-07-30T15:02:11.582Z","updated":"2023-07-30T15:07:53.874Z","comments":true,"path":"2023/07/30/first/","link":"","permalink":"http://example.com/2023/07/30/first/","excerpt":"","text":"导航覅哦萨芬时代u回复i阿三","categories":[{"name":"杂技","slug":"杂技","permalink":"http://example.com/categories/%E6%9D%82%E6%8A%80/"},{"name":"小品","slug":"杂技/小品","permalink":"http://example.com/categories/%E6%9D%82%E6%8A%80/%E5%B0%8F%E5%93%81/"}],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2023-07-30T14:41:03.779Z","updated":"2023-07-30T14:41:03.779Z","comments":true,"path":"2023/07/30/hello-world/","link":"","permalink":"http://example.com/2023/07/30/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[{"name":"前端","slug":"前端","permalink":"http://example.com/categories/%E5%89%8D%E7%AB%AF/"},{"name":"设计模式","slug":"设计模式","permalink":"http://example.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"中间件","slug":"中间件","permalink":"http://example.com/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"后端","slug":"后端","permalink":"http://example.com/categories/%E5%90%8E%E7%AB%AF/"},{"name":"杂技","slug":"杂技","permalink":"http://example.com/categories/%E6%9D%82%E6%8A%80/"},{"name":"小品","slug":"杂技/小品","permalink":"http://example.com/categories/%E6%9D%82%E6%8A%80/%E5%B0%8F%E5%93%81/"}],"tags":[]}